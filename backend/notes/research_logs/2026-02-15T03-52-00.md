---
title: Research Log Update
date: 2026-02-15T03:52:00
timestamp: 2026-02-15 03:52:00
tricks: 2
proposals: 2
experiments: 3
source: legacy_migration
---

## 2026-02-15 â€” 03:52 UTC

### ðŸŽ¯ High-Impact Proposals

- **Monarch-Gated State Transition SSM** (Proposal 006 â€” Priority: **HIGH**)
  - **Hypothesis**: Input-dependent Monarch-factored state transitions achieve near-dense expressivity at O(nâˆšn) cost, with native BMM implementation for 2â€“4Ã— higher GPU utilization than custom scan kernels.
  - **Why it matters**: This is the most exciting new proposal in this batch. It fills the *exact gap* between diagonal SSMs (O(n), no mixing) and dense transitions (O(nÂ²), impractical) with a principled, hardware-native solution. Three things make it stand out: (1) Monarch products are *closed* â€” chains of Monarch transitions stay Monarch, unlike DPLR or column-sparse, which is critical for scan-based training; (2) the block-diagonal + permutation structure maps directly to BMM, achieving near-peak GPU throughput without custom kernels; (3) the Sâ‚… permutation group composition task is a clean, decisive test â€” diagonal provably cannot do it. If Monarch-gated SSM hits >85% on Sâ‚… while diagonal stays <50%, that's a clear win with immediate scaling implications.
  - **Estimated cost**: **<$3** (MVE is a tiny model on synthetic Sâ‚… task; full-scale estimated at 32 A100-hrs â‰ˆ $50-60 on spot, but the MVE alone is decisive)
  - **Impact score**: **9/10** â€” Best novelty/cost ratio in the portfolio. Combines 5 proven tricks in an untested configuration. Monarch closure property is a unique structural advantage no other proposal has. MVE is trivially cheap.

- **Oscillatory-DPLR SSM** (Proposal 004 â€” Priority: **HIGH**, but needs debugging)
  - **Hypothesis**: Oscillatory discretization (|Î»|â‰¤1 by construction from second-order ODEs) + DPLR structure = stable, efficient, interpretable SSM without eigenvalue constraints.
  - **Why it matters**: Still the most theoretically elegant proposal â€” physics-based stability is strictly better than constraint-based stability. But **Experiment 002 just failed**, and the failure mode is concerning: training loss was completely flat at 0.854 across all 50 epochs, suggesting gradients are not flowing through the oscillatory parameterization at all. This needs root-cause analysis before any further investment.
  - **Estimated cost**: **<$1** (MVE debug runs are essentially free on CPU)
  - **Impact score**: **7/10** â†“ (down from 9/10 in previous log â€” the MVE failure is a real signal that needs investigation before re-ranking)

### ðŸ§ª Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: **completed â€” FAILED**)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Full MVE implemented and trained to completion. Model: 1 layer, n=16, r=2, ~129 params. Task: damped sinusoid extrapolation (train T=128, test T=512).
  - **Key findings**:
    - âŒ Training MSE: **0.854** (target: <1e-3) â€” model did not learn at all
    - âŒ Extrapolation MSE: **0.759** (target: <1e-2)
    - âš ï¸ Learned Ï‰ values *are* in the correct range [0.012, 0.096] matching ground truth [0.01, 0.1], suggesting the frequency parameterization works but the *output pathway* is broken
    - **Critical diagnostic**: Loss was flat across all 50 epochs (0.8544 â†’ 0.8544). Zero learning occurred. This is almost certainly a gradient flow bug, not a fundamental architectural problem.
    - Learned Î¶ values span [0.26, 0.71] (target [0.2, 0.8]) â€” also reasonable initialization
  - **Cost**: $0.00 actual (CPU, ~27 min). Well within budget.
  - **Verdict**: **DEBUG**. Three hypotheses to test: (1) gradients vanish through bilinear discretization; (2) DPLR low-rank component (P, Q) interference; (3) output projection scaling. Ablation: try pure diagonal oscillatory (drop DPLR) first.

- **Experiment 001: CS-NEG-DeltaNet D4 State Tracking** (Status: **implemented, not yet run**)
  - **Proposal**: 001-column-sparse-negative-eigenvalue-deltanet
  - **Progress**: Complete codebase with 4 model variants, D4 group multiplication, curriculum learning, W&B integration, Modal deployment configs. Ready to launch.
  - **Key findings**: Implementation analysis revealed DeltaProduct (nâ‚•=2 Householder reflections) may be a simpler, more gradient-friendly alternative to explicit column-sparse permutations for D4. Consider adding as 5th ablation before running.
  - **Cost**: $0.00 actual. Projected ~$1-2 on Modal T4.

### ðŸ“š New Discoveries

This cycle added **46 tricks** and **3 new proposals** (004, 005, 006), bringing the total knowledge base to a substantial structured matrix and parallelization toolkit. Key new additions beyond what was logged previously:

- **Neumann Series Approximate Inverse**: Explicit, inverse-free approximation to (I-A)â»Â¹ via matrix-matrix products only. Potentially useful for avoiding explicit inversion in DPLR resolvent computation â€” could complement the Woodbury identity when spectral radius is controlled.

- **Displacement Rank for Cauchy-Like Matrices**: Unified framework for structured matrix computation via compact generators. The O(n^{3/2}) multiply algorithm for displacement rank Î± matrices could accelerate S4-style Cauchy kernel evaluations when batched.

- **Alternating Low-Rank + Diagonal (Alt) Decomposition**: Iterative spectral algorithm for Î£ = D + UUáµ€ factorization. Directly applicable to state covariance approximation in Kalman-filter-inspired SSM variants.

- **Fast Kernel Transform (FKT)**: O(N log N) approximate MVMs for *any* isotropic kernel via automatic differentiation of generalized multipole expansions. Could replace handcrafted Cauchy kernel evaluations with a generic accelerator.

- **Tangent Space Projection for LRPD Matrices**: Differential geometry approach for integrating matrix ODEs on the low-rank + diagonal manifold at O(dÂ·p) cost. Relevant for Oscillatory-DPLR if the DPLR structure needs to evolve during training.

- **Tiled QR Factorization** + **TSQR**: Communication-avoiding parallel QR algorithms. Directly useful for accelerating Householder accumulation in DeltaNet/DeltaProduct training.

### Other Proposals

- **SSD-DeltaNet** (Proposal 002, high priority): WY â†’ semiseparable reformulation for tensor-core acceleration of DeltaNet training. Solid engineering proposal. Cost ~$5-8. Not started.

- **DPLR Column-Sparse SSM** (Proposal 003, medium priority): Column-sparse permutation wrapped around DPLR core. Interesting but the Monarch-gated proposal (006) may subsume this â€” Monarch matrices are a strict generalization of the DPLR + single-permutation structure, with better closure properties. **Consider deprioritizing in favor of 006.**

- **Segmented-HSS Linear Attention** (Proposal 005, medium priority): HSS-structured linear attention state matrices for O(d log d) updates + variable-length batching. Architecturally creative but implementation complexity is high (needs custom HSS kernels) and the benefit mainly kicks in at large d (>1024). **Highest risk in portfolio; defer until cheaper proposals are validated.**

### Strategic Insights

**The Monarch-gated proposal (006) should jump to #1 priority.** Here's why: Experiment 002 (Oscillatory-DPLR) just failed with flat loss, and debugging will take time. Meanwhile, Proposal 006 has the strongest structural advantage in the portfolio â€” Monarch closure under multiplication â€” which no other proposal achieves. The MVE (Sâ‚… permutation task) is trivially cheap (<$3) and will produce a clean, interpretable result. If it works, it opens a clear path to scaling; if it fails, the failure mode will be informative about the limits of sub-quadratic mixing.

**Revised priority order** (optimizing for information value per dollar):
1. **Monarch-Gated SSM (006)** â€” <$3, highest novelty, clean MVE, no dependencies
2. **Oscillatory-DPLR debug (004)** â€” <$1, diagnose gradient flow failure before abandoning
3. **CS-NEG-DeltaNet (001)** â€” <$2, ready to launch, add DeltaProduct ablation
4. **SSD-DeltaNet (002)** â€” ~$5-8, engineering optimization, lower urgency
5. **DPLR Column-Sparse (003)** â€” potentially subsumed by 006, hold
6. **Segmented-HSS (005)** â€” defer, implementation risk too high for current budget

**The Experiment 002 failure is actually informative**: the fact that Ï‰ and Î¶ learned reasonable values while the model produced zero output improvement suggests the oscillatory eigenvalue *initialization* works but the *forward pass* has a bug (likely in the bilinear discretization or output projection). This is probably fixable in <1 hour of debugging â€” worth doing before writing off the approach.



## 2026-02-15 - 02:57 UTC

### ðŸŽ¯ High-Impact Proposals

- **Oscillatory-DPLR SSM: Constraint-Free Stable State Spaces** (Priority: **HIGH**)
  - **Hypothesis**: Merge oscillatory discretization (guaranteed |Î»|â‰¤1 from second-order ODEs) with DPLR structure for O(n) Cauchy kernel convolution
  - **Why it matters**: Solves S4/S5's fragile stability problem without eigenvalue constraints while keeping full efficiency. Oscillatory parameters (Ï‰, Î¶) have direct physical meaning (frequency, damping), making models more interpretable and easier to initialize than opaque complex eigenvalues. This addresses a core pain point: S4 models are notoriously finicky to train due to eigenvalue escaping [-1,1].
  - **Estimated cost**: **<$5** (small SSM on sMNIST/psMNIST benchmarks, ~2-4 GPU hours on a consumer RTX 3090 or T4)
  - **Impact score**: **9/10** - Combines proven stable parameterization with proven efficient computation. Low risk, high theoretical foundation, extremely cheap to validate.

- **SSD-DeltaNet: Semiseparable Block Decomposition via WY Representation** (Priority: **HIGH**)
  - **Hypothesis**: Reformulate DeltaNet's WY-represented state matrix as block-semiseparable, enabling Mamba-2's SSD algorithm for 2-4Ã— speedup while keeping delta rule's superior associative memory
  - **Why it matters**: DeltaNet has best-in-class associative recall but trains slowly. Mamba-2's SSD trick (block decomposition + tensor cores) is fast but uses weaker linear attention. This proposal could give us the best of both worldsâ€”strong memory with fast training.
  - **Estimated cost**: **<$8** (requires slightly larger model ~125M params on small LM task, ~6-8 hours on single A100/H100 spot instance or ~$4-6 on cloud spot pricing)
  - **Impact score**: **8/10** - High novelty, solid theory (both components proven separately), feasible on modest hardware. Risk: semiseparable structure may not perfectly fit DeltaNet's update pattern.

### ðŸ“š New Discoveries: Key Algorithmic Insights

The last 12 hours brought **37 new tricks** spanning the full stack from hardware primitives to algebraic structures. Several clusters emerge:

**Hardware-Aware Foundations** (enabling cheap experiments):
- **IO-Aware Tiling** + **Kernel Fusion** + **Online Softmax**: The FlashAttention trinity that makes memory-bound operations tractable on consumer GPUs
- **2:4 Structured Sparsity**: Native Ampere/Ada support for 50% sparsity with zero overheadâ€”critical for running larger models on budget hardware
- **Tiled QR** + **TSQR**: Parallel QR factorization that hides sequential bottlenecks, enabling faster orthogonal weight updates

**Structured Matrix Zoo** (efficiency without approximation):
- **Semiseparable Block Decomposition**: Mamba-2's secret sauceâ€”diagonal blocks via dense matmul, off-diagonal via low-rank
- **HSS Matrices** + **Telescopic Decomposition**: Hierarchical low-rank structure for O(n) matrix function evaluation
- **Column-Sparse Transition Matrices**: Permutation-routing in SSMs while staying O(N) per-step
- **Group-and-Shuffle** + **Monarch Matrices**: Hardware-friendly structured matrices via block-diagonal + permutation

**Stability & Expressivity Unlocks**:
- **Oscillatory Eigenvalue Stability**: Second-order ODE discretization guarantees |Î»|â‰¤1 *by construction*â€”no constraints needed
- **Negative Eigenvalue Extension**: Simple 2Ã— multiplier unlocks NCÂ¹ expressivity for DeltaNet
- **Perturb-Then-Diagonalize**: Solves HiPPO's exponential ill-conditioning without discarding structure
- **Cayley Contractive Parameterization**: Skew-symmetric â†’ orthogonal map for RNN weights

**Parallelization Foundations**:
- **Segmented Scan**: Variable-length batching without padding via operator transformation
- **Blelloch Work-Efficient Scan**: O(n) work, O(log n) depthâ€”the gold standard for parallel prefix sums
- **Chunkwise Parallel Scan**: SSM workhorse for training parallelism

### ðŸ”¬ Other Proposals

- **Column-Sparse Negative-Eigenvalue DeltaNet** (Priority: HIGH, cost <$7): Combines PD-SSM's permutation routing with negative eigenvalues for non-solvable group simulation. Theoretically compelling but narrow use case (state tracking tasks).

- **DPLR Column-Sparse SSM** (Priority: MEDIUM, cost <$8): Wraps column-sparse matrix in DPLR structure to keep Cauchy kernel trick. Clever but uncertain if expressivity gains survive the DPLR projectionâ€”needs ablation studies.

- **Segmented-HSS Linear Attention** (Priority: MEDIUM, cost <$10): Hierarchical attention via HSS matrices + segmented scan for variable-length batching. O(n log n) complexity is attractive, but HSS overhead might dominate for small n. Best for very long sequences (n > 8K).

### ðŸŽ¯ Strategic Insights

**Three convergent themes** point to immediate research opportunities:

1. **Stability-by-construction is ready**: Oscillatory discretization eliminates the eigenvalue constraint problem that plagues S4/S5. Combined with DPLR (proposal #4), this is a **low-risk, high-reward** path to more robust SSMs. Start here.

2. **Delta rule needs acceleration**: DeltaNet's associative memory dominates linear attention, but training is slow. The WYâ†’semiseparable reformulation (proposal #2) directly targets this bottleneck with proven components. Second priority after oscillatory-DPLR.

3. **Hardware primitives unlock small-budget experiments**: The kernel tricks (IO-aware tiling, 2:4 sparsity, kernel fusion) mean all five proposals can be validated for **<$10 each**. The theoretical foundations are solidâ€”now it's cheap to test them empirically.

**Recommended focus order** (optimizing for cost-effectiveness + impact):
1. **Oscillatory-DPLR SSM** (~$4, highest impact/cost ratio)
2. **SSD-DeltaNet** (~$6, addresses clear bottleneck)
3. **Column-Sparse Negative-Eigenvalue DeltaNet** (~$7, if state-tracking tasks are priority)
4. Remaining two (both ~$8-10, more speculative)

The documentation quality is exceptionalâ€”every trick includes mathematical form, source papers, and clear categorization. This systematic knowledge base is paying dividends: proposals now combine 4-5 tricks in novel ways with clear theoretical justification.




Let me first examine the experiment code and results to understand what's been implemented and any findings.Explore the experiment at /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002. Read the README fully, any Python files, any results/logs/metrics files. I need to understand: what was implemented, what results were obtained (if any), what the training cost was, and whether the experiment validated its hypothesis. Be very thorough - read all files in the directory.Explore the experiment at /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001. Read the README, any Python files, any results/logs/metrics files. I need to understand: what was implemented, what results were obtained (if any), what the training cost was, and whether the experiment validated its hypothesis. Be very thorough - read all files in the directory.Now let me also quickly check the proposals for more detail on cost estimates and feasibility:Now I have enough information to generate a comprehensive log entry. Let me write it: