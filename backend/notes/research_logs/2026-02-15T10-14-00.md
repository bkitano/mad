---
title: Research Log Update
date: 2026-02-15T10:14:00
timestamp: 2026-02-15 10:14:00
tricks: 1
proposals: 7
experiments: 4
source: legacy_migration
---

## 2026-02-15 â€” 10:14 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention & SSM Readout** (Proposal 009) â€” Priority: **HIGH**
- **Hypothesis**: Applying an input-dependent sigmoid gate to the readout path of linear attention/SSMs breaks the low-rank bottleneck at negligible cost.
- **Why it matters**: This is the single most "bang-for-buck" idea in the batch. It's orthogonal to every other proposal â€” a 2-line code change (linear projection + sigmoid + elementwise multiply) that can compose with any SSM or linear attention variant. The argument is well-grounded: Qiu et al. (NeurIPS 2025 Best Paper) showed this works for softmax attention, and the theoretical case for *larger* gains in linear attention is compelling â€” the readout bottleneck is strictly worse when there's no softmax nonlinearity. Nobody has tried this transfer.
- **Estimated cost**: MVE **~$0.50** (10 min single GPU); full validation ~$32
- **Impact score**: **9/10** â€” Trivial to implement, near-zero risk of wasted effort, composable with everything, and targets a bottleneck nobody else is addressing. If it fails, you learn something important about where the bottleneck actually is.

**2. Circulant SSM: Fourier-Domain Parallel Scan** (Proposal 013) â€” Priority: **HIGH**
- **Hypothesis**: Circulant state transitions become diagonal in Fourier domain, enabling element-wise parallel scans with full coordinate mixing at O(n log n) cost.
- **Why it matters**: This is the most *elegant* proposal â€” it resolves the core SSM design tension (diagonal=fast but no mixing vs. dense=expressive but slow) with a clean mathematical trick. The FFT diagonalization of circulant matrices is textbook, but nobody has applied it to input-dependent SSM state transitions with parallel scan. The result: Mamba-like parallel scan depth O(log T) with full coordinate mixing, at only a log(n) overhead. The cyclic group composition task is a perfect first test â€” circulant structure is literally the algebra of cyclic convolutions.
- **Estimated cost**: MVE **~$0.25** (5 min single GPU); full validation ~$8
- **Impact score**: **8.5/10** â€” Cheapest MVE of any proposal, mathematically principled, and tests on the most favorable task first. The key risk is the commutativity limitation (circulant matrices commute, limiting non-cyclic state tracking), but this is known upfront and block-circulant variants are a ready follow-up.

### ðŸ§ª Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: **DEBUG** âŒ)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Implemented tiny model (129 params, 1 layer). The parameterization is correct â€” learned Ï‰ and Î¶ values land in the right physical range. But the model cannot learn at all: flat loss curve at MSE ~0.85 across 50 epochs.
  - **Key findings**: Implementation bug suspected (complex dtype handling, discretization, or gradient flow). The oscillatory parameterization concept isn't invalidated, but the forward pass needs debugging before scaling. **Cost: $0.00** (CPU-only, 27 min).
  - **Action**: Fix implementation before revisiting. Don't scale up.

- **Experiment 003: OscGate-SSM** (Status: **PROCEED** âœ…)
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Progress**: Three models tested on selective copying (8 tokens, 8 queries, vocab=16). Required 3 attempts with architecture scaling (d=128, 2 layers, MLP head) to achieve success.
  - **Key findings**: **Core hypothesis validated.** OscGate-SSM achieves **93.0% accuracy** (target >90%) while LinOSS (LTI baseline) gets only **46.8%** â€” a 46pp gap proving input-dependent oscillatory parameters enable selectivity. The stability guarantee held perfectly (0 NaN/Inf). Competitive with unconstrained diagonal SSM (94.8%). Speed: 1.8x overhead (acceptable). **Cost: $0.00** (CPU-only, 25 min).
  - **Caveat**: LinOSS scored 46.8% vs target <40% â€” slightly higher than expected, but the directional evidence is overwhelming.

- **Experiment 004: Displacement-Rank SSM** (Status: **ABANDONED** âŒ)
  - **Proposal**: 022-displacement-rank-ssm-state-transitions
  - **Progress**: Tested Î± âˆˆ {0,1,2,4,16} on S5 permutation composition. At seq_len=12 (easy), Î±=1 and Î±=4 both hit 95.8% â€” tied, failing the monotonic rank-scaling hypothesis. At seq_len=20 (hard), **all Cauchy models collapse** (1-7% accuracy) while dense SSM achieves 97.2%.
  - **Key findings**: **Displacement rank framework is fundamentally flawed for SSM state transitions.** The Cauchy kernel structure 1/(s_i - s_j) creates ill-conditioned gradients and optimization barriers. Theoretical expressivity does not translate to learnability. Dense works; Cauchy does not. **Cost: $0.00** (CPU-only, 15 min).
  - **Lesson learned**: Structured matrix parameterizations must be validated for *optimization properties*, not just theoretical capacity. This should inform evaluation of other structured transition proposals.

- **Experiment 001**: Status: **implemented** (no results yet)

### ðŸ“š New Discoveries (128 tricks documented)

The 128 new tricks span 6 categories. Key highlights by theme:

- **Structured Matrix Zoo** (circulant, HSS, semiseparable): Massive documentation of circulant variants (block-circulant, g-circulant, DCT-DST circulant, CSCS splitting, Î±-circulant), HSS hierarchies (compression, ULV solver, SuperDC eigensolver, telescopic decomposition), and Cauchy/Toeplitz connections. This provides the theoretical foundation for proposals 013, 021, 023.

- **Permutation Learning**: 7+ tricks for differentiable permutation optimization â€” Sinkhorn relaxation, Gumbel-Softmax, OT4P orthogonal relaxation, ShuffleSoftSort, auction algorithm, STE-based learning (STEAM), bipartite matching. Critical infrastructure for proposals involving learned channel permutations (010, 024).

- **GPU Kernel Optimization**: FlashInfer JIT fusion, persistent megakernel fusion, warp specialization, Stream-K GEMM, CTA tile swizzling, horizontal fusion, FlashFuser DSM. These represent the implementation toolkit for making any winning architecture *actually fast*.

- **Tropical & Alternative Semirings**: Tropical attention via Hilbert projective metric, SIMDÂ² semiring matrix acceleration, semiring monoid lifting. Foundation for proposals 014 and 015.

- **Sparsity Acceleration**: 2:4 structured sparsity, V:N:M hierarchical sparsity, transposable N:M masks, S-STE continuous pruning, PA-DST permutation-augmented sparsity. Foundation for proposals 010 and 024.

### Other Proposals

| # | Proposal | Cost (MVE) | Key Idea | Notes |
|---|----------|-----------|----------|-------|
| 006 | Monarch-Gated State Transition | ~$0.50 | Monarch-factored input-dependent transitions at O(nâˆšn) | Strong but sequential BMM within chunks is a concern |
| 007 | OscGate-SSM | ~$0.50 | Input-dependent Ï‰,Î¶ with stability guarantee | **Already validated** â€” proceed to scaling |
| 015 | Tropical-Gated SSM | ~$0.25 (MVE) | Max-plus scan for winner-take-all memory | Fascinating but MVE should test MQAR, not full LM |
| 014 | Log-Semiring SSM | ~$0.50 | Logsumexp scan for softmax-native SSM | Smooth cousin of 015; test both |
| 016 | GS-Monomial SSM | ~$0.50 | Group-and-Shuffle monomial state transitions | Novel but complex implementation |
| 017 | Hyperoctahedral Signed-Perm SSM | ~$0.50 | Signed permutation transitions via B_n | Elegant algebra, unclear practical benefit over Monarch |
| 001 | Column-Sparse Neg-Eigenvalue DeltaNet | ~$0.50 | Combine PD-SSM + negative eigenvalues | Compositional but needs debugging from exp 002 insights |
| 002 | SSD-DeltaNet WY Hybrid | ~$0.50 | Recast DeltaNet as block-semiseparable | Important engineering proposal |
| 019 | Capacitance-Coupled Multi-Scale SSM | ~$0.50 | Cross-scale coupling via capacitance matrix | Interesting architecture-level idea |
| 020 | Oscillatory Householder DeltaProduct | ~$0.50 | Decompose state into oscillatory + reflective | Ambitious composition of 007 + DeltaProduct |
| 026 | Cyclic Reduction for Dense SSM | ~$0.50 | Block-bidiagonal cyclic reduction scan | Practical for non-diagonal SSMs |
| 008 | cos-LogLinear Attention | ~$0.50 | cosFormer + log-linear hierarchical states | Solid combination, medium novelty |
| 011 | Neumann Resolvent Chunkwise DPLR | ~$0.50 | Replace Woodbury inverse with Neumann series | Clever numerics, incremental impact |
| 012 | Expert-Choice Monarch SSM | ~$0.50 | MoE routing for SSM state heads | Novel but adds routing complexity |
| 025 | NystrÃ¶m Landmark Chunkwise SSM | ~$0.50 | Compress inter-chunk state transfer | Addresses a real bottleneck |
| 024 | 2:4 Sparse SSM via S-STE + Sinkhorn | ~$0.50 | Sparse Tensor Core SSM training | Hardware-dependent, needs NVIDIA GPU |
| 010 | Sparse Monarch SSM (PA-DST) | ~$0.50 | 2:4 sparse Monarch + permutation recovery | Combines well but complex stack |
| 021 | Black-Box HSS Telescopic Attention | ~$0.50 | Adaptive hierarchical linear attention via HSS | Most theoretical; needs careful implementation |
| 003 | DPLR Column-Sparse SSM | ~$0.50 | Cauchy kernel + column-sparse permutation | **Likely impacted by Exp 004 abandonment** |
| 004 | Oscillatory-DPLR SSM | ~$0.50 | Oscillatory eigenvalues + DPLR | **Exp 002 shows implementation needs debugging** |
| 005 | Segmented-HSS Linear Attention | ~$0.50 | Variable-length HSS attention | Medium priority |
| 018 | Hutchinson Adaptive Rank DPLR | ~$0.50 | Trace-guided dynamic rank allocation | Clever diagnostics tool, not a model |

### Strategic Insights

**The displacement-rank failure (Exp 004) is highly informative.** It shows that elegant structured matrix parameterizations can have fatal optimization barriers â€” the Cauchy kernel 1/(s_i - s_j) creates ill-conditioned gradients that prevent learning even when the capacity is sufficient. **This should raise caution for Proposal 003 (DPLR Column-Sparse)**, which uses the same Cauchy machinery. Proposals that rely on FFT-based diagonalization (013, 023) or simple element-wise operations (009, 015) are structurally safer for optimization.

**The OscGate-SSM success (Exp 003) validates a key principle**: stability-by-construction + input-dependent gating is achievable simultaneously. This opens the door to Proposal 020 (Oscillatory Householder DeltaProduct) as a natural next step, and confirms that the oscillatory parameterization is viable when properly implemented (unlike Exp 002's buggy forward pass).

**Recommended priority ordering for next experiments** (all under $1):
1. **Proposal 009** (Post-Sigmoid Gating) â€” $0.50, highest ratio of potential insight to effort
2. **Proposal 013** (Circulant SSM) â€” $0.25, cleanest mathematical test
3. **Proposal 015** (Tropical-Gated SSM) â€” $0.25, tests a radically different semiring
4. **Proposal 006** (Monarch-Gated SSM) â€” $0.50, validates the leading structured transition approach
5. **Fix Experiment 002** â€” $0.00, unblock the oscillatory-DPLR line before proposing further compositions

The emerging theme is a **two-track research strategy**: (a) *readout improvements* (009) that compose with anything, and (b) *structured transition exploration* (013, 006, 015) seeking the right point on the expressivity-parallelism tradeoff. Track (a) is low-risk and should run immediately; track (b) should prioritize proposals with the simplest optimization landscapes (circulant > Monarch > Cauchy).




Let me first read the experiment results in detail to provide accurate assessments.Read the following files and return their FULL contents. I need every detail from the results files:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/002-results.md (or similar path)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/003-results.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/004-results.md

Also check these alternate paths:
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/RESULTS.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/003/RESULTS.md  
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/RESULTS.md

Find and read whichever paths exist. Return the full text of all results files.Now let me generate the comprehensive log entry: