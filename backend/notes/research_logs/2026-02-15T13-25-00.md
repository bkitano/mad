---
title: Research Log Update
date: 2026-02-15T13:25:00
timestamp: 2026-02-15 13:25:00
tricks: 1
proposals: 4
experiments: 7
source: legacy_migration
---

## 2026-02-15 â€” 13:25 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention / SSM Readout** (Proposal 009) â€” Priority: **HIGH**
- **Hypothesis**: Applying the NeurIPS 2025 Best Paper technique (post-attention sigmoid gating) to *linear attention and SSM readout* breaks the low-rank output bottleneck, yielding 5â€“15% perplexity improvement at <2% latency cost.
- **Why it matters**: This is the lowest-hanging fruit in the entire batch. It's a single elementwise sigmoid gate insertion â€” no new math, no new kernels, no custom CUDA. The theoretical rationale is strong: linear attention / SSMs suffer from a *more severe* information bottleneck than softmax attention (compressed state â†’ readout), so the multiplicative nonlinearity should help *more* here than in the softmax case where it already won Best Paper. Dead simple to A/B test on any existing SSM codebase.
- **Estimated cost**: **<$2** â€” modify 5 lines in an existing Mamba/GLA training script, run on a small LM task (WikiText-103 or similar).
- **Impact score**: **9/10** â€” Near-zero implementation risk, strong theoretical backing, immediately applicable to production models. The only question is magnitude of improvement.

**2. Circulant SSM: Fourier-Domain Parallel Scan** (Proposal 013) â€” Priority: **HIGH**
- **Hypothesis**: Block-circulant state transitions, diagonalized via FFT, enable element-wise parallel scans in frequency space â€” recovering diagonal-SSM parallelism with dense-transition expressivity at O(n log n) per step.
- **Why it matters**: This is the most elegant solution to the diagonal-vs-dense SSM tradeoff seen so far. The key insight is that circulant matrices are *simultaneously* dense (full coordinate mixing) and diagonalizable (O(n log n) via FFT), and this diagonalization commutes perfectly with parallel scan. Unlike DeltaProduct (sequential Householder products) or Monarch SSMs (multi-factor products), circulant transitions compose cleanly in the Fourier domain: `FFT(Câ‚ Â· Câ‚‚) = FFT(Câ‚) âŠ™ FFT(Câ‚‚)`. This means the scan operator is just elementwise complex multiply â€” identical cost to diagonal SSMs.
- **Estimated cost**: **<$5** â€” implement circulant transition layer, test on MQAR + S5 permutation composition benchmarks.
- **Impact score**: **8.5/10** â€” Could be a genuine architectural breakthrough. Risk: circulant matrices may be too structured (they're normal matrices, so no non-normal dynamics). Reward: if it works on S5, it's strictly better than diagonal SSMs at negligible extra cost.


### ðŸ§ª Experiment Updates

- **Experiment 007: OscGate-SSM** (Status: âœ… **COMPLETED â€” PROCEED**)
  - **Proposal**: 007 â€” Oscillatory-Gated Selective SSM
  - **Key finding**: **Core hypothesis validated.** OscGate-SSM achieves 93.0% on selective copying (vs. LinOSS 46.8%), proving that input-dependent oscillatory parameters enable selectivity while preserving stability-by-construction. 46.2pp gap is decisive. Speed overhead only 1.8Ã— vs diagonal SSM.
  - **Cost**: $0.00 (CPU only, ~25 min)
  - **Next**: Scale to language modeling. The stability guarantee + selectivity combination is novel and valuable.

- **Experiment 026: Cyclic Reduction vs Prefix Scan** (Status: âœ… **COMPLETED â€” PROCEED**)
  - **Proposal**: 026 â€” Cyclic Reduction for Dense SSM Recurrences
  - **Key finding**: **3.88Ã— CPU speedup at T=1024, 6.01Ã— GEMM reduction.** Numerical accuracy at machine epsilon. Speedup grows monotonically with sequence length. This is a pure algorithmic win â€” fewer matrix multiplies, same answer.
  - **Cost**: $0.00 (CPU only, ~3 min)
  - **Next**: GPU validation (Experiment 026_scaled planned, est. $18â€“24). The CPU results strongly suggest GPU speedups will hold, but need to verify tensor-core utilization patterns.

- **Experiment 022: Displacement-Rank SSM** (Status: âŒ **COMPLETED â€” ABANDON**)
  - **Proposal**: 022 â€” Displacement-Rank SSM State Transitions
  - **Key finding**: **Cauchy-like matrices are a dead end for SSM transitions.** Î±=4 doesn't outperform Î±=1 (both 95.8%), Cauchy matvec throughput only 0.20Ã— dense (target was >0.3Ã—), and the 1/(sáµ¢ - sâ±¼) terms create ill-conditioned gradients. Dense matrices win at Î±=16 (97.4%) with simpler code.
  - **Cost**: $0.00 (CPU only, ~15 min)
  - **Lesson learned**: Displacement rank theory is elegant but the Cauchy parameterization introduces optimization barriers that negate the structural efficiency gains.

- **Experiment 004: Oscillatory-DPLR SSM** (Status: ðŸ› **COMPLETED â€” DEBUG**)
  - **Proposal**: 004 â€” Oscillatory-DPLR SSM
  - **Key finding**: Complete training failure (loss stuck at 0.854). Parameterization is correct (learned Ï‰ in valid range), but model can't fit training data. Suspected forward-pass bug in complex dtype handling.
  - **Cost**: $0.00 (CPU only, ~27 min)

- **Experiment 015: Tropical-Gated SSM** (Status: ðŸ”§ **IMPLEMENTED** â€” awaiting run)
- **Experiment 020: OH-DeltaProduct** (Status: ðŸ”§ **IMPLEMENTED** â€” awaiting run)
- **Experiment 026_scaled: Cyclic Reduction GPU** (Status: ðŸ”§ **IMPLEMENTED** â€” awaiting run, est. $18â€“24)


### ðŸ“š New Discoveries (162 tricks documented)

A massive documentation sprint covering the full stack from algebraic foundations to GPU kernel optimization. Key clusters:

- **Circulant/Toeplitz machinery** (Tricks 004, 013, 016, 018, 024, 028, 029, 032, 038, 067, 079, 084, 100, 119, 129): Complete toolkit for FFT-based structured matrix layers. The Toeplitzâ†’Circulant embedding (129) and CSCS splitting (032) are particularly actionable â€” they turn any Toeplitz-structured computation into O(n log n) FFT operations.

- **HSS/hierarchical matrix framework** (Tricks 001, 008, 043, 052, 054, 059, 060, 063, 088, 097, 098, 102, 122, 123, 127, 131, 138, 146): An industrial-strength library of hierarchical matrix algorithms. The black-box HSS compression (008) is the gateway drug â€” it needs only matrix-vector products, no matrix entries.

- **Permutation learning & N:M sparsity** (Tricks 003, 006, 007, 017, 058, 070, 071, 085, 087, 089, 110, 114, 115, 120, 130, 133, 136, 140, plus nmSPARSE, MaskLLM): Comprehensive coverage of the sparsity-permutation co-design space. The V:N:M hierarchical sparsity (140) enabling >50% structured sparsity with Sparse Tensor Core acceleration is the standout for practical deployment.

- **GPU kernel optimization** (Tricks 011, 025, 033, 039, 046, 047, 049, 050, 051, 061, 068, 075, 091, 093, 103, 104, 121, 126, 135, 141): Deep coverage of kernel fusion techniques. The warp-specialized pipelining (141, from FlashAttention-3) and EVT fusion (039) are the most immediately useful for building fast SSM implementations.

- **Semiring generalization** (Tricks 108, 113, 132): The theoretical foundation for tropical and log-semiring SSMs. SIMDÂ² (113) is notable â€” it shows that 8 additional semiring operations can be hardware-accelerated with only 5% chip area overhead, making non-standard semiring SSMs a realistic hardware target.


### Other Proposals (ranked by cost-effectiveness)

| Rank | Proposal | Core Idea | Est. Cost | Notes |
|------|----------|-----------|-----------|-------|
| 3 | **014 â€” Log-Semiring SSM** | logsumexp scan = native softmax attention in SSM form | <$5 | Elegant theory; online-softmax trick makes it numerically tractable |
| 4 | **015 â€” Tropical-Gated SSM** | max-plus scan with annealing | <$5 | Already implemented (Exp 015), just needs running |
| 5 | **006 â€” Monarch-Gated SSM** | Monarch-factored input-dependent transitions | <$5 | BMM-native, good GPU utilization story |
| 6 | **016 â€” GS-Monomial SSM** | Block-diagonal monomial + shuffle | <$5 | O(nâˆšn) with only 2 factors, clean algebraic closure |
| 7 | **023 â€” Circulant-Diagonal SSM** | CD product transitions, compose in Fourier domain | <$5 | Similar spirit to 013 but with diagonal modulation |
| 8 | **027 â€” Cayley Circulant-Diagonal Orthogonal SSM** | Cayley(skew-circulant-diagonal) = exact orthogonal | <$5 | Stability guaranteed + O(n log n) |
| 9 | **020 â€” OH-DeltaProduct** | Oscillatory + Householder decomposition | <$3 | Already implemented (Exp 020), just needs running |
| 10 | **033 â€” EVT-Fused SSM Epilogues** | Fuse SwiGLU/gating into GEMM epilogues | <$8 | Pure systems optimization, guaranteed speedup if implemented |
| â€” | **019 â€” Capacitance-Coupled Multi-Scale SSM** | Multi-timescale SSMs coupled via capacitance matrix | <$5 | Interesting but complex; high implementation risk |
| â€” | **032 â€” Chimera-Fused Chunkwise SSM** | GEMM-chain fusion for chunkwise scan | >$10 | Requires CUTLASS expertise; high payoff but high effort |
| â€” | **026 scaled â€” Cyclic Reduction GPU** | GPU validation of CR speedup | ~$20 | Exceeds budget but high-confidence payoff based on CPU results |


### Strategic Insights

**The most important result today is negative**: Experiment 022 definitively kills the displacement-rank / Cauchy-like SSM direction. The ill-conditioned gradients from 1/(sáµ¢ - sâ±¼) terms are a fundamental barrier, not a tuning problem. This redirects attention toward **circulant** (Proposal 013) and **Monarch** (Proposal 006) factorizations as the winning structured alternatives to diagonal SSMs â€” both avoid the Cauchy pitfall while achieving similar expressivity goals.

**The biggest opportunity is Proposal 009 (post-sigmoid gating)**: It requires almost zero implementation effort and targets a known bottleneck (linear attention's output information bottleneck). If it delivers even 5% perplexity improvement, it's immediately publishable and deployable. **Run this first.**

**Emerging theme â€” "circulant everything"**: With 15+ circulant tricks now documented, a coherent architecture is crystallizing: circulant state transitions (Proposal 013), circulant FAVOR+ projections (Proposal 029), circulant fine-tuning adapters (CDVFT, Trick 024), and circulant-diagonal flows (CDFlow, Trick 023). The FFT-native computational model means all these components compose cleanly. Consider a unified "CirSSM" architecture that uses circulant structure end-to-end.

**Immediate action items** (total budget ~$15):
1. **Run Proposal 009** (post-sigmoid gating, ~$2) â€” highest expected value per dollar
2. **Run Experiment 015** (Tropical SSM, already implemented, ~$1)
3. **Run Experiment 020** (OH-DeltaProduct, already implemented, ~$1)
4. **Implement & run Proposal 013** (Circulant SSM, ~$5) â€” the most promising new architecture
5. **Debug Experiment 004** (Osc-DPLR, $0) â€” likely a simple complex-dtype bug




I'll analyze the recent activity systematically - first examining the experiment results, then the proposals, and finally the tricks to generate a comprehensive log entry.Read the following experiment result files and return their complete contents:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/022/results.md (or any results file in that directory)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/results.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/026/results.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/007/results.md

Also check for results files in these experiment directories:
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/014/
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/015/
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/028/
8. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/031/

Look for files named results.md, RESULTS.md, or any file containing results in these directories. Return the full content of each file found.Read the following proposal files and return their complete contents (especially the hypothesis, estimated cost, and success criteria sections):

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/006-monarch-gated-state-transition.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/036-near-far-field-chunkwise-gla.md

Focus on extracting: hypothesis, estimated compute cost, MVE design, and success criteria from each.Now let me check the experiment log for any additional insights:Now I have comprehensive data. Let me generate the log entry: