---
title: Research Log Update
date: 2026-02-15T11:25:00
timestamp: 2026-02-15 11:25:00
tricks: 1
proposals: 16
experiments: 4
source: legacy_migration
---

## 2026-02-15 â€” 11:25 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention and SSM Readout (Proposal 009)** (Priority: **high**)
- **Hypothesis**: Applying post-readout sigmoid gating â€” the NeurIPS 2025 Best Paper technique â€” to linear attention/SSM output projections will break the low-rank bottleneck, improving quality 5â€“15% with <2% latency overhead.
- **Why it matters**: This is the lowest-hanging fruit in the entire batch. It requires zero architectural change to the core recurrence â€” just adding a single sigmoid gate after the output projection. The technique is already validated on softmax attention (Best Paper!), and the argument that linear attention suffers *more* from the low-rank bottleneck (since it lacks softmax's implicit nonlinearity) is compelling. Implementation is trivial: one extra `sigmoid(linear(x))` element-wise multiply.
- **Estimated cost**: **<$2** â€” can be tested on any existing SSM/linear attention codebase with a 3-line code change and a small language modeling or associative recall run.
- **Impact score**: **9/10** â€” Exceptional cost-effectiveness. Near-zero risk (worst case: 2% overhead, no quality gain). Best case: a universal quality boost for all linear-complexity attention variants. Should be the very first thing tested.

**2. Circulant SSM: Fourier-Domain Parallel Scan (Proposal 013)** (Priority: **high**)
- **Hypothesis**: Block-circulant state transitions diagonalize via FFT, enabling element-wise parallel scans in frequency space â€” recovering diagonal-SSM parallelism with dense-transition expressivity at O(n log n) per step.
- **Why it matters**: This elegantly sidesteps the core SSM tradeoff (diagonal = fast but limited mixing, dense = expressive but O(nÂ³) scan). The circulant structure is the sweet spot: full coordinate mixing, FFT-diagonalizable, and the composition of circulant matrices stays circulant (closure!). Training reduces to n independent scalar scans in the Fourier domain. The trick database now has ~15 circulant-related entries providing deep implementation guidance.
- **Estimated cost**: **<$5** â€” Small model (d=128, n=64) on selective copying + S5 permutation composition benchmarks. The FFT operations are native PyTorch.
- **Impact score**: **8.5/10** â€” Strong theoretical grounding, clean implementation path, and directly addresses the expressivity-efficiency gap validated by Experiment 004 (where dense beat everything on hard tasks but diagonal failed).

### ðŸ§ª Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: **completed â€” FAILED**)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Full implementation on CPU, 50 epochs, ~27 min runtime, $0.00 cost
  - **Key findings**: Complete training failure â€” MSE stuck at 0.854 across all 50 epochs (target was <0.001). However, the learned oscillatory parameters (Ï‰, Î¶) were in correct ranges, suggesting the parameterization works but the forward pass has an implementation bug. **Verdict: DEBUG** â€” fix implementation before drawing architectural conclusions. The oscillatory parameterization idea is not invalidated.
  - **Cost**: $0.00 actual vs ~$0.40 estimated

- **Experiment 003: OscGate-SSM** (Status: **completed â€” SUCCESS** âœ…)
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Progress**: 3 models trained on selective copying task, 100 epochs each, ~25 min total
  - **Key findings**: **Core hypothesis validated.** OscGate-SSM achieved **93.0% accuracy** vs LinOSS (LTI baseline) at **46.8%** â€” a 46 pp gap proving input-dependent oscillatory parameters enable selectivity. Stability guarantee held perfectly (zero NaN events). Nearly matched unconstrained diagonal SSM (94.8%). Speed overhead only 1.8Ã—.
  - **Cost**: $0.00 actual vs ~$0.40 estimated
  - **Next**: Scale to MQAR with 8 layers, d=512 (~50M params) for real-world validation

- **Experiment 004: Displacement-Rank SSM** (Status: **completed â€” ABANDONED** âŒ)
  - **Proposal**: 022-displacement-rank-ssm-state-transitions
  - **Progress**: Tested Î± âˆˆ {0, 1, 2, 4, 16} on S5 permutation composition at seq_len=12 and seq_len=20
  - **Key findings**: **Kill criterion triggered.** Î±=4 did NOT outperform Î±=1 (both 95.8% on easy task). On the hard task (seq_len=20), all Cauchy-structured models failed completely (<4%) while dense achieved 97.2%. The Cauchy kernel's 1/(s_i - s_j) terms create optimization barriers: ill-conditioned gradients, NaN without normalization, and generators that collapse to near-zero with normalization. **Critical lesson: theoretical expressivity â‰  practical learnability.**
  - **Cost**: $0.00 actual vs ~$0.40 estimated

- **Experiment 001** (Status: **implemented** â€” no results yet)

### ðŸ“š New Discoveries

The 140 new tricks documented represent a massive knowledge expansion across 6 themes:

- **Circulant/Structured Matrix Universe** (~30 tricks): An extraordinary depth of circulant matrix technology â€” block-circulant FFT layers, g-circulant DCT/DST variants, circulant cycle decomposition, CSCS splitting, optimal circulant approximation, block-circulant quantization, CDFlow invertible layers. These collectively provide a complete toolkit for building circulant-based neural network layers from scratch.

- **HSS/Hierarchical Matrices** (~15 tricks): Full coverage of hierarchically semiseparable matrix algorithms â€” randomized compression, ULV factorization, telescopic decomposition, SuperDC eigensolvers, parallel algorithms. These enable O(n) to O(n log n) operations on structured matrices that would otherwise require O(nÂ²â€“nÂ³).

- **GPU Kernel Engineering** (~20 tricks): FlashInfer JIT fusion, persistent megakernel fusion, DSM inter-core fusion (FlashFuser), warp-specialized pipelining, Stream-K GEMM, CTA tile swizzling, Twill constraint-based optimization. This is a complete GPU optimization playbook.

- **Permutation Learning** (~12 tricks): Sinkhorn acceleration (overrelaxed, Newton-sparse, Îµ-scaling), Birkhoff parameterization, auction algorithms, ShuffleSoftSort, OT4P orthogonal relaxation, STEAM STE-based learning. Critical infrastructure for any proposal involving learned permutations.

- **Sparse Acceleration** (~8 tricks): Transposable N:M masks, V:N:M hierarchical sparsity, Samoyeds dual-side MoE sparsity, S-STE continuous pruning, permutation-augmented structured sparsity. Hardware-aware sparsity is now well-characterized.

- **Tropical & Alternative Semirings**: Tropical attention via Hilbert projective metric and semiring monoid lifting open the door to non-standard algebraic structures in neural computation â€” directly enabling Proposals 014 and 015.

### Other Proposals

- **Proposal 006 (Monarch-Gated SSM)**: Input-dependent Monarch transitions at O(nâˆšn). Strong but overlaps with Circulant SSM (013); test 013 first since circulant is simpler.
- **Proposal 016 (GS-Monomial SSM)**: Group-and-Shuffle monomial state transitions. Elegant but complex implementation.
- **Proposal 015 (Tropical-Gated SSM)**: Max-plus semiring parallel scan. Novel but exotic â€” needs SIMDÂ² hardware for full benefit.
- **Proposal 014 (Log-Semiring SSM)**: LogSumExp scan for softmax-native recurrence. Theoretically beautiful, builds on online-softmax trick. Medium priority â€” test after 009/013.
- **Proposal 027 (Cayley-Circulant Orthogonal SSM)**: Cayley transform of skew-circulant for exact orthogonality + FFT speed. Elegant synthesis but overlaps with 013.
- **Proposal 026 (Cyclic Reduction SSM)**: Alternative scan algorithm for non-diagonal transitions. Niche â€” only matters if Monarch/circulant SSMs succeed first.
- **Proposal 001 (Column-Sparse Negative-Eigenvalue DeltaNet)**: Combines two proven tricks. Solid but incremental.
- **Proposal 002 (SSD-DeltaNet)**: Block-semiseparable DeltaNet via WY. High potential but complex implementation.
- **Proposal 008 (cos-LogLinear)**: Cosine reweighting + log-linear attention. Clean composition, medium impact.
- **Proposal 010 (Sparse Monarch SSM)**: 2:4 sparsity on Monarch factors. Depends on Monarch SSM working first.
- **Proposal 024 (2:4 Sparse SSM via S-STE)**: Sparse state transitions with Sinkhorn permutation. Requires Monarch/circulant SSM as prerequisite.
- **Proposal 030 (Group-Matrix Displacement Rank SSM)**: Hyperoctahedral group matrices. Interesting algebra but Experiment 004 shows displacement-rank approach has optimization issues.
- **Proposals 003, 005, 018**: Medium priority â€” either incremental or dependent on prerequisite results.

### Strategic Insights

**The most important lesson from today's experiments is that optimization matters more than expressivity.** Experiment 004 definitively showed that Cauchy-like matrices â€” despite theoretically spanning all matrices at sufficient rank â€” cannot be trained effectively due to gradient pathologies from 1/(s_i - s_j) terms. Meanwhile, Experiment 003 showed that a relatively simple modification (making oscillatory parameters input-dependent) works immediately with clean gradients. **Favor architectures with clean optimization landscapes over theoretically elegant but ill-conditioned ones.**

**The researcher should execute proposals in this order: 009 â†’ 013 â†’ 007-scale-up â†’ 006.** Proposal 009 (post-sigmoid gating) is a near-free lunch that can be tested in hours for <$2. Proposal 013 (circulant SSM) is the most promising new architecture â€” it directly addresses the lesson from Experiment 004 (circulant matrices have clean FFT-based gradients unlike Cauchy) while providing the coordinate mixing that diagonal SSMs lack. The OscGate-SSM (007) success should be scaled up to validate on harder benchmarks. Monarch-gated SSM (006) is the backup if circulant SSMs hit unexpected issues.

**The circulant matrix ecosystem is now remarkably complete.** With 30+ circulant-related tricks spanning decomposition, GPU kernels, quantization, and training stability, the researcher has all the building blocks needed to implement circulant SSMs without reinventing any wheel. This concentration of knowledge around a single algebraic structure is a strong signal that circulant-based architectures should be the primary research direction.