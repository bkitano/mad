---
title: Research Log Update
date: 2026-02-15T14:08:00
timestamp: 2026-02-15 14:08:00
tricks: 1
proposals: 4
experiments: 5
source: legacy_migration
---

## 2026-02-15 â€” 14:08 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention / SSM Readout** (Proposal 009) â€” Priority: **HIGH**
- **Hypothesis**: Applying post-readout sigmoid gating (NeurIPS 2025 Best Paper trick) to linear attention/SSM outputs breaks the low-rank bottleneck, improving quality 5â€“15% with <2% latency overhead.
- **Why it matters**: This is the *simplest* possible experiment â€” it's a one-line architectural change (multiply attention output by sigmoid gate) applied to an existing limitation everyone agrees on (the VÂ·W_O low-rank bottleneck). The paper already proved it works for softmax attention; the question is whether it helps *more* for linear attention, which suffers from a worse bottleneck. Negative results are informative too: if gating doesn't help, the bottleneck in linear attention is elsewhere.
- **Estimated cost**: ~$0.50 MVE (10 min on 1 GPU)
- **Impact score**: 9/10 â€” Trivial to implement, cheap to test, and if successful, immediately applicable to every GLA/Mamba/linear attention model. The best experiment is one where even the failure mode teaches you something.

**2. Circulant SSM: Fourier-Domain Parallel Scan** (Proposal 013) â€” Priority: **HIGH**
- **Hypothesis**: Circulant state transitions, diagonalized via FFT, enable element-wise parallel scans in frequency space â€” recovering diagonal-SSM parallelism with full coordinate-mixing expressivity at O(n log n) per step.
- **Why it matters**: This elegantly resolves the core tension in SSM design (diagonal = fast but no mixing, dense = mixing but slow). The Z_8 cyclic group task is perfectly tailored â€” circulant matrices *are* cyclic group convolutions, so this tests both mechanism correctness and the hypothesis that circulant structure is a useful inductive bias. Already partially validated: Experiment 013 is implemented and ready to run.
- **Estimated cost**: ~$0.25 MVE (5 min on 1 GPU)
- **Impact score**: 8.5/10 â€” Cheapest possible test of a fundamental architectural idea. If circulant SSMs work on Z_8, the next step (non-abelian groups, real language modeling) is clear.


### ðŸ§ª Experiment Updates

- **Experiment 007 (OscGate-SSM)** â€” Status: âœ… **COMPLETED â€” VALIDATED**
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Key findings**: Input-dependent oscillatory gating works â€” OscGate-SSM hit **93.0%** on selective copying, while fixed-parameter LinOSS managed only **46.8%**. The 46-point gap is overwhelming evidence that content-dependent gating is essential for selective retrieval. Diagonal SSM (94.8%) matched OscGate, suggesting the oscillatory parameterization doesn't *hurt* but doesn't uniquely *help* either â€” the gating mechanism is what matters.
  - **Surprise**: LinOSS at 46.8% exceeded the <40% failure threshold, but only because a larger MLP head provided some memorization capacity. The conclusion stands.
  - **Cost**: $0.00 (CPU only, ~25 min)

- **Experiment 022 (DR-SSM: Displacement-Rank SSM)** â€” Status: âŒ **COMPLETED â€” ABANDONED**
  - **Proposal**: 022-displacement-rank-ssm-state-transitions
  - **Key findings**: **Kill criterion triggered.** Î±=1 already saturated at 95.8% on S5 at seq_len=12, making higher displacement rank (Î±=4) unnecessary. Worse: at the intended seq_len=20, *all* Cauchy-structured models collapsed to chance (<4%), while dense SSM sailed to 97.2%. The 1/(s_i - s_j) Cauchy kernel creates pathological gradient flow â€” this is a fundamental optimization barrier, not a capacity issue.
  - **Lesson learned**: Displacement rank is theoretically elegant but the Cauchy kernel singularities poison gradients. Stick with DPLR/Monarch approaches for structured state transitions.
  - **Cost**: $0.00 (CPU only, ~15 min)

- **Experiment 026 (Cyclic Reduction vs Prefix Scan)** â€” Status: âœ… **COMPLETED â€” VALIDATED**
  - **Proposal**: 026-cyclic-reduction-randmscan-ssm-recurrence
  - **Key findings**: Cyclic reduction achieved **3.88Ã— speedup** over prefix scan at T=1024, n=32 â€” nearly double the 2Ã— target. GEMM count savings: 6.01Ã—. Numerical precision: 8.48e-16 error (exact). Critical insight: once Python loop overhead in back-substitution was eliminated via vectorized indexing, the speedup materialized dramatically (from 1.09Ã— to 3.88Ã—). This validates CR as a drop-in replacement for prefix scan in any non-diagonal SSM.
  - **Cost**: $0.00 (CPU only, ~3 min)

- **Experiment 004 (Oscillatory-DPLR SSM)** â€” Status: âŒ **COMPLETED â€” NEEDS DEBUG**
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Key findings**: Training loss flat-lined (~0.85) over 50 epochs â€” the model did not learn at all. Learned Ï‰ and Î¶ values were in reasonable ranges (interpretability passed), but the model failed to fit even training data. Likely cause: model too small (129 params) for the damped oscillation extrapolation task, or learning rate/optimizer issues.
  - **Cost**: $0.00 (CPU only, ~27 min)

- **Experiments 013, 014, 015, 017, 019, 020, 021, 027, 028, 031** â€” Status: **IMPLEMENTED (awaiting execution)**
  - 10 experiments are coded and ready to run but have no results yet. Notable: Exp 013 (Circulant SSM), Exp 014 (Log-Semiring SSM), and Exp 015 (Tropical-Gated SSM) test the most novel semiring-alternative hypotheses.


### ðŸ“š New Discoveries (164 tricks documented)

The sheer volume (164 tricks in 12 hours) represents a comprehensive literature sweep. Key clusters:

- **Semiring alternatives** (Tricks 108, 113, 132): Tropical semiring, log-semiring, and SIMDÂ² hardware support for non-standard semirings. These provide the theoretical foundation for Proposals 014/015 â€” the idea that matrix multiply isn't the only game in town.

- **Circulant/FFT ecosystem** (Tricks 013, 016, 024, 028, 032, 079, 084, 100, 117, 119, 126, 129): A massive toolbox for FFT-based structured matrices â€” from basic block-circulant FFT to tcFFT (tensor-core FFT), split-FFT for block Toeplitz, and real-arithmetic DCT-DST decompositions. These enable Proposals 013, 023, 027, 029.

- **Householder/WY/CWY accumulation** (Tricks 062, 139, 145, 151, 152, 157): The complete pipeline for accumulating products of Householder reflections efficiently â€” WY representation, compact WY (CWY), UT transform, and Neumann-series CWY inverse. These are the building blocks for DeltaProduct and OH-DeltaProduct (Proposals 020, 028).

- **N:M Sparsity** (Tricks 130, 133, 136, 140, and nmSPARSE): Full stack from 2:4 basics through transposable masks (TSENOR), V:N:M hierarchical sparsity, and conflict-free GPU kernels. Directly enables Proposals 024, 031, 035.

- **Permutation learning** (Tricks 003, 007, 017, 040, 057, 085, 087, 110, 114, 115, 120): Sinkhorn relaxation, Gumbel-Softmax, OT4P orthogonal relaxation, overrelaxed Sinkhorn, SNS acceleration, and STEAM for Monarch permutation learning. Comprehensive toolkit for differentiable discrete optimization.

- **HSS/Hierarchical matrices** (Tricks 001, 008, 052, 054, 059, 060, 063, 088, 097, 098, 122, 123, 127, 131, 138, 146): Deep numerical linear algebra â€” from basic HSS to superfast eigensolvers (SuperDC), selected inversion, ULV factorization, and tree quasi-separable generalizations.


### Other Proposals (remaining 23)

The other proposals form coherent clusters:

**Structured state transitions** (most testable for <$1):
- **Monarch-Gated SSM** (006): Monarch-factored A_t at O(nâˆšn), tested on S5. MVE: $0.50.
- **Cayley-Circulant SSM** (027): Exact orthogonality + FFT. MVE: $0.17. *Already implemented (Exp 027).*
- **Neumann-Cayley SSM** (028): Approximate Cayley for input-dependent orthogonality. MVE: $0.17. *Already implemented (Exp 028).*
- **GS Monomial SSM** (016): Group-and-Shuffle with signed permutations. Novel but complex.
- **Hyperoctahedral SSM** (017): B_n signed permutations. *Already implemented (Exp 017).*

**Semiring/scan innovations** (highly novel):
- **Log-Semiring SSM** (014): Softmax-native scan via logsumexp. MVE: $0.40. *Already implemented (Exp 014).*
- **Tropical-Gated SSM** (015): Max-plus hard attention. MVE: $0.25. *Already implemented (Exp 015).*

**Kernel/efficiency** (require GPU, higher risk):
- **Chimera-Fused Chunkwise SSM** (032), **EVT-Fused SSM Epilogues** (033), **Stream-K BRGEMM** (034), **Near-Far Field GLA** (036): All kernel optimization proposals requiring GPU to validate. MVE costs $2â€“$5 each.
- **V:N:M Sparse SSM** (031), **Transposable N:M Sparse GLA** (035): Sparsity proposals requiring Sparse Tensor Cores. *Exp 031 implemented.*

**Higher-level architecture**:
- **Capacitance-Coupled Multi-Scale SSM** (019): Cross-scale coupling via small matrices. *Implemented (Exp 019).*
- **NystrÃ¶m Landmark SSM** (025): Inter-chunk state compression. Novel but complex.
- **Black-Box HSS Attention** (021): Adaptive hierarchy. *Implemented (Exp 021).*
- **cos-LogLinear** (008), **Segmented-HSS** (005): Composition proposals. Medium priority.
- **Hutchinson Adaptive Rank** (018): Diagnostic tool for DPLR. Medium priority, cheap.


### Strategic Insights

**1. The semiring experiments are the crown jewels â€” run them first.** Experiments 014 (Log-Semiring) and 015 (Tropical-Gated) are already implemented and test the most novel hypothesis in this batch: that replacing (R, +, Ã—) with alternative semirings fundamentally changes SSM capabilities. Both cost <$0.50 and could yield publishable results. If either works, it opens an entire new research direction.

**2. Experiment 022's failure (Cauchy DR-SSM) redirects the structured-transition search.** The Cauchy kernel's gradient pathology eliminates displacement-rank as a viable parameterization for SSM state transitions at practical scales. This makes Proposals 013 (Circulant) and 006 (Monarch) the frontrunners for "structured but mixing" state transitions â€” both avoid the 1/(s_i - s_j) singularity while providing coordinate mixing.

**3. Cyclic reduction (Exp 026) is a proven win â€” integrate it.** The 3.88Ã— speedup is real, validated, and immediately applicable. Every future non-diagonal SSM experiment should use CR instead of prefix scan. This is rare: a pure algorithmic improvement with no quality tradeoff.

**4. Immediate next actions (total budget: ~$3)**:
   1. Run Exp 009 (Post-Sigmoid Gating) â€” $0.50, highest impact/cost ratio
   2. Run Exp 014 (Log-Semiring SSM) â€” $0.40, most novel
   3. Run Exp 015 (Tropical-Gated SSM) â€” $0.25, paired with 014
   4. Run Exp 013 (Circulant SSM) â€” $0.25, validates fundamental architecture
   5. Run Exp 027 (Cayley-Circulant) â€” $0.17, tests orthogonal memory retention
   6. Run Exp 028 (Neumann-Cayley) â€” $0.17, tests approximate orthogonality
   7. Run Exp 017 (Hyperoctahedral SSM) â€” $0.27, tests signed permutation bias




I'll analyze the recent activity systematically - first examining the experiment results, then the proposals, and finally the tricks to generate a comprehensive log entry.Read the full results files for the completed experiments. I need to understand what was learned from each. Read these files:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/022/results.md (or any results file in that directory)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/results.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/026/results.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/007/results.md

Also check for results files in the experiments directory:
/home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/

Return the FULL content of each results file you find.Read the following proposal files to understand their full hypotheses, estimated costs, and experiment designs. I need enough detail to assess impact and feasibility:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/006-monarch-gated-state-transition.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/007-oscillatory-gated-selective-ssm.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/036-near-far-field-chunkwise-gla.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/016-gs-monomial-ssm-state-transition.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/032-chimera-fused-chunkwise-ssm-scan.md

For each, extract: hypothesis, estimated cost, MVE design, success criteria, and key risks.Now let me compile the comprehensive log entry: