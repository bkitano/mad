---
title: Research Log Update
date: 2026-02-15T17:03:00
timestamp: 2026-02-15 17:03:00
tricks: 1
proposals: 7
experiments: 8
source: legacy_migration
---

## 2026-02-15 â€” 17:03 UTC

### ðŸŽ¯ High-Impact Proposals (Top Picks for <$10 Budget)

**1. Log-Semiring SSM (Proposal 014)** â€” Priority: **HIGH**
- **Hypothesis**: Replace standard $(+, \times)$ semiring in SSM scans with $(\text{logsumexp}, +)$ to natively compute softmax-weighted attention in $O(T)$ time.
- **Why it matters**: This is the most theoretically clean unification of softmax attention and SSMs proposed in this batch. If it works, it collapses the quality gap between linear attention and softmax â€” the central open problem in efficient sequence modeling. The MVE is already implemented (code/014) and the selective copying task directly tests the core claim.
- **Estimated cost**: MVE ~$0.40 (8 min on T4). Small-scale validation ~$16.
- **Impact score**: 9/10 â€” Cheapest MVE in the batch with highest theoretical upside. The logsumexp operator is numerically well-understood and the online-softmax trick (trick 083) directly applies.

**2. Cyclic Reduction for Dense SSM Recurrences (Proposal 026)** â€” Priority: **HIGH**
- **Hypothesis**: Cyclic reduction achieves $O(Tn^3)$ work vs prefix scan's $O(Tn^3 \log T)$ for non-diagonal SSM training.
- **Why it matters**: **Already validated on CPU** â€” Experiment 026 showed 3.88Ã— speedup at T=1024 and 6Ã— GEMM reduction. This is the rare proposal where the MVE *already succeeded* and the next step (GPU kernel validation at code/026_scaled) is ready to run. Enabling practical dense-transition SSMs (DeltaNet, DeltaProduct) to train 2-4Ã— faster would be a significant infrastructure win.
- **Estimated cost**: GPU validation ~$5-8 (2 GPU-hours on A100).
- **Impact score**: 8.5/10 â€” Low risk (already proven on CPU), high practical value, clear next step.

**3. GS-Monomial SSM (Proposal 016)** â€” Priority: **HIGH**
- **Hypothesis**: Group-and-Shuffle monomial state transitions achieve dense-level expressivity at $O(n\sqrt{n})$ cost via block-diagonal monomial factors + shuffle permutation.
- **Why it matters**: Addresses the fundamental expressivity-efficiency gap. Monomial matrices are closed under multiplication (critical for parallel scan), and the GS factorization is proven to reach the full orthogonal group with just 2 factors. The MVE (code/016) is implemented and targets S5 composition â€” the canonical non-abelian benchmark.
- **Estimated cost**: MVE ~$0.50 (10 min T4).
- **Impact score**: 8/10 â€” Novel combination of GS factorization + monomial closure. Clear test (S5 composition). But requires careful Gumbel-Sinkhorn tuning.


### ðŸ§ª Experiment Updates

**Experiment 026: Cyclic Reduction vs Prefix Scan** (Status: âœ… COMPLETED)
- **Proposal**: 026-cyclic-reduction-randmscan-ssm-recurrence
- **Key findings**: 3.88Ã— wall-clock speedup (CPU), 6.01Ã— GEMM savings at T=1024 with < 10â»Â¹âµ numerical error. CR's advantage grows with sequence length. **Ready for GPU validation** (code/026_scaled implemented).
- **Cost**: $0.00 (CPU-only)

**Experiment 007: OscGate-SSM** (Status: âœ… COMPLETED)
- **Proposal**: 007-oscillatory-gated-selective-ssm
- **Key findings**: **93% accuracy** on selective copying vs 47% for fixed-frequency LinOSS â€” a 46pp gap that definitively proves input-dependent oscillatory parameters enable selectivity. Stability guarantee held (0 NaN events). Only 1.8Ã— overhead vs diagonal SSM.
- **Cost**: $0.00 (CPU-only)

**Experiment 011: Neumann-Approximate Resolvent** (Status: âœ… COMPLETED)
- **Proposal**: 011-neumann-resolvent-chunkwise-ssm
- **Key findings**: k=4 Neumann terms achieve < 10â»â´ relative error with 3.8-8.9Ã— speedup scaling. Convergence guaranteed (spectral radius < 0.11). But near-resonance motivation was weak; value is pure speed at large state dims.
- **Cost**: $0.00 (CPU-only)

**Experiment 029: Circulant FAVOR+** (Status: âŒ ABANDONED)
- **Proposal**: 029-circulant-favor-plus-linear-attention
- **Key findings**: Circulant projection matches dense FAVOR+ quality (validates the optimization), but FAVOR+ itself fails catastrophically on associative recall (23% test vs 98.5% for simple ReLU linear attention). **Lesson: don't optimize a broken primitive.**
- **Cost**: ~$0.10

**Experiment 022: Displacement-Rank SSM** (Status: âŒ ABANDONED)
- **Proposal**: 022-displacement-rank-ssm-state-transitions
- **Key findings**: Cauchy-like structure creates ill-conditioned gradients that prevent learning. Î±=4 shows no benefit over Î±=1; both fail on harder S5 task (1-3% vs Dense's 97%). **Lesson: theoretical expressivity â‰  practical trainability.**
- **Cost**: $0.00 (CPU-only)

**Experiment 025: NystrÃ¶m Landmark Compression** (Status: âœ… COMPLETED)
- **Proposal**: 025-nystrom-landmark-chunkwise-ssm
- **Key findings**: 4Ã— state compression with NystrÃ¶m (m=2) achieves **99.25% accuracy** vs 99.08% full â€” the model co-adapts around the compression bottleneck even when the low-rank assumption is violated. Validates the approach; needs scaling to n=32-128.
- **Cost**: ~$0.05

**Experiment 005: HSS Linear Attention** (Status: âŒ ABANDONED)
- **Proposal**: 005-segmented-hss-linear-attention
- **Key findings**: HSS tree structure is 6.3Ã— slower than dense on GPU (sequential traversals). Neither HSS nor dense achieved >25% accuracy on the task. GPU-unfriendly exotic structures are dead ends.
- **Cost**: ~$0.15

**Experiments 010, 014, 015, 016, 017, 019, 028, 030, 031** (Status: ðŸ”§ IMPLEMENTED, awaiting runs)
- All have code ready in their respective directories with configs and Modal deployment files. Many have had smoke tests pass. These represent the next batch to execute.


### ðŸ“š New Discoveries (177 Tricks)

This is an enormous documentation push â€” 177 tricks covering the full stack from algebraic foundations to GPU kernel engineering. Key highlights:

- **Semiring Monoid Lifting (108)**: Replace $(+, \times)$ with alternative semirings (tropical, log, min-plus) in neural nets. The theoretical backbone enabling proposals 014 and 015.
- **Newton-Schulz Polar Orthogonalization (164)**: SVD-free matrix orthogonalization via matrix multiplies only â€” the core of the Muon optimizer. Now with Chebyshev-optimal coefficients (CANS, trick 170).
- **Warp-Specialized Pipelining (141)**: FlashAttention-3's producer/consumer warp splitting. Foundational for 5+ kernel-level proposals.
- **Group-and-Shuffle Matrices (055)**: Monarch generalization that reaches full expressivity with fewer factors. Core of proposal 016.
- **TFLA Two-Level Tiled Chunkwise Parallelism (158)**: The current SOTA for linear RNN kernels. Multiple proposals (038, 039, 044) target improving it.
- **Tropical Attention (132)**: Attention in tropical projective space â€” replaces softmax with max-plus geometry. Novel expressivity direction.
- **V:N:M Hierarchical Sparsity (140)**: Flexible sparsity beyond 50% that still uses Sparse Tensor Cores. Key for proposals 031, 035.
- **CTA Tile Swizzling (033)**: L2 cache optimization for GPU kernels â€” a "free" 10-20% speedup applicable to any tiled kernel.


### Other Proposals

**Architecture proposals (need GPU runs, most MVEs <$1):**
- **Tropical-Gated SSM (015)**: Max-plus hard attention; MVE implemented, awaiting run. (~$0.50)
- **Cayley-Circulant Orthogonal SSM (027)**: Exact orthogonality + FFT; code ready. (~$0.50)
- **Circulant-Diagonal SSM (023)**: CD products for O(n log n) state mixing; code ready. (~$0.50)
- **Hyperoctahedral Signed-Permutation SSM (017)**: B_n group state transitions; code ready. (~$0.50)
- **Capacitance-Coupled Multi-Scale SSM (019)**: Cross-scale coupling via small matrix; code ready. (~$0.50)
- **Neumann-Cayley SSM (028)**: Input-dependent orthogonal via Neumann approx; code ready. (~$0.50)
- **OH-DeltaProduct (020)**: Oscillatory + Householder decomposition; code ready. (~$0.50)
- **Group-Matrix Displacement Rank SSM (030)**: B_4 group matrices with displacement rank; code ready. (~$0.50)

**Kernel optimization proposals (need Triton/CUDA, higher effort but concrete):**
- **Chimera-Fused Chunkwise SSM (032)**: GEMM-chain fusion for intra-chunk computation. MVE ~$1.50.
- **EVT-Fused SSM Epilogues (033)**: Eliminate elementwise kernels via CUTLASS EVT. MVE ~$4.
- **CTA-Swizzled TFLA (038)**: L2 cache optimization for linear RNN kernels.
- **Warp-Specialized Chunkwise Linear RNN (039)**: FA3-style pipelining for TFLA.
- **Persistent Megakernel Linear RNN (040)**: FlashMoE-style full-layer fusion.
- **MatMulScan Inter-Chunk State (044)**: Tensor-core-native scan for state propagation.
- **Stream-K BRGEMM State Accumulation (034)**: Work-balanced fused accumulation.
- **Contraction-Ordered Multi-Operand GLA Fusion (042)**: opt_einsum for chunkwise GLA.
- **EVT Joint Fwd-Bwd Graph Partitioning (041)**: Joint optimization of training passes.

**Sparsity proposals:**
- **V:N:M Sparse SSM Projections (031)**: 60-75% sparsity on projections with S-STE; code ready.
- **Transposable N:M Sparse GLA Pretraining (035)**: Sparse Tensor Cores for both fwd and bwd.
- **2:4 Sparse SSM State Transitions (024)**: Sparsify the transition matrix itself.

**Feature map / attention proposals:**
- **SADERF-SORF Feature Maps for GLA (037)**: Dense-exponential random features + SORF.
- **Near-Far Field Decomposition for GLA (036)**: FMMformer-style chunk decomposition.
- **Displacement-Rank SSM (022)**: âŒ Abandoned (optimization failure in experiment).
- **Newton-Schulz Orthogonal DeltaNet (043)**: Polar decomposition for chunk preprocessing.


### Strategic Insights

**1. The semiring frontier is the most promising architectural direction.** Proposals 014 (log-semiring) and 015 (tropical) represent genuinely novel SSM architectures that could close the softmax quality gap â€” the #1 open problem in efficient sequence modeling. Both MVEs are implemented and cost <$1 to run. **Run these first.**

**2. Experiment results reveal a sharp divide: "correct math" vs "trainable systems."** Experiments 022 (Cauchy-like) and 005 (HSS) had beautiful theory but failed due to optimization barriers and GPU-unfriendliness. Meanwhile, the simplest approaches (Experiment 007's input-dependent gating, Experiment 025's NystrÃ¶m co-adaptation) succeeded. **Favor approaches with clean gradient flow and GPU-friendly primitives (matmuls, FFTs).**

**3. The kernel optimization proposals are safe bets with guaranteed payoff.** Unlike architecture proposals (which may not train), kernel optimizations (032, 033, 038) can only succeed or provide useful profiling data. However, they require Triton/CUDA expertise and are higher-effort MVEs ($2-5 each). **Batch these as a second priority after architecture MVEs.**

**4. Recommended execution order for next $10 budget:**
   1. Run Experiment 014 (Log-Semiring SSM) â€” ~$0.40
   2. Run Experiment 015 (Tropical-Gated SSM) â€” ~$0.50
   3. Run Experiment 016 (GS-Monomial SSM) â€” ~$0.50
   4. Run Experiment 026_scaled (Cyclic Reduction GPU validation) â€” ~$5
   5. Run Experiments 017, 027, 028 (remaining orthogonal SSM variants) â€” ~$1.50
   
   Total: ~$8.40, testing 7 distinct architectural hypotheses.




I need to analyze a massive amount of activity â€” 179 tricks, 32 proposals, and 27 experiments. Let me explore the key details efficiently.Read the following experiment result files and summarize key findings for each completed experiment. Focus on: what worked, what failed, cost, and key insights.

Read these files:
1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/results.md (or RESULTS.md)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/022/results.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/029/results.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/026/results.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/results.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/005/results.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/007/results.md
8. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/011/results.md
9. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/025/results.md

For each, extract: status (pass/fail), key metrics, cost, and the most important takeaway. Just do research, don't write code.Now let me look at a few key proposals in more detail to assess their feasibility and costs:Read the following proposal files and extract for each: (1) the estimated compute cost, (2) model size in params, (3) what hardware is needed, (4) what task/benchmark is proposed, and (5) any specific implementation complexity notes. Just do research, don't write code.

Read these files at thoroughness "medium":
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/016-gs-monomial-ssm-state-transition.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/032-chimera-fused-chunkwise-ssm-scan.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/040-persistent-megakernel-linear-rnn-layer.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/027-cayley-circulant-orthogonal-ssm.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/023-circulant-diagonal-ssm-state-transitions.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/020-oscillatory-householder-deltaproduct.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.mdNow I have comprehensive data. Let me compose the log entry.