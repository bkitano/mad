---
title: Research Log Update
date: 2026-02-15T03:31:00
timestamp: 2026-02-15 03:31:00
tricks: 1
proposals: 2
experiments: 2
source: legacy_migration
---

## 2026-02-15 â€” 03:31 UTC

### ðŸŽ¯ High-Impact Proposals

- **Oscillatory-DPLR SSM** (Proposal 004 â€” Priority: **HIGH**)
  - **Hypothesis**: Combining oscillatory discretization (stability by construction from damped harmonic oscillators) with DPLR structure (Cauchy kernel efficiency) produces an SSM that is more stable, equally efficient, and more trainable than S4/S5.
  - **Why it matters**: This attacks a foundational pain point in SSMsâ€”stability engineering. S4/S5 impose stability through eigenvalue constraints and careful initialization, which is brittle at scale. Oscillatory parameterization gets stability *for free* from physics (Re(Î») = -Î¶Ï‰ â‰¤ 0 always), while DPLR preserves O(T log T) convolution mode. The parameters (frequency Ï‰, damping Î¶) are physically interpretable, which could make debugging and scaling dramatically easier. MVE already implemented.
  - **Estimated cost**: **<$1** (MVE runs in ~5 min on CPU; full LRA benchmark ~$5-8 on spot GPU)
  - **Impact score**: **8/10** â€” Excellent cost-effectiveness. Core mechanism can be validated for pennies. If it works, it simplifies the entire SSM initialization/stability story. Risk is low: worst case, you learn oscillatory parameterization doesn't outperform S4D on LRA.

- **Column-Sparse Negative-Eigenvalue DeltaNet** (Proposal 001 â€” Priority: **HIGH**)
  - **Hypothesis**: Combining column-sparse permutation routing with negative eigenvalue extension (Î² âˆˆ [0,2]) in DeltaNet achieves strictly greater expressivity than either alone, enabling simulation of automata over non-solvable groups.
  - **Why it matters**: This probes a *theoretical frontier*â€”whether SSMs can break past TCâ° expressivity to NCÂ¹ state tracking. The D4 dihedral group test is a clean, minimal benchmark: rotations need permutation routing, reflections need sign-flipping. If CS-NEG-DeltaNet hits >90% while ablations stay below 75%, that's a publishable result about the expressivityâ€“efficiency tradeoff. **However**, the researcher's own notes flag that DeltaProduct (Schlag et al., 2025) may supersede the column-sparse approach at lower cost (O(dÂ·nâ‚•) vs O(dÂ²))â€”this should be tested as a third ablation.
  - **Estimated cost**: **<$2** (model is ~2.4K params, curriculum training ~3-4 hrs on T4, or free on local CPU/MPS)
  - **Impact score**: **7/10** â€” Cheap and theoretically crisp, but the DeltaProduct concern slightly dilutes the novelty. Running it is still worthwhile to establish the empirical baseline before pivoting.


### ðŸ§ª Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: **implemented, results pending**)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Full MVE codebase built â€” model (oscillatory eigenvalue parameterization + DPLR + bilinear discretization), synthetic data generator (damped sinusoid extrapolation), training loop with curriculum. A `best_model.pt` checkpoint (3.7KB) was saved at 03:10 UTC, but **no `results.yaml` was generated**, suggesting the training either didn't complete or crashed before the final evaluation stage.
  - **Key findings**: None yet. Success criteria are clear: training MSE < 1e-3, extrapolation MSE < 1e-2 on 4Ã— longer sequences, learned Ï‰ values cluster in [0.01, 0.1].
  - **Cost**: ~$0 actual (ran on CPU). **Action needed: re-run and capture results.**

- **Experiment 001: CS-NEG-DeltaNet D4 State Tracking** (Status: **implemented, not yet executed**)
  - **Proposal**: 001-column-sparse-negative-eigenvalue-deltanet
  - **Progress**: Complete codebase including 4 model variants (Standard, NEG, CS, CS-NEG DeltaNet), D4 group multiplication table, curriculum learning from k=1â†’20, W&B integration, and Modal GPU deployment scripts. All 4 YAML configs ready. Zero training runs have been started.
  - **Key findings**: During implementation, theoretical analysis in `notes.md` revealed that **DeltaProduct with nâ‚•=2 Householder reflections may be a superior approach** to explicit column-sparse permutations for D4. This is an important architectural insight that emerged from the implementation process itself.
  - **Cost**: $0 actual. Projected ~$1-2 on Modal T4.


### ðŸ“š New Discoveries

A massive influx of **40 tricks** was documented, spanning the full stack from algebraic foundations to GPU kernel optimization. Key themes:

- **Structured matrix zoo**: HSS matrices, semiseparable block decomposition, Monarch factorization, group-and-shuffle matrices, block circulant matrices, DPLR, and the alternating low-rank-diagonal decomposition. These form a rich *menu of state matrix parameterizations* at various points on the expressivityâ€“efficiency Pareto frontier.

- **Parallelization primitives**: Blelloch work-efficient scan, segmented scan, recurrence-to-scan reduction, chunkwise parallel scan, TSQR. These are the building blocks for training any recurrent model at GPU-friendly parallelism. The recurrence-to-scan reduction trick (augmenting state into pairs to get a single associative operator) is the foundational move enabling SSMs like Mamba to train efficiently.

- **Stability & expressivity knobs**: Oscillatory eigenvalue stability (stable by construction from physics), Cayley contractive parameterization (exact orthogonality), negative eigenvalue extension (TCâ° â†’ NCÂ¹), Perturb-Then-Diagonalize (robust HiPPO initialization). These represent the *design space for controlling gradient flow* in long-range recurrences.

- **Algebraic expressivity theory**: Krohn-Rhodes monoid decomposition (the "Jordan-HÃ¶lder for automata"), Cartan-DieudonnÃ© decomposition (every orthogonal = â‰¤n reflections), semiring monoid lifting (alternative algebraic structures for "linearity"), signed permutation matrices (unifying reflections and permutations via hyperoctahedral group). These provide the *theoretical ceiling* for what recurrent models can express.

- **Efficiency identities**: Woodbury resolvent, Schur complement, matrix determinant lemma, Cauchy kernel trick, capacitance matrix method. These are the algebraic hammers that turn O(nÂ³) into O(n) when your matrix has the right structure.

- **Log-linear attention**: A middle ground between linear attention (constant state, limited recall) and full attention (quadratic) using Fenwick tree organization of O(log T) hidden states. Potentially the most architecturally novel trick in the batch.

### Other Proposals

- **SSD-DeltaNet** (Proposal 002, high priority): Reformulate DeltaNet's WY representation as block-semiseparable to get SSD-style tensor-core acceleration. Estimated 2-4Ã— training speedup. Cost: ~$5-8 to benchmark. Well-grounded but more of an *engineering optimization* than a scientific hypothesis.

- **DPLR Column-Sparse SSM** (Proposal 003, medium priority): Wrap column-sparse permutation routing around a DPLR core to get Cauchy kernel efficiency + permutation expressivity. Elegant idea but combines two complex components; debugging may be expensive. Cost: ~$5-10.

- **Segmented-HSS Linear Attention** (Proposal 005, medium priority): Use HSS structure for the linear attention state matrix to get O(d log d) updates instead of O(dÂ²), plus segmented scan for variable-length batching. Theoretically appealing but implementation complexity is high (needs custom HSS kernels). Cost: likely >$10 for meaningful validation. **Deprioritize for now.**


### Strategic Insights

**The research is converging on a clear question**: *What is the right structured matrix parameterization for recurrent state transitions?* The 40 new tricks define a design space with three axes: (1) stability guarantees (oscillatory, Cayley, eigenvalue constraints), (2) expressivity class (diagonal < DPLR < column-sparse < full orthogonal), and (3) hardware efficiency (diagonal > block-diagonal > low-rank > dense). Every proposal is exploring a different point in this space.

**Immediate priority should be getting Experiments 001 and 002 to produce results.** Both are fully implemented with <$2 projected cost each. Experiment 002 (Oscillatory-DPLR) appears closest to completionâ€”just needs a successful re-run to capture `results.yaml`. Experiment 001 (CS-NEG-DeltaNet) hasn't been started at all; consider adding a DeltaProduct ablation before launching. Together, these two experiments will answer whether *physics-based stability* and *algebraic expressivity* are the right axes to push on.

**The DeltaProduct insight from Experiment 001's notes is the most important meta-finding**: if implicit Householder accumulation already builds the permutations you need, then explicit column-sparse routing may be unnecessary complexity. This could redirect Proposals 001 and 003 toward simpler DeltaProduct-based architectures, saving significant implementation effort.




Let me first examine the experiment details more closely to understand what happened and get the full context.I need to understand the full details of the recent experiments and proposals in this research project. Please:

1. Read the full results file for experiment 002 at /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/ - look for any results.md, README.md, or output files
2. Read the full README or any results for experiment 001 at /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001/
3. Read the full proposal files:
   - Look in /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/ for files 001 through 006
4. Check if there's a log file or any existing log entries at /home/bkitano/Desktop/vault/projects/mad-architecture-search/

Focus on: experiment outcomes, cost data, what was learned, and proposal details about estimated compute costs and feasibility.Now let me check for the existing log file to understand the format and append properly.Now I have full context. Let me read the proposals for the 3 new ones (004, 005, 006) that appeared in this batch to understand the new additions vs. what was already logged.Now I have all the context I need. Let me generate the log entry.Here's a summary of the log entry I just generated:

## Log Entry Summary (2026-02-15 â€” 03:52 UTC)

### Key Headlines

1. **ðŸ† New #1 Priority: Monarch-Gated State Transition SSM (Proposal 006)** â€” The most promising new proposal. It fills the expressivity gap between diagonal (cheap, no mixing) and dense (expensive, full mixing) SSMs using Monarch-factored transitions at O(nâˆšn) cost. Unique advantage: Monarch products are *closed* (stay Monarch under multiplication), which is critical for scan-based training. MVE is trivially cheap (<$3).

2. **âŒ Experiment 002 (Oscillatory-DPLR) FAILED** â€” Training loss was completely flat across 50 epochs. Zero learning occurred. However, the learned frequency/damping parameters were in the correct ranges, suggesting the parameterization works but the output pathway has a gradient flow bug. Likely fixable with targeted debugging (<$1).

3. **46 new tricks documented** â€” Massive knowledge base expansion covering structured matrices (Neumann series, displacement rank, HSS, FKT), parallelization primitives (tiled QR, TSQR), and algebraic foundations.

4. **3 new proposals** (004, 005, 006) bring the total to 6. Proposal 003 (DPLR Column-Sparse) may be subsumed by the more general Monarch approach.

### Revised Priority Stack
| Rank | Proposal | Cost | Rationale |
|------|----------|------|-----------|
| 1 | Monarch-Gated SSM (006) | <$3 | Highest novelty, clean MVE, Monarch closure advantage |
| 2 | Oscillatory-DPLR debug (004) | <$1 | Fix gradient bug before abandoning |
| 3 | CS-NEG-DeltaNet (001) | <$2 | Ready to launch, add DeltaProduct ablation |
| 4 | SSD-DeltaNet (002) | ~$5-8 | Engineering optimization, lower urgency |
| 5-6 | 003, 005 | $5-10+ | Hold/defer |

The log has been appended to `/home/bkitano/Desktop/vault/projects/mad-architecture-search/notes/log.md`.



Let me first examine the experiment results and proposals in more detail to give a thorough assessment.Read the following files and return their full contents. I need to understand the experiment results and proposal details:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/RESULTS.md (or any results file)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/003/README.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001/README.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/007-oscillatory-gated-selective-ssm.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/006-monarch-gated-state-transition.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/010-sparse-monarch-ssm-pa-dst.md
8. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/001-column-sparse-negative-eigenvalue-deltanet.md
9. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/002-ssd-deltanet-wy-hybrid.md
10. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/004-oscillatory-dplr-ssm.md

Just read all these files and return their contents so I can analyze them.Now I have all the details needed for a thorough log entry. Let me compose the report.