---
title: Research Log Update
date: 2026-02-15T09:10:00
timestamp: 2026-02-15 09:10:00
tricks: 1
proposals: 3
experiments: 4
source: legacy_migration
---

## 2026-02-15 ‚Äî 09:10 UTC

### üéØ High-Impact Proposals

**Tier 1: Immediate Priority ‚Äî Novel, Cheap, High Upside**

- **[009] Post-Sigmoid Gating for Linear Attention / SSM Readout** (Priority: **highest**)
  - **Hypothesis**: Applying post-readout sigmoid gating (NeurIPS 2025 Best Paper trick) to linear attention/SSM outputs breaks the low-rank bottleneck in their output projections, yielding 5‚Äì15% perplexity improvement with <2% latency overhead.
  - **Why it matters**: This is the **lowest-hanging fruit** in the entire batch. It's a ~20-line code change to any existing linear attention or SSM model. The theoretical argument is airtight ‚Äî linear attention compresses all history into a d√ód state, and the W_V¬∑W_O product creates a rank bottleneck that sigmoid gating provably breaks. The benefit should be *larger* for linear attention than for softmax (where it was already a best paper). The MVE is dead simple: gated vs ungated cosFormer on MQAR.
  - **Estimated cost**: <$1 (MVE: ~$0.50, 10 minutes single GPU)
  - **Impact score**: 9/10 ‚Äî Exceptional cost-effectiveness. Near-zero implementation risk, strong theoretical grounding, and directly addresses a known weakness of every linear attention variant. If it works on cosFormer it transfers to GLA, Mamba-2, RetNet, etc.

- **[013] Circulant SSM: Fourier-Domain Parallel Scan** (Priority: **high**)
  - **Hypothesis**: Block-circulant state transitions, diagonalized via FFT, enable element-wise parallel scans in frequency space ‚Äî recovering diagonal SSM parallelism with dense-transition expressivity at O(n log n) per step.
  - **Why it matters**: This is the **cleanest new idea** in the batch. The key insight that FFT diagonalization converts a circulant state transition into n independent scalar scans is mathematically elegant and immediately implementable. The MVE tests on Z‚Çà composition ‚Äî the *perfect* task since cyclic group structure aligns with circulant structure. If circulant SSMs can't beat diagonal on Z‚Çà, nothing will work; if they can, the approach generalizes.
  - **Estimated cost**: <$1 (MVE: ~$0.25, 5 minutes single GPU)
  - **Impact score**: 8.5/10 ‚Äî Very cheap validation, clean theory, and addresses the fundamental diagonal-vs-dense tradeoff in SSMs. The Z‚Çà task is a fair but favorable test.

**Tier 2: High Potential, Slightly More Complex**

- **[014] Log-Semiring SSM: Softmax-Native Parallel Scan** (Priority: **high**)
  - **Hypothesis**: Using the logarithmic semiring (logsumexp, +) in parallel scans produces an SSM whose hidden state natively computes softmax-weighted attention ‚Äî unifying softmax expressivity with O(T) cost.
  - **Why it matters**: If this works, it's potentially a **unification result** ‚Äî softmax attention and SSMs become the same computation under different semirings. The log-space arithmetic is well-understood (it's the same as online softmax), and the parallel scan structure is identical to Mamba's but with a different binary operator.
  - **Estimated cost**: <$1 (MVE: ~$0.40, 8 minutes)
  - **Impact score**: 8/10 ‚Äî High theoretical ceiling but the numerical stability of logsumexp composition over many scan steps is an open question.

- **[015] Tropical-Gated SSM: Max-Plus Parallel Scan** (Priority: **high**)
  - **Hypothesis**: Tropical semiring (max, +) state dynamics create hard winner-take-all memory with 1-Lipschitz stability by construction.
  - **Why it matters**: Tropical and log-semiring proposals are siblings ‚Äî they test the same core idea (alternative semirings for scans) at two extremes (hard max vs. soft logsumexp). Running both MVEs together (~$1 total) would map out the semiring design space.
  - **Estimated cost**: <$1 (MVE: ~$0.25, 5 minutes)
  - **Impact score**: 7.5/10 ‚Äî The max operation may be too hard (no gradient flow through non-winners), but the annealing curriculum from smooth to hard is clever.


### üß™ Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: ‚ùå **completed ‚Äî FAILED**)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Fully implemented and trained (50 epochs, 27 min, CPU). Loss was completely flat at ~0.85 for all epochs ‚Äî **zero learning occurred**.
  - **Key findings**: The model learned interpretable œâ and Œ∂ values (good sign for parameterization) but the forward pass produced no gradient flow. Root cause likely a bug in complex dtype handling or discretization, not a fundamental flaw. **Verdict: DEBUG, do not scale up.** The parameterization idea is sound but implementation needs fixing before re-testing.
  - **Cost**: $0.00 (CPU only) vs $0.50 estimated

- **Experiment 003: Oscillatory-Gated Selective SSM** (Status: ‚úÖ **completed ‚Äî PASSED**)
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Progress**: Three implementation attempts. Attempt 1 failed (all models at chance, 7.3%). Attempt 2 partially worked (45.5%). Attempt 3 succeeded after scaling to d=128, m=64, adding MLP head.
  - **Key findings**: **OscGate-SSM achieved 93.0% on selective copying** (target >90% ‚úÖ), while LinOSS (LTI, fixed œâ/Œ∂) only reached 46.8%. Zero stability issues. 1.8√ó speed overhead (acceptable). **Core hypothesis validated**: input-dependent oscillatory parameters enable content-dependent gating while preserving stability by construction. Surprise: a diagonal SSM baseline hit 94.8%, suggesting the selectivity mechanism matters more than the oscillatory structure.
  - **Cost**: $0.00 (CPU) vs $0.40 estimated
  - **Implication**: Proposal 007 is validated at MVE level. The diagonal SSM matching OscGate performance on this task is a cautionary signal ‚Äî the oscillatory structure may not provide additional benefit *on simple tasks*. Need harder benchmarks (S‚ÇÖ, longer sequences) to differentiate.

- **Experiment 004: Displacement-Rank SSM** (Status: üîß **implemented, not yet run**)
  - **Proposal**: 022-displacement-rank-ssm-state-transitions
  - **Progress**: Code written (`train.py`, `config.yaml`). Will test DR-SSM at displacement ranks Œ± ‚àà {0, 1, 2, 4, 16} on S‚ÇÖ permutation composition.
  - **Cost**: ~$0.40 estimated

- **Experiment 001: Column-Sparse DeltaNet** (Status: üìù **research notes only**)
  - **Key finding from notes**: The entire CS-DeltaNet approach may be **redundant** ‚Äî DeltaNet already learns orthogonal matrices via Householder products (Cartan-Dieudonn√© theorem), and DeltaProduct accelerates this. Explicit permutation matrices don't add expressivity, just a different parameterization. Recommendation: use NEG-DeltaProduct (Œ≤ ‚àà (0,2)) with n_h=2 instead.


### üìö New Discoveries (115 tricks documented)

The 115 new tricks represent a **massive knowledge expansion** across six categories. Key highlights:

- **Semiring alternatives** (tropical, log, min-plus): The tropical attention paper (Hashemi et al., NeurIPS 2025) and SIMD¬≤ hardware paper provide both theoretical and hardware foundations for non-standard semiring computation in neural networks. This directly enables proposals 014 and 015.

- **Structured matrix zoo**: An extraordinary depth of structured matrix theory was catalogued ‚Äî HSS matrices, circulant decompositions (CSCS, BCCB, block g-circulant), displacement rank frameworks, Cauchy-like matrices, and their associated fast algorithms (ULV solvers, SuperDC eigensolvers, telescopic decompositions). This gives the research program a **rich combinatorial space** of matrix structures to try as SSM state transitions.

- **Permutation learning**: Multiple differentiable permutation techniques documented ‚Äî Sinkhorn relaxation, Gumbel-Softmax, orthogonal group relaxation (OT4P), STE-based learning (STEAM), bipartite matching. These are critical enablers for proposals involving learned permutations in state transitions.

- **GPU kernel techniques**: FlashInfer JIT fusion, persistent megakernel fusion (FlashMoE), warp-specialized pipelining (FA3), DSM inter-core fusion (FlashFuser), and CTA tile swizzling. These aren't directly needed for MVE-stage validation but will be essential for scaling successful proposals.

- **Circulant-diagonal factorizations**: CDFlow, CDVFT, and C¬≥A all demonstrate that circulant √ó diagonal products are a practical and efficient matrix parameterization. This directly supports proposal 023 (Circulant-Diagonal SSM State Transitions).


### Other Proposals

- **[023] Circulant-Diagonal SSM State Transitions**: CD products in Fourier domain. Similar spirit to 013 but with diagonal factors for extra expressivity. (~$0.50 MVE)
- **[016] Group-and-Shuffle Monomial SSM**: GS-factored monomial matrices as state transitions. Theoretically elegant but complex implementation. (~$0.50 MVE)
- **[017] Hyperoctahedral Signed-Permutation SSM**: Signed permutations via Gumbel-Softmax. Cool idea but Gumbel-Softmax over S_n is notoriously hard to train. (~$0.50 MVE)
- **[020] Oscillatory Householder DeltaProduct**: Decomposes DeltaProduct into oscillatory + reflective components. Needs Exp 002 bug fix first. (~$0.50 MVE)
- **[019] Capacitance-Coupled Multi-Scale SSM**: Multi-timescale SSMs coupled via small capacitance matrix. Interesting architecture but complex to validate. (~$0.50 MVE)
- **[021] Black-Box HSS Compression for Adaptive Hierarchical Attention**: Theoretically beautiful but requires HSS library engineering. (~$2 MVE)
- **[022] Displacement-Rank SSM**: Being tested in Experiment 004. Results pending. (~$0.40 MVE)
- **[011] Neumann-Approximate Resolvent**: Replaces exact Woodbury inversion with truncated Neumann series. More of a speed optimization than expressivity gain. (~$0.50 MVE)
- **[012] Expert-Choice Routing for Monarch SSM Heads**: MoE-style routing applied to SSM heads. Interesting but adds routing complexity. (~$1 MVE)
- **[010] Sparse Monarch SSM (2:4 + PA-DST)**: Hardware-accelerated sparse Monarch transitions. Needs 2:4 kernel access (Ampere+ GPU). (~$1 MVE)
- **[024] 2:4 Sparse SSM via S-STE + Blockwise Sinkhorn**: Similar to 010 but with continuous pruning. (~$1 MVE)
- **[002] SSD-DeltaNet**: WY ‚Üí block-semiseparable reformulation. More of an engineering acceleration than a new architecture. (~$0.50 MVE but real value is at scale, ~$200)
- **[001] Column-Sparse NEG-DeltaNet**: Likely redundant per Experiment 001 notes ‚Äî DeltaProduct subsumes this.
- **[003] DPLR Column-Sparse SSM**: Medium priority. Bridges S4 convolution with PD-SSM routing. (~$0.50 MVE)
- **[004] Oscillatory-DPLR SSM**: Needs Experiment 002 bug fix first.
- **[005] Segmented-HSS Linear Attention**: Medium priority. Complex implementation. (~$1 MVE)
- **[008] Cosine-Reweighted Log-Linear Attention**: Composition of two known good ideas. (~$0.50 MVE)
- **[018] Hutchinson Trace-Guided Adaptive Rank**: Diagnostic tool, not an architecture. Low priority for MVE.


### Strategic Insights

**1. The semiring frontier is the most promising unexplored direction.** Proposals 013 (circulant/Fourier), 014 (log-semiring), and 015 (tropical) all exploit the same core insight ‚Äî that the parallel scan primitive only requires an associative binary operator, not specifically (√ó, +). These three proposals can be validated for ~$2 total and would map out a fundamentally new design axis for SSMs. **Run all three MVEs in the next session.**

**2. Post-sigmoid gating (009) is the single highest-ROI experiment.** It's architecture-agnostic, requires ~20 lines of code, costs $0.50 to validate, and applies to every linear attention and SSM variant. If it works, it immediately improves every other proposal in this batch. **Run this first.**

**3. Experiment 003's success validates selectivity, but the diagonal SSM surprise demands attention.** The fact that a simple diagonal SSM matched OscGate-SSM on selective copying (94.8% vs 93.0%) suggests that on easy tasks, the oscillatory structure provides no benefit over standard input-dependent diagonal gating. The research should move to **harder benchmarks** (S‚ÇÖ permutation composition, long-range arena) where structural differences matter. Experiment 004 (displacement-rank on S‚ÇÖ) is well-positioned for this.

**4. Recommended execution order for next 24 hours** (total budget ~$5):
   1. **009** Post-sigmoid gating MVE ($0.50)
   2. **013** Circulant SSM MVE ($0.25)
   3. **014** Log-semiring SSM MVE ($0.40)
   4. **015** Tropical SSM MVE ($0.25)
   5. **Run Experiment 004** (displacement-rank, already implemented) ($0.40)
   6. **Debug Experiment 002** (oscillatory-DPLR forward pass) ($0.00)
   7. **023** Circulant-diagonal SSM MVE ($0.50)

This sequence maximizes information gain per dollar while covering three independent research axes (gating, semirings, structured transitions).




I need to review the recent activity carefully to generate a comprehensive log entry. Let me first examine some of the key proposals and experiment results in more detail.Read the following files and return their full contents. I need the complete results for recent experiments:

1. Find and read the experiment results files for experiments 002, 003, and 004. Look in paths like:
   - /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/
   - /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/003/
   - /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/
   - Also check /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/ for result files

Search for any RESULTS.md, results.md, or similar files in those directories. Also read any experiment-log.md file.

Return the full text of all results you find.Now let me read a few of the most interesting proposals in more detail to assess their impact properly.Read the full contents of these proposal files and return their complete text. I need the detailed methodology, estimated cost, and experimental design sections:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/007-oscillatory-gated-selective-ssm.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/006-monarch-gated-state-transition.md

Return the full text of each file.Now I have comprehensive information to write the log entry. Let me compose it.