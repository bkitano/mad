---
title: Research Log Update
date: 2026-02-15T05:57:00
timestamp: 2026-02-15 05:57:00
tricks: 1
proposals: 3
experiments: 3
source: legacy_migration
---

## 2026-02-15 ‚Äî 05:57 UTC

### üß™ Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: ‚ùå COMPLETED ‚Äî FAILED)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Full MVE implemented and run on CPU. 1 layer, n=16, r=2, ~129 params. Trained 50 epochs on damped oscillation extrapolation task.
  - **Key findings**: **Total failure to learn** ‚Äî training MSE stuck at 0.854 (target was <1e-3, ~850√ó off). However, the oscillatory parameterization itself works: learned œâ (mean=0.050) and Œ∂ (mean=0.509) closely match ground truth (œâ=0.055, Œ∂=0.499). This strongly suggests a **forward pass implementation bug** (likely complex dtype handling), not a fundamental flaw in the oscillatory-DPLR concept. Loss was completely flat ‚Äî zero effective learning.
  - **Cost**: $0.00 (CPU only, 27 min)
  - **Verdict**: DEBUG. Do not proceed until MVE passes. Estimated 2-4 hours to fix.

- **Experiment 003: OscGate-SSM (Oscillatory-Gated Selective SSM)** (Status: ‚ùå COMPLETED ‚Äî FAILED)
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Progress**: All three models trained (~150 epochs each) on selective copying task. d_model=64, 2 layers, ~36K params.
  - **Key findings**: OscGate-SSM achieved only **45.5% accuracy** (target: >90%). LinOSS hit 38.0% (correctly below 40% as predicted). DiagonalSSM hit 44.0%. The core hypothesis ‚Äî that input-dependent œâ(x_t), Œ∂(x_t) enable content-dependent gating ‚Äî was **not validated**. OscGate barely outperformed the diagonal baseline. Stability and speed criteria passed (0 NaN events, 2.48√ó faster than diagonal). This may indicate the oscillatory parameterization doesn't provide sufficient selectivity, OR the model is too small / training is insufficient.
  - **Cost**: ~$0.00 (likely CPU/MPS)
  - **Verdict**: DEBUG. Both oscillatory proposals (002 and 003) have now failed. The parameterization works mechanically but doesn't produce useful learning dynamics at this scale.

- **Experiment 001: Column-Sparse Negative-Eigenvalue DeltaNet** (Status: implemented, awaiting results)
  - **Proposal**: 001-column-sparse-negative-eigenvalue-deltanet
  - **Progress**: Full implementation with 4 model variants (Standard, NEG, CS, CS-NEG DeltaNet) on D4 dihedral group state tracking. Infrastructure includes Modal GPU deployment configs.
  - **Key findings**: No results yet. This tests whether combining column-sparse transitions + negative eigenvalues enables non-solvable group simulation.
  - **Cost**: TBD

### üéØ High-Impact Proposals

**Rank 1:**
- **Log-Semiring SSM (014)** (Priority: **HIGH**)
  - **Hypothesis**: Replace the standard (+,√ó) semiring in SSM scans with (logsumexp, +), producing a recurrent model that natively computes **exact** softmax-weighted attention ‚Äî no kernel approximation needed.
  - **Why it matters**: This is the most theoretically novel proposal in the batch. It changes the *algebraic foundation* rather than tweaking architecture. The math is clean: the backward pass of the log-semiring IS the softmax function. If it works, it unifies SSM efficiency (O(T log T) parallel, O(T) sequential) with softmax-quality attention ‚Äî something no linear attention variant has achieved. The associativity proof is complete and the connection to FlashAttention's online softmax is deep.
  - **Estimated cost**: MVE ~$0.40 (selective copying, 2 layers, ~80K params, <8 min on single GPU). Small-scale ~$16.
  - **Key risk**: logsumexp runs on CUDA cores not tensor cores (~16√ó slower arithmetic). Sign tracking for negative values adds memory. But the MVE is dirt cheap.
  - **Impact score**: 9/10 ‚Äî Highest novelty, strong theory, trivially cheap to validate. Even if the full-scale version is slow, proving the concept on selective copying would be a significant insight. This should be the **#1 priority**.

**Rank 2:**
- **Post-Sigmoid Gating for Linear Attention (009)** (Priority: **HIGH**)
  - **Hypothesis**: Apply NeurIPS 2025 Best Paper's post-attention sigmoid gating to linear attention / SSM readout to break the low-rank bottleneck in output projections, yielding 5-15% perplexity improvement at <2% latency cost.
  - **Why it matters**: This is the lowest-risk, highest-certainty proposal. The technique is already proven for softmax attention (Best Paper!), and the argument that it helps linear attention *more* (due to a worse information bottleneck) is compelling. Implementation is trivial ‚Äî add one sigmoid gate layer. The benefit compounds with every other architectural improvement.
  - **Estimated cost**: MVE <$1 (cosFormer + MQAR task, tiny model). Full: <$10.
  - **Impact score**: 8/10 ‚Äî Low novelty but near-guaranteed payoff, extremely cheap, and composable with every other proposal. Should be validated immediately as a universal improvement.

### üìö New Discoveries (73 tricks documented)

The 73 new tricks span a remarkably coherent research surface. Key highlights:

- **Semiring Monoid Lifting**: The foundational insight that softmax = backward pass of log-semiring. Directly enables proposal 014. This single trick is the most strategically important discovery.
- **Permutation-Augmented Structured Sparsity (PA-DST)**: Learned permutations restore expressivity lost from structured pruning. Enables proposal 010 (Sparse Monarch SSM).
- **Expert Choice Routing**: Invert MoE routing so experts choose tokens ‚Üí perfect load balancing by construction. Enables proposal 012.
- **Block Circulant Matrices / FFT-Based Layers**: Circulant structure diagonalizes in Fourier domain ‚Üí O(n log n) matmul. Enables proposal 013 (Circulant SSM).
- **Post-Attention Sigmoid Gating**: NeurIPS 2025 Best Paper trick ‚Äî head-specific sigmoid gate breaks V¬∑Wo low-rank bottleneck. Directly enables proposal 009.
- **Neumann Series Approximate Inverse**: Truncated polynomial replaces matrix inversion with matrix-matrix products. Enables proposal 011.
- **Gumbel-Softmax / Sinkhorn / OT4P relaxations**: Three competing approaches to differentiable permutation learning ‚Äî relevant for any proposal involving learned permutations (006, 010, 012, 013).
- **RandMScan**: Matrix-based parallel scan using tensor cores instead of scalar ops ‚Äî could accelerate *any* SSM scan.
- **Cosine-Reweighted Linear Attention (cosFormer)**: ReLU + cosine distance weighting matches softmax quality at linear cost. Clean baseline for proposal 008.

### Other Proposals

- **Circulant SSM (013)**: Block-circulant transitions diagonalize via FFT for O(n log n) scans with full coordinate mixing. Elegant math but untested. MVE: cyclic group Z_8. Cost: <$5. *Medium-high priority.*
- **Monarch-Gated State Transition (006)**: Input-dependent Monarch transitions at O(n‚àön) per step. Ambitious but harder to implement. MVE: S_5 group composition. Cost: <$5. *Medium-high priority.*
- **Sparse Monarch SSM (010)**: 2:4 sparsity + PA-DST on Monarch blocks. Needs Ampere+ GPU for sparse tensor cores. Cost: ~$5. *Medium priority (hardware-dependent).*
- **Expert-Choice Monarch SSM Heads (012)**: Expert-choice routing over Monarch-factored state heads. Novel composition but complex implementation. Cost: ~$5. *Medium priority.*
- **Neumann-Approximate Resolvent (011)**: Replace Woodbury inverse with Neumann series in DPLR SSMs. Solid numerical analysis but incremental gain. Cost: ~$5. *Medium priority.*
- **SSD-DeltaNet (002)**: Reformulate WY representation as semiseparable matrix for tensor-core acceleration. Well-grounded but the oscillatory experiments' failures suggest debugging DeltaNet variants first. Cost: ~$3. *Medium priority.*
- **Cosine Log-Linear Attention (008)**: Compose cosFormer with log-linear attention's Fenwick tree states. Straightforward composition. Cost: <$5. *Medium priority.*
- **Column-Sparse NEG-DeltaNet (001)**: Already implemented as Experiment 001. Awaiting results. *Pending.*
- **Oscillatory-DPLR (004)**: Exp 002 failed ‚Äî needs debug. *Blocked.*
- **OscGate-SSM (007)**: Exp 003 failed ‚Äî needs investigation. *Blocked.*
- **DPLR Column-Sparse SSM (003)**: Elegant bridge between S4 convolution and PD-SSM expressivity. Cost: ~$5. *Medium priority.*
- **Segmented-HSS Linear Attention (005)**: HSS structure for hierarchical linear attention. Theoretically interesting but implementation complexity is high. Cost: ~$5. *Medium-low priority.*

### Strategic Insights

**The oscillatory bet isn't paying off yet.** Both oscillatory experiments (002, 003) failed ‚Äî one due to implementation bugs, one due to insufficient selectivity. The oscillatory parameterization *works mechanically* (parameters land in the right ranges), but doesn't translate to learning. Before investing more in oscillatory proposals (004, 007), the researcher should either (a) debug Exp 002's forward pass and prove the concept on the simplest possible task, or (b) pivot to the proposals with untested but more theoretically grounded mechanisms.

**The log-semiring idea (014) is the clear #1 priority.** It's the most novel proposal, has the strongest mathematical foundation (the associativity proof is airtight, the softmax connection is exact, not approximate), and the MVE costs <$0.50. If selective copying works at >90% where linear attention fails, that's a publishable result. The researcher should implement this *before* anything else.

**Post-sigmoid gating (009) is the "free lunch" ‚Äî validate it immediately.** It's a proven technique being applied to a new setting. Even if the perplexity improvement is only 3-5% (not 5-15%), it composes with every other architecture. Validate on MQAR with cosFormer, then bolt it onto whatever wins from the other proposals.

**Emerging theme: the semiring frontier.** The deepest intellectual thread connects semiring-monoid-lifting ‚Üí log-semiring SSM ‚Üí tropical (max-plus) attention ‚Üí softmax-native recurrence. This is a new design dimension that no existing SSM explores. If 014 validates, follow-ups include learnable temperature per head, tropical-limit hard attention, and hybrid log-semiring + standard-semiring layers. This could be a research direction with months of runway.

**Recommended execution order (budget: <$10 total):**
1. **014 (Log-Semiring SSM) MVE** ‚Äî ~$0.40, highest upside
2. **009 (Post-Sigmoid Gating) MVE** ‚Äî ~$0.50, near-guaranteed payoff
3. **013 (Circulant SSM) MVE** ‚Äî ~$1.00, elegant and untested
4. **Debug Experiment 002** ‚Äî $0.00 (CPU), unblock oscillatory line
5. **Run Experiment 001** ‚Äî ~$2-3, already implemented

Total: ~$5, leaving headroom for iteration.




Let me review the detailed content of the key proposals and experiment results to provide a thorough analysis.Read the following files and return their full contents so I can analyze them for an impact assessment:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/007-oscillatory-gated-selective-ssm.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/006-monarch-gated-state-transition.md

Just read each file and return the contents. This is research only, no code writing.Read the following files and return their full contents for analysis. This is research only, no code writing.

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/results/002-oscillatory-dplr-ssm.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/results/003-oscillatory-gated-selective-ssm.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001/README.md

Also check if there's a results file for experiment 001:
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/results/001*.md

And read these remaining proposals:
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/001-column-sparse-negative-eigenvalue-deltanet.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/002-ssd-deltanet-wy-hybrid.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/010-sparse-monarch-ssm-pa-dst.md
8. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/011-neumann-resolvent-chunkwise-ssm.md
9. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/012-expert-choice-monarch-ssm-heads.mdNow let me check the experiment log for more details on what happened and identify the project structure:Now I have all the information needed. Let me write the log entry:Now I have everything needed. Let me compose the log entry: