---
title: Research Log Update
date: 2026-02-15T18:26:00
timestamp: 2026-02-15 18:26:00
tricks: 1
proposals: 0
experiments: 9
source: legacy_migration
---

## 2026-02-15 â€” 18:26 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Tropical-Gated SSM (Proposal 015)** â€” Priority: **HIGH**
- **Hypothesis**: Replacing the standard $(+, \times)$ semiring with the tropical semiring $(\max, +)$ in SSM recurrences creates hard winner-take-all state dynamics with non-fading memory.
- **Why it matters**: This is the most architecturally novel proposal in the batch. It doesn't just tweak existing SSMs â€” it changes the *algebraic foundation* of the recurrence. The tropical semiring has native parallel-scan compatibility (associativity), and the log-semiring annealing from smoothâ†’hard gives a training-friendly pathway. Experiment 015 (implemented, not yet completed) is already testing this. If the hard-max dynamics enable precise retrieval without softmax, this could be a fundamentally new SSM primitive.
- **Estimated cost**: <$1 (MVE: 2-layer, ~150K params, MQAR task, <5 min GPU)
- **Impact score**: 9/10 â€” Maximum novelty, trivial cost, strong theoretical grounding. The risk is low (training instability from hard max) and the reward is a new class of SSM.

**2. Cyclic Reduction for Dense SSMs (Proposal 026)** â€” Priority: **HIGH**
- **Hypothesis**: Cyclic reduction achieves $O(Tn^3)$ work vs $O(Tn^3 \log T)$ for prefix scan on dense SSM recurrences, saving the $\log T$ factor.
- **Why it matters**: **Already validated.** Experiment 026 (completed) showed **3.88Ã— CPU speedup at T=1024, n=32** with 6.01Ã— GEMM reduction. This is one of the cleanest wins in the batch â€” the math works, the numerics are excellent ($8.5 \times 10^{-16}$ error), and the scaling is monotonic. Experiment 026_scaled is now ready for GPU validation. This directly enables practical training of DeltaNet/DeltaProduct with non-diagonal transitions.
- **Estimated cost**: <$1 (MVE already done; GPU kernel benchmark ~$2-5)
- **Impact score**: 9/10 â€” Already proven on CPU. The only remaining question is whether GPU tensor cores amplify or diminish the advantage. Near-guaranteed payoff.


### ðŸ§ª Experiment Updates

**Completed (with results):**

- **Experiment 026: Cyclic Reduction vs Prefix Scan** âœ… SUCCESS
  - 3.88Ã— speedup at T=1024, n=32. Work-optimal and hardware-friendly. Ready for GPU scaling.
  - Cost: $0.00 (CPU only)

- **Experiment 007: OscGate-SSM** âœ… SUCCESS
  - 93% accuracy on selective copying (vs 47% for LTI baseline). Proves input-dependent oscillatory gating enables selectivity while preserving stability.
  - Cost: $0.00

- **Experiment 025: NystrÃ¶m Landmark Compression** âœ… SUCCESS
  - 99.25% accuracy with 3.2Ã— memory compression. Model **co-adapts** with compression â€” even though approximation error is high (0.86â€“0.91), the network routes information through preserved dimensions.
  - Cost: $0.05

- **Experiment 011: Neumann Resolvent DPLR** âœ… SUCCESS
  - k=4 terms gives $6.65 \times 10^{-5}$ error, 8.92Ã— speedup at N=256. Near-resonance motivation is weak (HiPPO eigenvalues already well-separated), but raw speed is real.
  - Cost: $0.00

- **Experiment 002: SSD-DeltaNet Block Decomposition** âŒ FAIL
  - 0.84Ã— (16% slower). Python/CUDA kernel launch overhead dominates at 3,746 launches. Needs fused Triton kernel to realize theoretical savings.
  - Cost: $0.10

- **Experiment 022: Displacement-Rank SSM** âŒ FAIL
  - Cauchy $1/(s_i - s_j)$ structure creates **ill-conditioned gradients**. All Cauchy models fail at seq_len=20 (<4%) while dense reaches 97.2%. Beautiful math, broken optimization.
  - Cost: $0.00

- **Experiment 029: Circulant FAVOR+** âŒ FAIL
  - Circulant projection matches dense FAVOR+ (23.8% â‰ˆ 23.1%), validating CBE theory. But FAVOR+ itself is catastrophically broken on associative recall (23% vs 98.5% for ReLU linear attention).
  - Cost: $0.10

- **Experiment 004: Oscillatory-DPLR** âŒ FAIL
  - Forward pass bug prevents any learning (MSE stayed at ~0.85). Learned Ï‰ distribution is correct, suggesting the parameterization works but implementation needs debugging.
  - Cost: $0.00

- **Experiment 005: HSS Linear Attention** âŒ FAIL
  - 6.3Ã— slower than dense baseline. GPU-unfriendly tree traversals kill performance. Memory advantage only appears at dâ‰¥1024.
  - Cost: $0.15

**Implemented (awaiting completion):**
- Experiments 010, 014, 015, 016, 017, 019, 020, 021, 027, 028, 030, 031 â€” all implemented and ready to run


### ðŸ“š New Discoveries (Key Themes from 191 Tricks)

The 191 tricks documented today cluster into several powerful themes:

- **Circulant/Structured Decompositions** (028, 032, 084, 129, 175, 180, 181): A deep bench of circulant approximation theory â€” optimal (Chan's), superoptimal (Tyrtyshnikov's), real-valued low-rank, Gohberg-Semencul inverse â€” all enabling $O(n \log n)$ operations. These are the building blocks for circulant SSMs.

- **Hierarchical Matrix Algebra** (060, 097, 098, 127, 146): HSS, HODLR, and $\mathcal{H}^2$ matrix machinery â€” fast solvers, eigendecomposition (SuperDC: 136Ã— faster than LAPACK at n=32K), and telescopic decomposition. While Experiment 005 showed HSS is GPU-unfriendly for small models, the algebra remains powerful for large-scale structured layers.

- **GPU Kernel Fusion** (039, 046, 051, 061, 075, 091, 135, 141, 183): A comprehensive toolkit from epilogue visitor trees to persistent megakernels to warp specialization. These are the *implementation* tricks that turn theoretical savings into wall-clock speedups.

- **Parallel Scan Variants** (099, 107, 166, 167, 172, 173): MatMulScan, segmented scan, decoupled look-back â€” modern scan primitives designed for tensor cores rather than scalar units.

- **Random Feature Approximations** (045, 149, 150, 155, 162): From FAVOR+ to SORF to Chefs' Random Tables â€” structured random projections for kernel approximation, all with $O(d \log d)$ cost.

- **N:M Sparsity Toolchain** (130, 133, 136, 140): Transposable masks (TSENOR), hierarchical V:N:M, smooth STE â€” a complete toolkit for training with hardware-accelerated sparsity.


### Other Proposals (Notable)

- **GS-Monomial SSM (016)**: Monomial state transitions with shuffle permutation. Experiment 016 is implemented. <$1 MVE. Impact 8/10.
- **Circulant-Diagonal SSM (023)**: CD product transitions at $O(n \log n)$. Clean and simple. <$1 MVE. Impact 7/10.
- **Cayley-Circulant Orthogonal SSM (027)**: Exact orthogonality + FFT. Experiment 027 implemented. <$1 MVE. Impact 7/10.
- **Near-Far Field Chunkwise GLA (036)**: FMMformer decomposition for 2-4Ã— larger chunks. <$2 MVE. Impact 7/10.
- **Chimera-Fused Chunkwise SSM (032)**: GEMM-chain fusion for 40-60% HBM reduction. <$2 MVE. Impact 7/10 (pure kernel optimization, numerically identical).
- **Warp-Specialized Chunkwise Linear RNN (039)**: FlashAttention-3 pipelining for linear RNNs. Impact 7/10 but requires Hopper hardware.
- **Persistent Megakernel Linear RNN (040)**: Fuse entire layer into one kernel. Impact 8/10 but extreme engineering effort.
- **V:N:M Sparse SSM Projections (031)**: 60-75% sparsity on projections. Experiment 031 implemented. <$2 MVE. Impact 7/10.
- **Transposable Sparse GLA (035)**: 4:8 sparsity on all projections. Impact 7/10 but needs A100+.
- **Hutchinson Adaptive Rank DPLR (018)**: Only *medium* priority â€” adaptive rank allocation via trace estimates. Interesting but incremental.


### Strategic Insights

**Theme 1: The "structured non-diagonal SSM" frontier is wide open.** Four successes (OscGate-SSM, NystrÃ¶m compression, Neumann resolvent, cyclic reduction) and three failures (DR-SSM, FAVOR+, HSS attention) reveal a clear pattern: **approaches that maintain GPU-friendly operations (BMM, FFT, elementwise) succeed; approaches requiring sequential tree traversals or ill-conditioned Cauchy kernels fail.** The winning proposals (Tropical SSM, GS-Monomial, Circulant-Diagonal) all stay in BMM/FFT territory.

**Theme 2: Kernel fusion is the next multiplier.** With 9 kernel fusion tricks documented (EVT, Chimera, FlashFuser, persistent megakernel, warp specialization), the gap between "what the math says" and "what the GPU delivers" is the primary bottleneck. Experiment 002's failure (correct math, 16% slower) is the cautionary tale. Proposals 032, 038, 039, 040, 041 all target this gap.

**Theme 3: Recommended execution order (cost-optimized):**
1. **Run Experiments 015, 016, 027** â€” all implemented, <$1 each, test the three most novel SSM parameterizations (tropical, GS-monomial, Cayley-circulant)
2. **Scale Experiment 026** to GPU (026_scaled) â€” validates the already-proven cyclic reduction on tensor cores
3. **Implement Proposal 023** (Circulant-Diagonal SSM) â€” simplest untested structured-transition idea
4. **Then** tackle kernel optimization proposals (032, 038) once the best SSM parameterization is identified

**Total budget for steps 1-3: ~$3-5.** This covers the highest-novelty, highest-impact experiments in the batch.




I need to analyze this massive activity dump â€” 201 tricks, 32 proposals, and 27 experiments. Let me first read the key proposal and experiment files to understand the details needed for impact assessment.

Let me also check the kernel-focused and GPU-specific proposals that might have different cost profiles.

Let me also read the remaining proposal files to get a complete picture of the sparsity and distributed training proposals.

Let me check on the progress of the agents that have been running longest.Good, I'm getting detailed results data. Let me wait for all the agents to complete and then also check the experiment log for a full picture.Now let me wait for the remaining agents and then compile the log.All agents have completed. I now have comprehensive data on all experiments, proposals, and tricks. Let me compile the log entry.