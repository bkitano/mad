---
title: Research Log Update
date: 2026-02-15T15:58:00
timestamp: 2026-02-15 15:58:00
tricks: 1
proposals: 7
experiments: 9
source: legacy_migration
---

## 2026-02-15 â€” 15:58 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention / SSM Readout** (Proposal 009) â€” Priority: **HIGH**
- **Hypothesis**: Applying sigmoid gating after linear attention readout breaks the low-rank bottleneck in the output path, yielding 5â€“15% perplexity improvement with <2% latency overhead.
- **Why it matters**: This is the *only* proposal targeting the readout/output path â€” every other proposal focuses on state transitions. It's orthogonal and composable with all other work. The technique (from Qiu et al., NeurIPS 2025 Best Paper) is proven for softmax attention but has *never* been applied to linear attention or SSMs, where the bottleneck is arguably worse (fixed dÃ—d state vs. TÃ—T matrix). Dead simple to implement.
- **Estimated cost**: <$1 for MVE, ~$32 for small-scale
- **Impact score**: 9/10 â€” Extremely high cost-effectiveness. Near-zero risk (it either helps or doesn't, can't break anything), already has experiment 009 implemented and ready to run.

**2. Circulant SSM: Fourier-Domain Parallel Scan** (Proposal 013) â€” Priority: **HIGH**
- **Hypothesis**: Circulant state transitions, diagonalized via FFT, enable element-wise parallel scans in frequency space â€” combining the O(log T) parallelism of diagonal SSMs with full coordinate mixing, at O(n log n) per step.
- **Why it matters**: Elegantly sidesteps the core SSM dilemma. Instead of factoring dense matrices (Monarch, Householder), it exploits that circulants are already diagonalized by the DFT â€” so the parallel scan runs on scalars in frequency space. Already has experiment 013 implemented. The commutativity limitation (circulant algebra is abelian) is real but well-characterized: it should excel on cyclic groups and fail on non-abelian tasks, giving a clean scientific signal.
- **Estimated cost**: <$0.25 for MVE, ~$8 for small-scale
- **Impact score**: 8.5/10 â€” Cheapest proposal to validate, clean theoretical story, and the abelian limitation directly motivates the upgrade path (Proposal 023: Circulant-Diagonal, which breaks commutativity).


### ðŸ§ª Experiment Updates

**Experiment 026: Cyclic Reduction vs Prefix Scan** (Status: âœ… completed)
- **Proposal**: 026-cyclic-reduction-randmscan-ssm-recurrence
- **Key findings**: **3.88Ã— wall-clock speedup** over prefix scan at T=1024, matching the theoretical prediction. CR even beat sequential scan (18.55ms vs 61.34ms) because `torch.bmm` thrashes Python-loop sequential code. Numerically more accurate than prefix scan.
- **Cost**: $0.00 (CPU-only)
- **Verdict**: âœ… PROCEED â€” validates that cyclic reduction is the right parallelization primitive for dense SSM recurrences. GPU validation warranted.

**Experiment 007: OscGate-SSM** (Status: âœ… completed)
- **Proposal**: 007-oscillatory-gated-selective-ssm
- **Key findings**: **93.0% accuracy** on selective copying vs 46.8% for LTI baseline â€” a 46 point gap proving input-dependent oscillatory parameters create selectivity while maintaining stability (zero NaN events). Nearly matched unconstrained diagonal SSM (94.8%).
- **Cost**: $0.00 (CPU-only)
- **Verdict**: âœ… PROCEED â€” input-dependent oscillatory SSMs are viable. Feeds directly into Proposal 020 (OH-DeltaProduct).

**Experiment 029: Circulant FAVOR+** (Status: âœ… completed)
- **Proposal**: 029-circulant-favor-plus-linear-attention
- **Key findings**: FAVOR+ itself is fundamentally broken for associative recall (23% test accuracy). Plain ReLU linear attention hits 98.5%. The circulant optimization works (matches dense FAVOR+) but optimizes a broken approach.
- **Cost**: $0.10
- **Verdict**: âŒ ABANDON â€” optimizing FAVOR+ is pointless when simpler methods dominate.

**Experiment 022: Displacement-Rank SSM** (Status: âœ… completed)
- **Proposal**: 022-displacement-rank-ssm-state-transitions
- **Key findings**: Cauchy-like state transitions are **untrainable** in practice. At Î±=4, the model collapsed to <4% accuracy while unconstrained dense reached 97.2%. The 1/(s_i âˆ’ s_j) kernel creates ill-conditioned gradients.
- **Cost**: $0.00
- **Verdict**: âŒ ABANDON â€” theoretical expressivity â‰  practical learnability.

**Experiment 025: NystrÃ¶m Landmark Chunkwise SSM** (Status: âœ… completed)
- **Proposal**: 025-nystrom-landmark-chunkwise-ssm
- **Key findings**: **99.25% accuracy with 4Ã— state compression** (m=2 landmarks, n=8). Surprisingly, the model doesn't learn low-rank states as predicted â€” it *co-adapts* with the compression, routing information through preserved dimensions. The approximation error is high (0.86â€“0.91) yet accuracy is near-perfect.
- **Cost**: $0.05
- **Verdict**: âœ… PROCEED â€” but the mechanism is different than hypothesized. The co-adaptation story is more interesting than the NystrÃ¶m approximation story.

**Experiment 002: SSD-DeltaNet Block Decomposition** (Status: âœ… completed)
- **Proposal**: 002-ssd-deltanet-wy-hybrid
- **Key findings**: 16% *slower* than naive sequential due to Python kernel launch overhead (3746 individual CUDA kernel launches). Math is correct, implementation kills it.
- **Cost**: $0.10
- **Verdict**: âŒ ABANDON at PyTorch level â€” needs custom Triton/CUDA.

**Experiment 011: Neumann Resolvent** (Status: âœ… completed)
- **Proposal**: 011-neumann-resolvent-chunkwise-ssm
- **Key findings**: k=4 Neumann terms achieve <1e-4 error, **8.92Ã— speedup** at N=256. However, the near-resonance motivation is a non-issue (HiPPO eigenvalues are well-separated).
- **Cost**: $0.00
- **Verdict**: âš ï¸ PROCEED with caveats â€” speedup is real but the standard Cauchy kernel trick already achieves similar scaling.

**Experiment 005: HSS Linear Attention** (Status: âœ… completed) â€” âŒ ABANDON (both HSS and dense failed the task; HSS 6.3Ã— slower)

**Experiment 004: Oscillatory-DPLR** (Status: âœ… completed) â€” ðŸ› DEBUG (flat loss despite correct parameter ranges; likely implementation bug in complex dtype handling)

**17 experiments implemented, awaiting runs**: 003, 006, 009, 010, 012, 013, 014, 015, 016, 017, 019, 020, 021, 027, 028, 030, 031


### ðŸ“š New Discoveries

- **167 tricks documented** in a single 12-hour window â€” a massive expansion covering decomposition (HSS, circulant, displacement-rank, Monarch), GPU kernels (warp specialization, CTA swizzling, EVT fusion, persistent megakernels), algebraic structures (tropical semirings, Krohn-Rhodes, hyperoctahedral groups), and approximation methods (FAVOR+, NystrÃ¶m, random features).

Key highlights:
- **Tropical Attention (132)**: Max-plus attention operating in tropical projective space â€” first bridge between tropical geometry and neural attention. Enables combinatorial algorithm reasoning.
- **Semiring Monoid Lifting (108)**: Replace (R, +, Ã—) with alternative semirings in neural nets. The tropical and log semirings directly power Proposals 014 and 015.
- **V:N:M Hierarchical Sparsity (140)**: Goes beyond 2:4 to 60â€“75% sparsity ratios while still leveraging Sparse Tensor Cores. Critical for Proposal 031.
- **PaTH Attention (151)**: Data-dependent position encoding via accumulated Householder transformations â€” conceptual sibling of DeltaProduct, applying the Householder idea to position encoding rather than state transitions.
- **TFLA Two-Level Tiled Chunkwise (158)**: The state-of-the-art kernel for linear RNN training. Multiple proposals (038, 039, 040) build directly on this as the baseline to beat.
- **Neumann-Series CWY Inverse (157)**: Replaces sequential triangular solves with parallel matrix multiplications for the WY representation â€” directly accelerates DeltaNet/DeltaProduct training.


### Other Proposals

- **Log-Semiring SSM (014)**: Logsumexp scan = exact softmax as recurrence. High novelty but sign-tracking complexity and no tensor-core acceleration are concerns. MVE already implemented.
- **Tropical-Gated SSM (015)**: Hard max-plus scan for winner-take-all dynamics. Elegant theory but sparse gradients require careful annealing. MVE implemented.
- **GS-Monomial SSM (016)**: Group-and-Shuffle monomial state transitions. Clever blend of block-diagonal and permutation mixing. MVE implemented.
- **Hyperoctahedral Signed-Permutation SSM (017)**: Signed permutations (B_n group) for state transitions. Strictly more expressive than pure permutations. MVE implemented.
- **Oscillatory Householder DeltaProduct (020)**: Oscillatory stability + Householder expressivity. Deep mathematical grounding but most expensive to validate (~$200+ for full scale). MVE implemented.
- **CD-SSM (023)**: Circulant-diagonal factored transitions. Breaks the commutativity limit of pure circulant (013) but scan composition blowup is a real concern. MVE: <$0.25.
- **Cayley-Circulant Orthogonal SSM (027)**: Cayley transform of skew-circulant for exact orthogonality at O(n log n). MVE implemented.
- **Neumann-Cayley Orthogonal SSM (028)**: Neumann-approximated Cayley transform for input-dependent orthogonal transitions. Validated by Experiment 011's Neumann results.
- **EVT-Fused SSM Epilogues (033)**: Pure systems optimization â€” fuse SwiGLU/gate/residual into GEMM epilogues. Zero algorithmic risk but requires CUTLASS 3.x engineering.
- **Warp-Specialized Chunkwise Linear RNN (039)**: FlashAttention-3's pingpong scheduling applied to TFLA kernels. High potential speedup (1.5â€“2Ã—) but requires Hopper (H100) GPU and deep CUDA expertise.
- **Persistent Megakernel Linear RNN (040)**: Fuse entire linear RNN layer into one kernel. Most ambitious systems proposal â€” highest reward but highest implementation effort.
- **Kernel-focused proposals (032, 034, 035, 036, 037, 038)**: Various optimizations targeting chunkwise SSM throughput via GEMM chain fusion, Stream-K, BRGEMM, CTA swizzling, and random feature maps.


### Strategic Insights

**The clearest finding from the last 12 hours is that theoretical mathematical elegance often fails in practice while simple, composable ideas succeed.** Displacement-rank SSMs (Cauchy-like) and FAVOR+ were mathematically beautiful but practically untrainable or dominated by trivial baselines. Meanwhile, the cheapest interventions â€” post-sigmoid gating (009), circulant FFT diagonalization (013), and NystrÃ¶m co-adaptation (025) â€” showed the most promise per dollar spent.

**Recommended focus order for the next 24 hours** (total budget: ~$5):
1. **Run Experiment 009** (post-sigmoid gating) â€” $0.50. Lowest risk, highest composability, addresses the only unoptimized part of the SSM pipeline (readout).
2. **Run Experiment 013** (circulant SSM) â€” $0.25. Cleanest theoretical prediction, will definitively test whether FFT-domain parallel scan works for state tracking.
3. **Run Experiment 014** (log-semiring SSM) â€” $0.40. The most intellectually exciting proposal â€” if logsumexp scan = exact softmax recurrence, it would be a fundamental advance.
4. **Run Experiment 027** (Cayley-circulant) â€” $0.25. Tests orthogonal stability on delayed copy, feeds into the orthogonal SSM thesis.
5. **Run Experiment 016** (GS-Monomial) or **017** (hyperoctahedral) â€” $0.25 each. Tests non-abelian state tracking with structured matrices.

**Emerging theme**: The research is converging on a taxonomy of SSM state-transition structures ordered by expressivity cost: Diagonal â†’ Circulant â†’ Circulant-Diagonal â†’ Monomial â†’ Monarch â†’ Householder â†’ Dense. Each step adds one algebraic capability (coordinate mixing, sign flipping, non-commutativity) at a known cost. The remaining experiments will fill in this landscape empirically.




I need to analyze this massive activity dump â€” 177 tricks, 34 proposals, and 27 experiments. Let me start by reading the key proposal and experiment files to understand the details before writing the log.Read the full results files for the completed experiments. I need to understand what was learned from each completed experiment. Read these files:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/results.md (or RESULTS.md or similar)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/022/results.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/029/results.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/026/results.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/results.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/005/results.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/007/results.md
8. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/011/results.md
9. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/025/results.md

Search for results files in each experiment directory. Try patterns like results.md, RESULTS.md, results/*.md, etc. Report the full content of each results file found.Read the full content of these proposal files to understand their experimental design and cost estimates:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/016-gs-monomial-ssm-state-transition.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/026-cyclic-reduction-randmscan-ssm-recurrence.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/027-cayley-circulant-orthogonal-ssm.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/023-circulant-diagonal-ssm-state-transitions.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/032-chimera-fused-chunkwise-ssm-scan.md
8. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/033-evt-fused-ssm-swiglu-epilogues.md

For each, I need: the hypothesis, estimated cost, what models/tasks are proposed, and success criteria. Report the full content of each file.Now let me check which experiments are still in "implemented" status (running or awaiting results) and get a sense of priorities:Find and read the experiment log file at /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/experiment-log.md (or similar path). Also check if there are any results files in the "implemented" experiments that I haven't read yet:

- code/010/ (Sparse Monarch SSM with PA-DST)
- code/014/ (Log-Semiring SSM)
- code/015/ (Tropical-Gated SSM)
- code/016/ (GS-Monomial SSM)
- code/017/ (Hyperoctahedral SSM)
- code/019/ (Capacitance-Coupled Multi-Scale SSM)
- code/028/ (Neumann-Cayley SSM)
- code/030/ (Group-Matrix Displacement Rank SSM)
- code/031/ (V:N:M Sparse SSM)

Check each directory for results.md files. Also look for any aggregate experiment tracking.Now I have a comprehensive picture. Let me write the log entry.