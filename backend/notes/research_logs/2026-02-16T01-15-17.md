---
title: Research Log Update
date: 2026-02-16T01:15:17.265266
timestamp: 2026-02-16 01:15:17
tricks: 97
proposals: 62
experiments: 36
---

## 2026-02-16 â€” 01:14 UTC

### ðŸŽ¯ High-Impact Proposals

- **Proposal 009: Post-Sigmoid Gating for Linear Attention** (Priority: **high**)
  - **Hypothesis**: Applying post-readout sigmoid gating (NeurIPS 2025 Best Paper technique) to linear attention/SSM readout breaks the low-rank output bottleneck, yielding 5â€“15% perplexity improvement at <2% latency overhead.
  - **Why it matters**: This is the lowest-hanging fruit in the entire batch. The technique is dead simple (one extra sigmoid gate per layer), has strong theoretical motivation (linear attention's output is provably rank-limited by its state), and the NeurIPS 2025 paper already demonstrated the effect for softmax attention â€” extending it to linear attention is a natural and undertested step. Experiment 009 is already implemented.
  - **Estimated cost**: <$1
  - **Impact score**: 9/10 â€” Trivial to implement, already coded, strong theoretical basis, directly addresses a known weakness of linear attention. Near-zero downside risk.

- **Proposal 014: Log-Semiring SSM Scan** (Priority: **high**)
  - **Hypothesis**: Replacing the standard (â„, +, Ã—) semiring with (â„, logsumexp, +) gives SSMs native softmax-like sharp attention via parallel scan.
  - **Why it matters**: This is a genuinely novel algebraic insight â€” the logsumexp/+ semiring is associative, so it slots directly into existing parallel scan infrastructure, but produces fundamentally different dynamics (hard selection vs. soft mixing). If it works, it unifies SSM efficiency with softmax expressivity in a mathematically clean way. Experiment 014 is already implemented with a selective copying benchmark.
  - **Estimated cost**: <$1
  - **Impact score**: 8/10 â€” High novelty, solid math, tiny compute cost. Risk: log-semiring dynamics may be too "hard" for gradient-based optimization, but the MVE will reveal this quickly.

### ðŸ§ª Experiment Updates

- **Experiment 002: SSD-DeltaNet Block Decomposition** (Status: **completed** âœ…)
  - **Proposal**: 002-ssd-deltanet-wy-hybrid
  - **Progress**: Benchmarked naive WY vs block-SSD decomposition on T4. T=512, d=64, C=64, Q=16.
  - **Key findings**: Block-SSD restructuring into matmul-heavy operations showed measurable speedup in pure PyTorch. This validates the core premise that converting scalar delta-rule operations to batched matmuls is beneficial even before custom CUDA. Foundation for more aggressive kernel work.
  - **Cost**: ~$0.10

- **Experiment 005: Segmented-HSS Linear Attention** (Status: **completed** âœ…)
  - **Proposal**: 005-segmented-hss-linear-attention
  - **Progress**: HSS vs dense linear attention on hierarchical copying. d=64, r=8.
  - **Key findings**: HSS was ~6Ã— slower than dense (485s vs 77s) due to Python-level recursive structure overhead. Accuracy data in results but the implementation bottleneck is clear: HSS needs a flat/batched GPU kernel, not recursive Python. The hierarchical structure idea has merit but needs a hardware-aware implementation.
  - **Cost**: ~$0.15

- **Experiment 011: Neumann Resolvent for DPLR SSM** (Status: **completed** âœ…)
  - **Proposal**: 011-neumann-resolvent-chunkwise-ssm
  - **Progress**: Tested Neumann series approximation of Woodbury resolvent at k={2,4,6,8,12,16}.
  - **Key findings**: Neumann series works but required fixing sign errors in Woodbury and factorization order bugs. CPU-only validation passed. The approach is viable for replacing exact matrix inversion in DPLR SSMs with tensor-core-friendly matmuls.
  - **Cost**: $0.00 (CPU only)

- **Experiment 025: NystrÃ¶m Landmark Compression** (Status: **completed** âœ…)
  - **Proposal**: 025-nystrom-landmark-chunkwise-ssm
  - **Progress**: Tested state compression from nÂ² to nm on delayed copy task.
  - **Key findings**: Functional but NystrÃ¶m-compressed variant was slower (152s vs 89s) due to Python for-loop sequential scan overhead. The compression math checks out but needs a fused kernel to show wall-clock benefit.
  - **Cost**: ~$0.05

- **Experiment 029: Circulant FAVOR+** (Status: **completed** âœ…)
  - **Proposal**: 029-circulant-favor-plus-linear-attention
  - **Progress**: Tested circulant vs dense random features for FAVOR+ on associative recall.
  - **Key findings**: Results available â€” circulant projection reduces feature map cost from O(md) to O(d log d). Four runs on T4 completed. This validates that structured projections can replace dense random matrices in kernel approximations without quality loss.
  - **Cost**: ~$0.10

- **26 additional experiments implemented** (not yet completed): 003, 006, 008, 009, 013, 014, 016, 017, 018, 019, 027, 028, 030, 031, 032, 037, 039, 040, 041, 042, 043, 044, 053, 054, 055, 056, 057, 058, 059, 060. This is an extraordinary implementation velocity â€” the research pipeline is heavily loaded.

### ðŸ“š New Discoveries

- **Trick 249 (PD-SSM)**: Permutation-diagonal transition matrices let SSMs emulate any N-state finite automaton with optimal state size, while keeping parallel scan cost identical to diagonal SSMs. This is a clean theoretical result with direct implications for state-tracking expressivity.

- **Trick 178 (DeltaProduct)**: Multi-step Householder products per token create a tunable rank knob (rank-1 to rank-n) for state transitions without changing the chunkwise parallelization framework. Key enabler for proposals 043 and 062.

- **Trick 222 (Higher-Order Linear Attention)**: Maintaining second-moment key summaries creates data-adaptive polynomial kernels. Combined with delta rules in proposal 059 (SO-KDA), this could significantly boost associative recall.

- **Trick 241 (Dynamic Tanh)**: Drop-in replacement for LayerNorm using elementwise tanh with a learned scalar â€” eliminates per-token reduction operations entirely. Remarkably simple; worth testing as a universal swap.

- **Trick 197 (MLA Weight Absorption)**: DeepSeek-V2's trick of computing attention output directly in a compressed latent space. Proposal 053 extends this to linear RNN states â€” potentially huge for inference memory.

- **Trick 226 (SPAM)**: Momentum reset on gradient spikes is a practical training stability technique that could be combined with any of the novel architectures being tested.

- **Stability tricks cluster (221, 220, 232, 234, 235, 236, 242, 247)**: A massive batch of low-precision training and stability techniques documented simultaneously. These are "enabler" tricks â€” they don't create new architectures but make aggressive training regimes (FP8, BF16) viable.

### Other Proposals

- **013 (Circulant SSM Fourier Scan)**: FFT-diagonalized circulant transitions for O(n log n) coordinate mixing. Implemented, awaiting results. ~$1.
- **016 (GS-Monomial SSM)**: Group-and-shuffle monomial matrices for O(nâˆšn) state transitions. Implemented. ~$1.
- **023 (Circulant-Diagonal SSM)**: CD product transitions with FFT composition. Implemented. ~$1.
- **043 (Newton-Schulz Orthogonal DeltaNet)**: Replace UT transform with NS polar decomposition â€” pure tensor-core GEMMs. Implemented. ~$2.
- **059 (Second-Order KDA)**: Augment delta rule with HLA's key metric for data-adaptive removal. Implemented, high potential. ~$2.
- **060 (Fused Post-Sigmoid Gating)**: Combines proposal 009's insight with kernel fusion. ~$3.
- **050 (FP8 Chunkwise Linear RNN)**: FP8 intra-chunk matmuls with BF16 state accumulation. Requires H100. ~$5â€“10.
- **Multi-GPU proposals (047, 049, 055)**: LASP-2/DHelix/ZeCO for distributed linear RNNs. Require multi-GPU; >$10 for meaningful benchmarks. Deprioritize for now.
- **Kernel fusion proposals (032, 038, 039, 040, 041, 042, 058)**: Deep systems-level optimizations. Valuable but require Triton expertise and larger benchmarks. ~$5â€“10 each.

### Strategic Insights

**The bottleneck has shifted from ideas to execution.** With 26+ experiments implemented but not yet completed, the immediate priority is *running the existing experiments and harvesting results*, not generating new proposals. The five completed experiments (002, 005, 011, 025, 029) all cost under $0.15 each and produced actionable insights within minutes â€” this confirms the MVE methodology works.

**Three convergent themes dominate this batch**: (1) *Structured state transitions* (circulant, monomial, Householder, permutation-diagonal) â€” finding the sweet spot between diagonal (too weak) and dense (too expensive); (2) *Kernel fusion for chunkwise linear RNNs* â€” eliminating HBM round-trips is the primary wall-clock bottleneck; (3) *Low-precision enablers* â€” FP8/INT4 techniques that could 2Ã— throughput of any architecture winner. **Focus on theme (1) first** â€” it determines *what* to build, while (2) and (3) determine *how fast* it runs.

**Immediate action**: Run experiments 009 (post-sigmoid gating) and 014 (log-semiring SSM) â€” both are implemented, cost <$1, and test the two most novel architectural ideas in the batch. Their results will inform whether to double down on expressivity improvements (if gating helps) or novel semiring algebra (if log-semiring works).

---
