---
title: Research Log Update
date: 2026-02-16T01:15:22.911325
timestamp: 2026-02-16 01:15:22
tricks: 97
proposals: 62
experiments: 36
---

## 2026-02-16 â€” 01:14 UTC

### ðŸŽ¯ High-Impact Proposals

- **Proposal 009: Post-Sigmoid Gating for Linear Attention** (Priority: **high**)
  - **Hypothesis**: Applying post-readout sigmoid gating (NeurIPS 2025 Best Paper technique) to linear attention/SSM readout breaks the low-rank output bottleneck, improving quality by 5â€“15% with <2% latency overhead.
  - **Why it matters**: This is the single highest bang-for-buck idea in the queue. It's architecturally trivial (one extra linear projection + sigmoid), requires zero custom kernels, and targets a *known* expressivity bottleneck in every linear attention variant. The insight that the benefit should be *larger* for linear attention than softmax (because linear attention already suffers worse low-rank collapse) is well-grounded. Experiment 009 is already implemented with an MQAR benchmark â€” results will be decisive.
  - **Estimated cost**: <$1
  - **Impact score**: 9/10 â€” Near-zero implementation risk, universally applicable to GLA/Mamba-2/DeltaNet/mLSTM, and validated theory from softmax attention transfers cleanly.

- **Proposal 014: Log-Semiring SSM Scan** (Priority: **high**)
  - **Hypothesis**: Replacing the standard (+, Ã—) semiring with (logsumexp, +) in SSM parallel scans produces a recurrence whose hidden state natively computes softmax-weighted attention over input history.
  - **Why it matters**: This is a fundamentally novel algebraic reformulation â€” not an approximation but an *exact* softmax-like mechanism within the scan framework. If it works on selective copying (which requires hard attention), it would be the first demonstration that SSM recurrences can match softmax's sharp retrieval without kernel approximations. Already implemented and ready to run.
  - **Estimated cost**: <$1
  - **Impact score**: 8.5/10 â€” High novelty, elegant theory, trivial compute cost. Risk is that logsumexp parallelization may have numerical edge cases, but the selective copying task will expose this cheaply.

### ðŸ§ª Experiment Updates

- **Experiment 002: SSD-DeltaNet Block Decomposition** (Status: **completed** âœ…)
  - **Proposal**: 002-ssd-deltanet-wy-hybrid
  - **Progress**: Fully implemented and benchmarked on T4 GPU. Two PyTorch implementations compared (Naive WY vs Block-SSD) at T=512, d=64, C=64.
  - **Key findings**: Block-SSD restructuring into matmul-heavy operations works as a proof of concept. Cost was ~$0.10. This validates that the WYâ†’SSD reformulation is algebraically correct and sets up custom kernel work.
  - **Cost**: ~$0.10 actual vs <$1 estimated

- **Experiment 029: Circulant FAVOR+** (Status: **completed** âœ…)
  - **Proposal**: 029-circulant-favor-plus-linear-attention
  - **Progress**: Tested circulant random projections vs dense FAVOR+ on associative recall (8 KV pairs, seq_len=64).
  - **Key findings**: All four variants (Dense FAVOR+, C-FAVOR+, ReLU, Softmax) compared at ~119K params. Circulant projection achieves O(d log d) feature computation. Results available for analysis.
  - **Cost**: ~$0.10 actual

- **Experiment 011: Neumann Resolvent for DPLR SSM** (Status: **completed** âœ…)
  - **Proposal**: 011-neumann-resolvent-chunkwise-ssm
  - **Key findings**: Neumann series at k=8 achieves <1e-3 relative error vs exact Woodbury. Implementation bugs found and fixed (Woodbury sign error, Neumann factorization order). CPU-only validation â€” no GPU needed.
  - **Cost**: $0.00

- **Experiment 005: Segmented-HSS Linear Attention** (Status: **completed** âœ…)
  - **Proposal**: 005-segmented-hss-linear-attention
  - **Key findings**: HSS linear attention ran 6.3Ã— slower than dense baseline (485s vs 77s) due to Python-level overhead of recursive HSS structure. The hierarchical state representation works mathematically but needs native kernel support to be practical. Important negative result for GPU efficiency.
  - **Cost**: ~$0.15

- **Experiment 025: NystrÃ¶m Landmark Compression** (Status: **completed** âœ…)
  - **Proposal**: 025-nystrom-landmark-chunkwise-ssm
  - **Key findings**: Compression from nÂ²â†’nm works but required significant downscaling from proposal specs (n=8 instead of 32) due to sequential scan overhead. Validates the compression idea but highlights that Python-level sequential loops are the real bottleneck, not the linear algebra.
  - **Cost**: ~$0.05

- **26 experiments newly implemented** (status: implemented, awaiting results): 003, 006, 008, 009, 013, 014, 016, 017, 018, 019, 027, 028, 030, 031, 032, 037, 039, 040, 041, 042, 043, 044, 053, 054, 055, 056, 057, 058, 059, 060. This is a massive batch â€” prioritize running 009 and 014 first.

### ðŸ“š New Discoveries

- **Trick 249 (PD-SSM Permutation-Diagonal Transitions)**: SSMs can emulate *any* N-state finite automaton with optimal state size N by using input-dependent column one-hot Ã— diagonal transitions â€” provably optimal and scan-compatible. This is the theoretical ceiling for state-tracking expressivity.

- **Trick 178 (DeltaProduct Multi-Step Householder)**: Taking n_h gradient descent steps per token gives diagonal+rank-n_h transitions â€” a clean expressivity knob between DeltaNet (rank-1) and dense matrices. The WY representation extends naturally, making this immediately practical.

- **Trick 197 (MLA Weight Absorption)**: DeepSeek-V2's trick of absorbing projection weights into the latent KV cache eliminates decompression during inference. Proposal 053 correctly identifies this as transferable to linear RNN state compression â€” a potentially transformative inference optimization.

- **Trick 222 (Higher-Order Linear Attention)**: Maintaining second-moment key statistics enables data-adaptive polynomial kernels, directly addressing linear attention's rank bottleneck. Proposal 059 (SO-KDA) smartly combines this with KDA's delta rule.

- **Trick 241 (Dynamic Tanh / DyT)**: Drop-in replacement for LayerNorm using elementwise tanh with a learnable scale â€” eliminates per-token reduction operations entirely. Trivial to test and potentially significant for kernel fusion.

- **Trick 226 (SPAM â€” Spike-Aware Adam)**: Periodic momentum reset to flush gradient spike contamination from Adam's EMA. Directly applicable to any training run experiencing loss spikes.

- **Stability tricks cluster (221, 220, 234, 236, 227, 232, 242, 247)**: A massive influx of FP8/BF16 training stability techniques â€” Kahan summation, ÏƒReparam, TWEO outlier prevention, unit scaling, stochastic rounding, Smooth-SwiGLU, Peri-LN, scaled embeddings. These form a coherent "stable low-precision training" toolkit.

### Other Proposals

- **Proposal 060 (Fused Post-Sigmoid Gating for Chunkwise Linear RNN)**: Extends proposal 009 by fusing the sigmoid gate into the chunkwise kernel epilogue â€” the natural next step if 009 validates.
- **Proposal 059 (Second-Order KDA)**: Augments KDA with HLA's key covariance metric for smarter delta-rule removal. Clever combination, ~$2 to test.
- **Proposal 013 (Circulant SSM Fourier-Domain Scan)**: FFT-diagonalized circulant transitions enabling element-wise parallel scan with full coordinate mixing. Already implemented â€” run it.
- **Proposal 043 (Newton-Schulz Orthogonal DeltaNet)**: Replace sequential UT transform with tensor-core-friendly NS iteration. Implemented and ready. ~$1.
- **Proposal 006 (Monarch-Gated State Transition)**: Monarch-factored input-dependent transitions at O(nâˆšn). Implemented. The S5 composition task will be decisive. ~$1.
- **Proposal 016 (GS-Monomial SSM)**: Interleaved monomial blocks with shuffle permutation. Elegant O(nâˆšn) approach. Implemented. ~$1.
- **Proposals 023, 027, 028 (Circulant/Cayley/Neumann orthogonal SSMs)**: Three flavors of structured orthogonal transitions. All implemented. Run as a batch to compare.
- **Proposals 050, 054 (FP8/INT4 chunkwise kernels)**: Hardware optimization proposals requiring H100/Ada GPUs â€” higher cost (~$5â€“10), but high practical impact if validated.
- **Proposals 047, 049, 055 (multi-GPU sequence parallelism)**: Important for scale but require multi-GPU setups ($10+). Lower priority for budget-constrained validation.

### Strategic Insights

**The research is converging on three high-value axes**: (1) *algebraic reformulation of the scan semiring* (proposals 014, 015) â€” the most theoretically novel direction, potentially unifying SSMs and softmax attention; (2) *output bottleneck breaking* (proposals 009, 060, 059) â€” the lowest-hanging fruit with immediate quality gains across all linear models; and (3) *structured non-diagonal transitions* (proposals 006, 013, 016, 023, 027, 043) â€” a large batch of competing approaches to the diagonal-vs-dense tradeoff, which can be compared head-to-head on the same S5/group-composition benchmarks.

**Immediate priority**: Run experiments **009** and **014** first â€” they're the cheapest (<$1 each), most novel, and their results will inform whether to pursue the dependent proposals (060, 015). Then batch-run the structured transition experiments (006, 013, 016, 027, 028, 043) as a head-to-head comparison â€” they all use the same S5/group-composition tasks and each costs ~$1, so the full comparison is <$10.

**Emerging pattern from completed experiments**: The consistent finding across 005, 025, and 011 is that *the algebra works but Python-level sequential loops kill GPU efficiency*. This validates the kernel-fusion proposals (032, 039, 040, 057) as necessary follow-ups once the algorithmic winners are identified. Don't invest in custom kernels until you know which algorithms win on quality.

---
