---
title: Research Log Update
date: 2026-02-15T11:49:00
timestamp: 2026-02-15 11:49:00
tricks: 1
proposals: 8
experiments: 5
source: legacy_migration
---

## 2026-02-15 ‚Äî 11:49 UTC

### üéØ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention / SSM Readout** (Proposal 009) ‚Äî Priority: **HIGH**
- **Hypothesis**: Applying input-dependent sigmoid gating (from the NeurIPS 2025 Best Paper on softmax attention) to linear attention and SSM output readout breaks the low-rank bottleneck in $W_V W_O$, yielding 5‚Äì15% quality improvement at <2% latency cost.
- **Why it matters**: This is the single highest risk/reward proposal in the queue. It requires *zero architectural innovation* ‚Äî just adding a sigmoid gate after the readout ‚Äî and tests whether a known-good technique from softmax attention transfers to the sub-quadratic world. If it works, every linear attention and SSM model instantly gets a quality bump. The MVE is a clean A/B test on MQAR (gated vs. ungated cosFormer), trainable on CPU/single GPU in ~10 minutes.
- **Estimated cost**: <$1 (MVE), ~$0.50 on spot GPU
- **Impact score**: 9/10 ‚Äî Extremely cheap to test, universal applicability, clear success/fail criteria. Even a partial positive result is immediately actionable.

**2. Circulant SSM: Fourier-Domain Parallel Scan** (Proposal 013) ‚Äî Priority: **HIGH**
- **Hypothesis**: Block-circulant state transitions diagonalized via FFT enable element-wise parallel scans in frequency space, recovering diagonal SSM parallelism with full coordinate mixing at $O(n \log n)$ per step.
- **Why it matters**: This directly attacks the central tension in SSM design: diagonal = fast but no mixing, dense = expressive but $O(n^3)$ in the scan. Circulant structure is the sweet spot ‚Äî it's the simplest matrix class that mixes all coordinates and admits FFT diagonalization. The MVE tests on $\mathbb{Z}_8$ composition (the *best case* for circulant ‚Äî if it fails here, it fails everywhere), making this a clean falsification test.
- **Estimated cost**: <$0.50 (MVE ~5 min on single GPU)
- **Impact score**: 8.5/10 ‚Äî Cheap, clean, and if positive, opens an entire family of FFT-domain SSM architectures. The Fourier-domain scan idea could generalize beyond circulant to any diagonalizable structured transition.

**3. Monarch-Gated State Transition SSM** (Proposal 006) ‚Äî Priority: **HIGH**
- **Hypothesis**: Input-dependent Monarch-factored state transitions achieve dense-like expressivity at $O(n\sqrt{n})$ per-step cost, with BMM structure enabling high GPU utilization.
- **Why it matters**: Monarch matrices are hardware-aligned (batch matmul maps directly to GPU), expressive (they can represent DFT, Hadamard, and more), and sit at a natural complexity sweet spot. Experiment 005 was allocated for this but is currently an empty stub ‚Äî it's next in the pipeline. The S‚ÇÖ permutation composition benchmark will directly test non-abelian state tracking, which is the hardest capability gap between diagonal and dense SSMs.
- **Estimated cost**: <$0.50 (MVE), ~$16 small-scale
- **Impact score**: 8/10 ‚Äî Well-motivated, hardware-friendly, and the empty experiment directory means infrastructure is ready to go. However, Experiment 004's failure (Cauchy-like transitions couldn't optimize on S‚ÇÖ at length 20) is a cautionary signal: structured transitions can fail due to gradient pathology, not just expressivity limits.


### üß™ Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: ‚ùå COMPLETED ‚Äî FAILED)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Implemented tiny model (1 layer, n=16, ~129 params) on damped oscillation extrapolation task. Training MSE stuck at 0.854 across all 50 epochs ‚Äî zero learning.
  - **Key findings**: Learned œâ and Œ∂ distributions matched ground truth statistics, confirming the parameterization is correct. The failure is likely a forward-pass bug (complex dtype handling). **Decision: DEBUG before proceeding.** The oscillatory parameterization concept is not invalidated ‚Äî the implementation is.
  - **Cost**: $0.00 (CPU only, ~27 min)

- **Experiment 003: OscGate-SSM** (Status: ‚úÖ COMPLETED ‚Äî SUCCESS)
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Progress**: Three models tested on selective copying task across 3 debugging iterations. Final run: d=128, m=64, 10K samples, 100 epochs.
  - **Key findings**: **OscGate-SSM hit 93.0% accuracy** (target >90%) while LinOSS (LTI baseline) reached only 46.8%. This definitively proves that input-dependent oscillatory parameters enable content-dependent gating while preserving stability by construction. DiagonalSSM also hit 94.8%, suggesting the oscillatory structure isn't strictly *better* than diagonal for selectivity ‚Äî the key contribution is stability-for-free.
  - **Cost**: $0.00 (CPU only, ~25 min). 3/4 success criteria met.

- **Experiment 004: Displacement-Rank SSM** (Status: ‚ùå COMPLETED ‚Äî ABANDONED)
  - **Proposal**: 022-displacement-rank-ssm-state-transitions
  - **Progress**: Tested DR-SSM with Cauchy-like transitions at Œ± ‚àà {0,1,2,4,16} on S‚ÇÖ permutation composition.
  - **Key findings**: **Kill criterion triggered.** At easy length (12), Œ±=1 already matched Œ±=4 ‚Äî no rank-scaling signal. At hard length (20), **all Cauchy-structured models collapsed** (<4% accuracy) while dense achieved 97.2%. Root cause: the $1/(s_i - s_j)$ Cauchy kernel creates pathological gradient flow. **Displacement rank does not control expressivity in practice.**
  - **Cost**: $0.00 (CPU only, ~15 min). This is a valuable negative result.

- **Experiment 005: Monarch-Gated SSM** (Status: üî≤ STUB ‚Äî not yet implemented)
  - **Proposal**: 006-monarch-gated-state-transition
  - **Progress**: Empty directory structure created. No code written yet. This is the next experiment in the pipeline.

- **Experiment 001: CS-NEG-DeltaNet** (Status: üîß IMPLEMENTED ‚Äî not yet run)
  - **Proposal**: 001-column-sparse-negative-eigenvalue-deltanet
  - **Progress**: Full implementation exists (models, tasks, training infra, configs). Notes reveal that DeltaProduct likely makes this approach redundant ‚Äî Householder product decomposition already achieves the expressivity CS-DeltaNet was targeting, but through a cleaner mechanism.


### üìö New Discoveries

**144 tricks documented** in this window ‚Äî a massive cataloging effort spanning structured matrices, GPU kernel optimization, and algebraic techniques. Key themes:

- **Circulant/structured matrix universe fully mapped**: Block-circulant, g-circulant, skew-circulant, DCT-DST decomposition, optimal circulant approximation, circulant-diagonal products (CDFlow, CDVFT, C¬≥A). This gives the project a complete toolkit for any circulant-based SSM architecture.

- **Tropical Attention** (Hashemi et al., NeurIPS 2025): Replaces softmax with max-plus tropical geometry + Hilbert projective metric. Direct precursor to Proposal 015 (Tropical-Gated SSM).

- **FlashInfer JIT Fusion**: Composable, JIT-compiled attention kernel framework handling the combinatorial explosion of attention variants. Critical infrastructure for deploying custom attention kernels without hand-writing CUDA.

- **Expert Choice Routing** (Zhou et al., 2022): Inverts MoE routing so experts choose tokens instead of vice versa ‚Äî guarantees perfect load balance by construction. Foundation for Proposal 012.

- **Permutation learning toolkit**: ShuffleSoftSort ($N$-parameter), Birkhoff-Frank-Wolfe (with guarantees), OT4P (orthogonal group relaxation), STEAM (STE-based Monarch permutation learning). These are the building blocks for any proposal needing learned permutations.

- **HSS/hierarchical matrix deep dive**: 15+ tricks on HSS construction, factorization, compression, eigensolvers, and preconditioners. While most are numerical linear algebra infrastructure, the telescopic decomposition and black-box randomized compression are directly relevant to Proposal 021 (HSS-compressed attention).


### Other Proposals

| Proposal | Summary | Est. MVE Cost | Quick Take |
|----------|---------|---------------|------------|
| **015 Tropical-Gated SSM** | Max-plus semiring recurrence with hard-winner dynamics | ~$0.50 | Fascinating but risky ‚Äî tropical semiring loses the smooth gradient flow that makes standard SSMs trainable. The smooth‚Üíhard annealing schedule is the make-or-break detail. |
| **016 GS-Monomial SSM** | Group-and-Shuffle monomial state transitions | ~$0.50 | Elegant algebraic construction but complex implementation. Test *after* simpler circulant/Monarch approaches. |
| **014 Log-Semiring SSM** | LogSumExp recurrence = softmax-native scan | ~$0.50 | Theoretically beautiful (SSM that *is* softmax attention), but online-softmax stabilization in the scan operator adds non-trivial complexity. |
| **027 Cayley-Circulant Orthogonal SSM** | Cayley transform of skew-circulant-diagonal = orthogonal + FFT-fast | ~$0.50 | Overlaps with 013 but adds orthogonality guarantee. Test 013 first; if positive, try this variant. |
| **028 Neumann-Cayley SSM** | Approximate Cayley inverse via Neumann series for input-dependent orthogonal transitions | ~$0.50 | Clever approximation trick. Depends on whether k=4 Neumann terms give sufficient orthogonality. |
| **026 Cyclic Reduction SSM** | Cyclic reduction for non-diagonal recurrences | ~$0.50 | Important infrastructure if Monarch/circulant SSMs prove viable ‚Äî it's the parallelization strategy for dense scans. |
| **029 Circulant FAVOR+** | Circulant projections in FAVOR+ random features | ~$0.50 | Low-risk incremental improvement to linear attention feature maps. |
| **003 DPLR Column-Sparse** | Column-sparse permutation + DPLR core | ~$0.50 | Medium priority ‚Äî bridges S4 convolution with PD-SSM routing, but Exp 004's Cauchy failure raises concerns about structured-matrix gradient flow. |
| **005 Segmented-HSS Attention** | HSS + segmented scan for variable-length linear attention | ~$1.00 | Interesting for production systems (variable-length batching) but complex to implement correctly. |
| **018 Hutchinson Adaptive Rank** | Trace estimation for dynamic rank allocation in DPLR SSMs | ~$1.00 | Useful optimization but not a capability breakthrough. |
| **020 OH-DeltaProduct** | Oscillatory + Householder hybrid | ~$0.50 | Exp 003 validated oscillatory gating; this proposes combining it with DeltaProduct. Promising but needs Exp 002's bug fixed first. |


### Strategic Insights

**The experimental scorecard is clarifying the landscape fast.** Three experiments completed for $0.00 in compute ‚Äî the CPU-only MVE strategy is working beautifully. The key lessons: (1) Experiment 003 proves input-dependent oscillatory gating works and stability-by-construction holds in practice. (2) Experiment 004 proves that theoretical elegance (displacement rank) can completely fail in practice due to gradient pathology ‚Äî **always test the optimization landscape, not just the expressivity theory**. (3) Experiment 002's debugging failure is a reminder to validate the forward pass before running full training.

**Recommended priority order for next experiments:**
1. **Proposal 009 (Post-Sigmoid Gating)** ‚Äî fastest to test, broadest applicability, zero architectural risk
2. **Proposal 013 (Circulant SSM)** ‚Äî clean falsification test, opens a whole architecture family
3. **Proposal 006 (Monarch-Gated SSM)** ‚Äî already has infra allocated (Exp 005), direct comparison to Exp 004's failed Cauchy approach
4. **Fix Experiment 002** ‚Äî the oscillatory-DPLR concept isn't dead, just bugged

**The emerging meta-theme:** The winning SSM state transition will be one that is (a) diagonalizable or decomposable in a way that preserves gradient flow, (b) expressive enough for non-abelian state tracking, and (c) maps to existing GPU primitives (BMM, FFT, SpTC). Circulant and Monarch matrices both satisfy all three. Cauchy-like matrices satisfy (a) and (c) but fail (b) in practice. The next 48 hours of experiments should resolve whether circulant or Monarch is the better path.




I need to review the recent activity comprehensively. Let me start by reading the key proposals and experiment results in detail.

Let me also read the remaining high-priority proposals and check experiment status for the ones marked as "implemented" but not yet completed.

I'll wait for the agents to complete. Let me also check the experiment code directories for the "implemented" experiments to understand their status.

Let me wait for all agents to complete before synthesizing the log entry.Let me read the key remaining proposals directly instead of waiting for the agent.Now let me read the remaining experiment results:Now I have enough data. Let me also quickly check the experiment 004 results file:Now I have comprehensive data. Let me write the log entry.Now let me write the comprehensive log entry: