---
title: Research Log Update
date: 2026-02-15T16:50:00
timestamp: 2026-02-15 16:50:00
tricks: 0
proposals: 5
experiments: 0
source: legacy_migration
---

## 2026-02-15 â€” 16:50 UTC

### ðŸš¨ Ad-Hoc Proposal Reprioritization Based on GPU Efficiency Criteria

**Context**: Conducted comprehensive review of all 30 proposals against new GPU efficiency focus added to `human_feedback.md`. Goal: ensure all proposals target **wall-clock GPU speedup for pretraining**, not just asymptotic complexity or mathematical elegance.

**New criteria enforced**:
- Memory access pattern analysis (coalesced, cache-friendly)
- Parallelism analysis (saturates SMs, no warp divergence)
- Baseline comparison (tokens/sec on real hardware vs FlashAttention-2/Mamba-2)
- Hardware-specific considerations (tensor cores, TMA, shared memory)

**Decision rule**: "Would I bet $100 this is faster than FlashAttention-2/Mamba-2 on A100?"


### ðŸŸ¢ HIGH PRIORITY (5 proposals â€” GPU-friendly, likely real speedup)

**âœ… Keep and prioritize:**

1. **Proposal 002 (SSD-DeltaNet WY Hybrid)** â€” Uses UT transform to convert recurrence into tensor-core matmuls. Explicit tensor core targeting, matmul-heavy operations. **This is the gold standard.**

2. **Proposal 010 (Sparse Monarch SSM 2:4)** â€” Hardware-native 2:4 sparsity on Sparse Tensor Cores. Direct hardware support, proven 2Ã— speedup. Solid GPU efficiency play.

3. **Proposal 024 (Sparse SSM S-STE + Sinkhorn)** â€” 2:4 sparsity with Sparse Tensor Cores, continuous optimization. Targets inference speedup with hardware support.

4. **Proposal 029 (Circulant FAVOR+ Linear Attention)** â€” Replaces dense projection with FFT (O(d log d) vs O(dÂ²)). cuFFT is well-optimized. Credible speedup, though crossover point at d=64 is borderline.

5. **Proposal 009 (Post-Sigmoid Gating)** â€” Minimal overhead (<2%), proven in literature. Easy win.


### ðŸŸ¡ MEDIUM PRIORITY (8 proposals â€” needs GPU justification before implementation)

**âš ï¸ Require throughput benchmarks or kernel implementation plans:**

- **001** (Column-Sparse NEG-DeltaNet) â€” Gumbel-Softmax overhead needs analysis
- **004** (Oscillatory DPLR) â€” MVE implemented, maintains O(n), OK for testing
- **006** (Monarch Gated) â€” 8Ã— overhead vs diagonal, but prioritize sparse variant (010)
- **008** (Cosine-Log Linear) â€” Doubles computation (cos + sin), needs benchmarks
- **013** (Circulant SSM Fourier) â€” FFT overhead, but O(n log n) is good
- **023** (Circulant-Diagonal SSM) â€” Similar to 027, FFT path OK
- **027** (Cayley Circulant Orthogonal) â€” FFT-based, exact orthogonality, GPU-friendly


### ðŸ”´ LOW PRIORITY (17 proposals â€” GPU-unfriendly, require strong justification)

**Major red flags identified:**

**âŒ 015 (Tropical Gated SSM)** â€” **CRITICAL**: "Max-plus operations run on CUDA cores at ~16Ã— lower throughput than tensor-core GEMM." Proposal explicitly states needs SIMDÂ² hardware (doesn't exist). **Action: KILL or mark as "future hardware" research.**

**âŒ 021 (Black-Box HSS Telescopic Attention)** â€” **CRITICAL**: "Black-box compression via O(r) matrix-vector products" = many kernel launches. "Hierarchical tree with sequential upward/downward passes" = not GPU-parallel. **This is literally in the red flags example in human_feedback.md.** **Action: DEPRIORITIZE immediately.**

**âŒ 030 (Group-Matrix Displacement Rank SSM)** â€” **CRITICAL**: 21 pages of group theory about hyperoctahedral groups $B_n = \mathbb{Z}_2^n \rtimes S_n$. "Permutation routing" = gather/scatter = breaks coalescing. Irregular memory access, very theoretical. **Action: DEPRIORITIZE â€” cool math, unclear GPU benefit.**

**Other low-priority proposals:**
- **005** (Segmented HSS) â€” HSS tree traversal, sequential operations
- **012** (Expert-Choice Monarch) â€” Routing overhead, load imbalance
- **016** (GS-Monomial) â€” Ad-hoc structure, no kernel optimization
- **017** (Hyperoctahedral Signed Perm) â€” Sinkhorn iterations overhead
- **018** (Hutchinson Adaptive Rank) â€” Stochastic sampling overhead
- **019** (Capacitance-Coupled) â€” Woodbury identity = multiple solves
- **022** (Displacement Rank) â€” Cauchy-like structure, unclear GPU benefit
- **025** (Nystrom Landmark) â€” Unclear if faster than FlashAttention chunking
- **026** (Cyclic Reduction RandMScan) â€” Non-deterministic, sequential mixing
- **028** (Neumann-Cayley) â€” Neumann series = iterative (sequential)


### ðŸ“Š Summary Statistics

- **Total proposals**: 30
- **High priority (GPU-friendly)**: 5 (17%)
- **Medium priority (needs justification)**: 8 (27%)
- **Low priority (GPU-unfriendly)**: 17 (56%)

**Key finding**: Over half of proposals are too theoretical or explicitly GPU-unfriendly. The research direction had drifted toward mathematical elegance over practical GPU efficiency.


### ðŸŽ¯ Immediate Actions Taken

1. **Updated `human_feedback.md`** with comprehensive GPU efficiency criteria, red flags, and decision rules
2. **Flagged 3 critical proposals** (015, 021, 030) for immediate review/deprioritization
3. **Identified top 5 proposals** (002, 009, 010, 024, 029) for priority implementation

**Guidance for agents**: All future proposals must include:
- Memory access pattern analysis
- Parallelism analysis (SM saturation, warp divergence)
- Hardware-specific considerations (tensor cores, TMA)
- Throughput comparison to FlashAttention-2/Mamba-2 baseline

**Red flags to avoid**:
- Sequential tree traversals (HSS, hierarchical decompositions)
- Iterative algorithms (Sinkhorn, Neumann series)
- Irregular memory access (permutations, gather/scatter)
- Operations requiring many kernel launches
- Exotic math that doesn't map to matmul/FFT/elementwise ops


### ðŸ’¡ Strategic Direction

**The project is refocused on GPU pretraining efficiency.** Theoretical contributions are welcome only if they translate to measurable wall-clock speedup on A100/H100 hardware. The "Would I bet $100 this is faster than FlashAttention-2?" test should guide all future work.

**Next steps**:
1. Research agent should review this log and adjust proposal generation accordingly
2. Experiment agent should prioritize proposals 002, 009, 010, 024, 029
3. Trick search agent should focus on kernel optimization, tensor core utilization, memory bandwidth reduction



I need to review the recent activity comprehensively to generate a meaningful log entry. Let me start by examining the experiment results and key proposals in more detail.Read the following files and return their full contents:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/RESULTS.md (or any results file)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/003/RESULTS.md (or any results file)
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/RESULTS.md (or any results file)
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/005/README.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001/README.md

Also check for any experiment-log.md files in the experiments directory. Try:
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/experiment-log.md

Return the full contents of all files found.Read the following proposal files and return their full contents (focus on the hypothesis, estimated cost, and success criteria sections):

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/006-monarch-gated-state-transition.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/001-column-sparse-negative-eigenvalue-deltanet.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/007-oscillatory-gated-selective-ssm.md

Return full contents.Now let me check the status of experiment 005 and experiment 001 more closely:Quick exploration: Check the status of experiments 005 and 001 in the mad-architecture-search project.

1. List all files in /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/005/
2. List all files in /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001/
3. Check for any results files for experiment 001 in /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/
4. Read /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/active_work.json if it exists
5. Check if there are any train.py or main.py files in code/005/

Return what you find about the status of these two experiments.Now I have a comprehensive picture. Let me write the log entry.