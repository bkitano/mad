---
title: Research Log Update
date: 2026-02-15T04:54:00
timestamp: 2026-02-15 04:54:00
tricks: 1
proposals: 4
experiments: 3
source: legacy_migration
---

## 2026-02-15 â€” 04:54 UTC

### ðŸŽ¯ High-Impact Proposals

- **Post-Sigmoid Gating for Linear Attention (Proposal 009)** (Priority: **high**)
  - **Hypothesis**: Applying post-readout sigmoid gating (NeurIPS 2025 Best Paper technique) to linear attention / SSM readout will break the low-rank bottleneck in `q^T Â· S` output, yielding 5â€“15% perplexity improvement at <2% latency overhead.
  - **Why it matters**: This is the single most promising proposal right now. It's **orthogonal** to every other proposal (can be composed freely), targets a well-understood bottleneck (the purely linear readout path in linear attention has zero nonlinearity before the output projection), and the MVE is dead simple: 2-layer cosFormer Â±gate on MQAR, ~80K params, <10 min on CPU/laptop GPU. The theoretical reasoning is sharp â€” sigmoid gating has already been validated on softmax attention at scale, and the benefit should be *larger* for linear attention precisely because the bottleneck is worse.
  - **Estimated cost**: **<$1** (CPU-feasible, ~10 min)
  - **Impact score**: **9/10** â€” Cheapest experiment with strongest theoretical foundation. High chance of a clear positive signal. If it works, it's a universal plug-in improvement for every linear attention and SSM variant.

- **Monarch-Gated State Transition SSM (Proposal 006)** (Priority: **high**)
  - **Hypothesis**: Input-dependent Monarch-factored state transitions achieve near-dense expressivity at O(nâˆšn) cost, with native BMM (batch matrix multiply) structure giving 2â€“4Ã— higher GPU utilization than custom scan kernels.
  - **Why it matters**: This is the most architecturally novel proposal that hasn't been tested. Unlike the oscillatory line (which has now failed twice), Monarch factorization is well-proven for hardware efficiency (Dao et al., ICML 2022), and composing it with input-dependent gating addresses the real expressivity gap in diagonal SSMs. The MVE targets S5 permutation group composition â€” a concrete, discriminative benchmark. ~120K params, trainable on a laptop.
  - **Estimated cost**: **<$5** (single GPU, ~30 min)
  - **Impact score**: **7/10** â€” More complex than 009 but addresses the core architectural question (state transition expressivity) that diagonal SSMs leave on the table. Risk is moderate: the Monarch Ã— gating composition is untested.

### ðŸ§ª Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: âŒ completed â€” FAILED)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Full implementation, 50 epochs training, ~27 min on CPU. Cost: ~$0.00.
  - **Key findings**: Model completely failed to fit training data. Train MSE stagnated at 0.854 (target was <1e-3 â€” **three orders of magnitude off**). Interestingly, learned Ï‰ and Î¶ parameters *did* cluster in the ground-truth range, suggesting the parameterization captures the right structure but the optimization landscape is broken. Likely culprit: gradient flow through the bilinear discretization or the DPLR resolvent is pathological at this scale.
  - **Cost**: $0.00 actual vs ~$0.40 estimated

- **Experiment 003: Oscillatory-Gated Selective SSM** (Status: âŒ completed â€” FAILED)
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Progress**: Three-model comparison (OscGate, LinOSS, DiagonalSSM) on selective copying task. All fully implemented.
  - **Key findings**: **All three models performed at chance level** (~7% accuracy vs 6.25% random baseline, target was >90%). This isn't an OscGate-specific failure â€” even the diagonal SSM baseline couldn't solve selective copying, suggesting either the task setup, training hyperparameters, or model scale is wrong. Stability passed (0 NaN events) and speed overhead was acceptable (2.12Ã—), so the *mechanism* works â€” it just doesn't learn.
  - **Cost**: ~$0.00 actual vs ~$0.40 estimated

- **Experiment 001: CS-DeltaNet Analysis** (Status: implemented â€” superseded)
  - **Proposal**: 001-column-sparse-negative-eigenvalue-deltanet
  - **Progress**: Theoretical analysis concluded that DeltaProduct (Schlag et al., ICLR 2025) makes column-sparse DeltaNet redundant â€” DeltaNet's accumulated Householder reflections already implicitly build permutations. For D4, just n_h=2 Householder steps suffice.
  - **Key findings**: This is actually a valuable negative result: it narrows the search space by showing that DeltaNet's existing architecture already subsumes the column-sparse approach.

### ðŸ“š New Discoveries (60 tricks documented)

The 60 new tricks span six categories. Key highlights:

- **Post-Attention Sigmoid Gating**: Input-dependent sigmoid gate *before* the output projection breaks the low-rank bottleneck in multi-head attention. This is the NeurIPS 2025 Best Paper trick â€” and it directly inspired Proposal 009.

- **Permutation-Augmented Structured Sparsity (PA-DST)**: Learned permutations restore expressivity lost from N:M structured sparsity, at near-zero overhead. Critical enabler for Proposal 010 (Sparse Monarch SSM).

- **Cosine-Reweighted Linear Attention (cosFormer)**: Decomposes softmax's two key properties (non-negativity + concentration) and replaces them with ReLU + cosine reweighting. Achieves competitive quality at O(TdÂ²) â€” a cleaner linear attention baseline than random features.

- **Log-Linear Attention**: O(log T) hidden states via Fenwick tree partitioning, bridging the gap between fixed-state linear attention and quadratic softmax. Theoretically elegant but complex to implement.

- **Oscillatory Eigenvalue Stability (LinOSS)**: Stability by physics â€” eigenvalues guaranteed on/within the unit circle from the harmonic oscillator ODE structure. Sound theory, but experiments 002 and 003 show the optimization landscape is problematic.

- **Gumbel-Softmax Reparameterization**: Differentiable sampling from categorical distributions â€” essential infrastructure for any proposal involving learned discrete permutations (PA-DST, OT4P).

- **Expert Choice Routing**: Inverted MoE routing where experts choose tokens instead of vice versa. Perfect load balance by construction, >2Ã— faster than token-choice. Relevant if MoE is explored.

- **Monarch Matrix Factorization + Group-and-Shuffle Matrices**: Hardware-efficient structured matrices (BMM-native) that generalize butterfly transforms. Core building block for Proposal 006.

### Other Proposals

- **Sparse Monarch SSM (Proposal 010)**: 2:4 sparsity on Monarch blocks + PA-DST permutations. Promising but depends on Proposal 006 succeeding first. Estimated cost: <$5.
- **SSD-DeltaNet (Proposal 002)**: Engineering-focused speedup (2â€“4Ã—) via tensor cores. High value if DeltaNet is the chosen architecture, but requires custom Triton kernels â€” implementation-heavy.
- **DPLR Column-Sparse SSM (Proposal 003)**: Bridges S4-style convolution with PD-SSM routing. Interesting theory but complex, and Experiment 001 showed the column-sparse direction may be redundant.
- **Cosine-Reweighted Log-Linear Attention (Proposal 008)**: cosFormer + log-linear multi-resolution states. Appealing composition but log-linear attention's Fenwick tree is implementation-heavy.
- **Oscillatory-DPLR SSM (Proposal 004)**: Tested in Experiment 002, failed. Needs debugging before re-evaluation.
- **Segmented-HSS Linear Attention (Proposal 005)**: Variable-length hierarchical attention. Theoretically clean but complex; lower priority until simpler proposals are validated.
- **Column-Sparse Negative-Eigenvalue DeltaNet (Proposal 001)**: Superseded by DeltaProduct analysis.

### Strategic Insights

**The oscillatory line is stalled.** Two experiments (002, 003) both returned DEBUG verdicts with all models at chance level. The stability guarantees are theoretically sound, but something is wrong with optimization â€” possibly gradient flow through bilinear discretization, learning rate sensitivity, or insufficient model scale. Before investing more compute here, **the researcher should debug the training loop** (check gradients, try a trivial fitting task, increase model size slightly).

**Proposal 009 (post-sigmoid gating) is the clear next experiment.** It's the cheapest (<$1), fastest (<10 min), most theoretically grounded, and completely orthogonal to everything else. A positive result here would be immediately composable with any future architecture. **Do this one first.**

**After 009, run Proposal 006 (Monarch-gated SSM).** This tests the core architectural hypothesis (structured-but-expressive state transitions) on a concrete group-theory benchmark. If it works, Proposal 010 (sparse Monarch) becomes the natural follow-up for hardware efficiency. If it fails, the diagonal-state-transition paradigm (Mamba-style) is further validated.

**Emerging theme**: The most impactful tricks are *readout/output-path improvements* (sigmoid gating, GLU) rather than state-transition innovations. The state transition is already "good enough" with diagonal + input-dependent gating; the bottleneck may be in how information is extracted from the state, not how the state is updated. This reframes the research: **fix the readout first, then optimize the transition.**




I'll analyze the recent activity systematically by first examining the key proposals and experiment results in more detail.Read the following files and return their full contents. I need to understand the experiment results and top proposals:

1. First, find and read the experiment 002 results file - look in /home/bkitano/Desktop/vault/projects/mad-architecture-search/ for results related to experiment 002
2. Read the experiment 003 README and any results: /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/003/README.md
3. Read the experiment 001 README: /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001/README.md
4. Read these proposal files:
   - /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
   - /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
   - /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
   - /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/007-oscillatory-gated-selective-ssm.md
   - /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/006-monarch-gated-state-transition.md

Return ALL content from each file - do not summarize.Now let me get a quick look at cost estimates and any additional details from the remaining proposals.Look for any results files for experiment 003 in /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/003/ - check for results.yaml, results.md, any log files, or output files. Also check /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/ for any 003 results files. Report what you find.Now I have all the information I need. Let me generate the comprehensive log entry.