---
title: Research Log Update
date: 2026-02-15T17:26:18.084838
timestamp: 2026-02-15 17:26:18
tricks: 179
proposals: 32
experiments: 27
---

I need to analyze a massive amount of activity ‚Äî 179 tricks, 32 proposals, and 27 experiments. Let me explore the key details efficiently.Read the following experiment result files and summarize key findings for each completed experiment. Focus on: what worked, what failed, cost, and key insights.

Read these files:
1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/results.md (or RESULTS.md)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/022/results.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/029/results.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/026/results.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/results.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/005/results.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/007/results.md
8. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/011/results.md
9. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/025/results.md

For each, extract: status (pass/fail), key metrics, cost, and the most important takeaway. Just do research, don't write code.Now let me look at a few key proposals in more detail to assess their feasibility and costs:Read the following proposal files and extract for each: (1) the estimated compute cost, (2) model size in params, (3) what hardware is needed, (4) what task/benchmark is proposed, and (5) any specific implementation complexity notes. Just do research, don't write code.

Read these files at thoroughness "medium":
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/016-gs-monomial-ssm-state-transition.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/032-chimera-fused-chunkwise-ssm-scan.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/040-persistent-megakernel-linear-rnn-layer.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/027-cayley-circulant-orthogonal-ssm.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/023-circulant-diagonal-ssm-state-transitions.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/020-oscillatory-householder-deltaproduct.md
- /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.mdNow I have comprehensive data. Let me compose the log entry.

## 2026-02-15 ‚Äî 17:22 UTC

### üéØ High-Impact Proposals

**1. Cyclic Reduction for Dense SSM Recurrences** (Proposal 026) ‚Äî Priority: **HIGH** ‚úÖ VALIDATED
- **Hypothesis**: Cyclic reduction achieves O(Tn¬≥) work vs. prefix scan's O(Tn¬≥ log T) for non-diagonal SSM state recurrences
- **Why it matters**: Already experimentally validated ‚Äî **3.88√ó CPU speedup at T=1024** with machine-precision accuracy. This is the most de-risked proposal in the batch. It directly unblocks all non-diagonal SSM architectures (DeltaNet, DeltaProduct, Monarch SSMs) that currently pay an unnecessary log T penalty on every training step. A fused CUDA kernel could push this to 5-10√ó.
- **Estimated cost**: <$1 (already validated on CPU; GPU kernel benchmark ~$2-5)
- **Impact score**: 9/10 ‚Äî Near-zero risk, already proven, scales with sequence length, and enables an entire class of more expressive SSMs

**2. Log-Semiring SSM Scan** (Proposal 014) ‚Äî Priority: **HIGH**
- **Hypothesis**: Replacing (+, √ó) with (logsumexp, +) in SSM scans enables native softmax-weighted attention over history without kernel approximation
- **Why it matters**: This is the most theoretically elegant bridge between softmax attention and SSMs ‚Äî it computes exact log-partition functions via parallel scan. Unlike FAVOR+ (which Experiment 029 just proved fails catastrophically on associative recall at 23% vs ReLU's 98.5%), the log-semiring computes the *exact* softmax kernel. MVE already implemented and running.
- **Estimated cost**: <$1 MVE, ~$16 small-scale validation
- **Impact score**: 8/10 ‚Äî Addresses a fundamental limitation, but logsumexp runs on CUDA cores (~16√ó slower than tensor cores), capping practical throughput gains

---

### üß™ Experiment Updates

**Completed Experiments (5 total, ~$0.40 total spend):**

- **Experiment 026: Cyclic Reduction** (Status: ‚úÖ PASS ‚Üí PROCEED)
  - **Progress**: Pure kernel benchmark, CPU-only
  - **Key findings**: 3.88√ó speedup at T=1024, 6.01√ó GEMM reduction, machine-precision accuracy. Speedup scales monotonically with T. **Clear win ‚Äî ready for GPU kernel implementation.**
  - **Cost**: $0.00 actual vs <$1 estimated

- **Experiment 007: OscGate-SSM** (Status: ‚úÖ PASS ‚Üí PROCEED)
  - **Progress**: Full MVE on selective copying task
  - **Key findings**: 93% accuracy (vs. 46.8% for LTI baseline). Input-dependent oscillatory params transform non-selective ‚Üí selective with stability guarantee. Only 1.8√ó overhead vs diagonal SSM.
  - **Cost**: $0.00 actual vs ~$0.40 estimated

- **Experiment 025: Nystr√∂m Landmark Chunkwise SSM** (Status: ‚úÖ PASS ‚Üí PROCEED)
  - **Progress**: Delayed copy task with chunk boundaries
  - **Key findings**: 99.25% accuracy at 3.2√ó memory compression. Surprising result: **model co-adapts with compression even when low-rank assumption is violated** (approximation error is 0.86, yet accuracy is preserved). Stronger result than predicted.
  - **Cost**: $0.05 actual vs ~$0.10 estimated

- **Experiment 029: Circulant FAVOR+** (Status: ‚ùå ABANDON)
  - **Progress**: Full MVE, 4 model comparison
  - **Key findings**: The circulant projection *does* match dense FAVOR+ quality (within 0.7%), validating the math. But **FAVOR+ itself fails catastrophically** ‚Äî 23% test accuracy vs. ReLU linear attention at 98.5%. Optimizing a broken foundation is pointless.
  - **Cost**: $0.10 actual

- **Experiment 022: Displacement-Rank SSM** (Status: ‚ùå ABANDON)
  - **Progress**: Full MVE on S5 permutation composition
  - **Key findings**: Increasing displacement rank Œ± provides **zero benefit**. Dense SSM gets 97.2% on hard S5; all Cauchy-structured variants get <4%. The 1/(s_i ‚àí s_j) kernel creates ill-conditioned gradients. **Theoretical expressivity ‚â† learnability.**
  - **Cost**: $0.00 actual

**Implemented but not yet completed (12 experiments):**
Notable: Experiments 014 (Log-Semiring), 015 (Tropical SSM), 016 (GS-Monomial), 017 (Hyperoctahedral SSM), 013 (Circulant SSM), 027 (Cayley-Circulant), 028 (Neumann-Cayley), 030 (Group-Matrix DR-SSM) are all implemented and ready to run or running.

---

### üìö New Discoveries

179 tricks documented ‚Äî an extraordinary cataloging effort. Key thematic clusters:

- **Semiring alternatives** (Tricks 108, 113, 132): The tropical semiring, log-semiring, and SIMD¬≤ hardware for non-standard semirings open a design axis beyond standard matrix multiply. SIMD¬≤ shows 8 additional semiring operations can be hardware-accelerated with only 5% chip area overhead.

- **Structured orthogonal parameterizations** (Tricks 020, 022, 062, 078, 111, 145, 157, 159): A deep stack connecting Cartan-Dieudonn√© ‚Üí Householder products ‚Üí WY/CWY representations ‚Üí Neumann approximation of CWY inverse ‚Üí recursive WY merge. This is the full toolkit for efficient orthogonal state transitions.

- **N:M sparsity ecosystem** (Tricks 116, 130, 133, 136, 140): From basic 2:4 to V:N:M hierarchical sparsity, transposable masks via optimal transport (TSENOR), and S-STE continuous training. A complete pipeline for sparse SSM training now exists in the trick database.

- **GPU kernel fusion patterns** (Tricks 025, 039, 046, 051, 061, 075, 091, 135): From epilogue fusion ‚Üí horizontal fusion ‚Üí persistent megakernels ‚Üí DSM inter-core fusion ‚Üí Twill optimal joint scheduling. These represent the entire evolution of GPU kernel optimization.

- **Newton-Schulz / Neumann series** (Tricks 081, 157, 164, 170, 171): Multiple applications of polynomial matrix approximation ‚Äî from CWY inverse (157) to polar orthogonalization (164, Muon optimizer) to Chebyshev-accelerated variants (170). A recurring pattern: replace O(n¬≥) exact operations with O(kn¬≤) iterative ones.

---

### Other Proposals

**Architecture-level proposals (most need >$10 for proper validation):**
- **Tropical-Gated SSM** (015): Hard winner-take-all dynamics; ~$250 GPU-hours full-scale. MVE implemented. Sparse gradient training is a significant risk.
- **GS-Monomial SSM** (016): Structured block-diagonal + shuffle state transitions; ~$260 GPU-hours full-scale. MVE implemented. Risk: monomial blocks may be too restrictive.
- **OH-DeltaProduct** (020): Oscillatory + reflective decomposition; ~$150 GPU-hours full-scale. MVE implemented. Risk: component interference.
- **Hyperoctahedral SSM** (017): Signed permutation state transitions; MVE implemented. Elegant but hardware efficiency unclear.
- **Capacitance-Coupled Multi-Scale SSM** (019): Cross-scale coupling via capacitance matrix; MVE implemented. Clean idea but may be redundant with multi-head SSMs.

**Kernel optimization proposals (pure engineering, need H100):**
- **Chimera-Fused Chunkwise SSM** (032): Analytical GEMM-chain fusion for intra-chunk computation. ~$100 full-scale. High potential but requires Triton expertise.
- **Persistent Megakernel Linear RNN** (040): Fuse entire layer into one kernel. ~$120 GPU-hours. Very high ceiling but extremely hard to implement.
- **Warp-Specialized Chunkwise RNN** (039): FA3-style producer-consumer pipelining. H100-only. 1.5-2√ó potential.
- **CTA-Swizzled TFLA** (038): L2 cache optimization for linear RNNs. 1.1-1.25√ó potential.
- **Stream-K BRGEMM Chunkwise** (034): Work-centric decomposition for state accumulation. Solid but incremental.

**Proposals informed by experiment failures:**
- **Circulant-Diagonal SSM** (023): Uses CDFlow-style parameterization. Risk: Experiment 022 showed Cauchy-structured approaches create optimization barriers. Circulant avoids Cauchy but shares the "structured ‚â† learnable" concern.
- **Displacement-Rank SSM** (022): Already experimentally disproven. ‚ùå
- **Circulant FAVOR+** (029): Already experimentally disproven (FAVOR+ itself fails). ‚ùå

---

### Strategic Insights

**The most important lesson from today's experiments is that theoretical expressivity does not predict practical learnability.** Both Experiment 022 (Cauchy displacement rank) and Experiment 029 (FAVOR+) showed architectures with provable approximation properties failing catastrophically, while simpler alternatives (dense SSM, ReLU features) dominated. This should recalibrate how we assess the remaining proposals ‚Äî **proposals grounded in simple, well-conditioned operations should be weighted higher** than mathematically elegant but numerically fragile constructions.

**The $0.40 total spend across 9 completed experiments validates the ultra-cheap MVE approach.** Four experiments passed and will proceed, four were correctly killed before wasting budget, and one flagged an implementation bug. The researcher should continue this rapid-fire MVE cadence with the 12 already-implemented experiments, especially Experiments 014 (Log-Semiring) and 013 (Circulant SSM), which test the two most promising semiring alternatives.

**The #1 next action should be running the 12 implemented-but-not-completed experiments**, particularly focusing on the semiring variants (014, 015) and the structured orthogonal approaches (016, 017, 020, 027, 028, 030). The cyclic reduction result (Exp 026) is the clearest win so far and should be fast-tracked to a GPU kernel benchmark (Exp 026_scaled is already staged). If even 2-3 of the 12 pending experiments show positive signals comparable to Exp 007 (OscGate) or Exp 025 (Nystr√∂m), the researcher will have a compelling portfolio of validated architectural innovations.

---
