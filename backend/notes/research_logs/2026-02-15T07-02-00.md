---
title: Research Log Update
date: 2026-02-15T07:02:00
timestamp: 2026-02-15 07:02:00
tricks: 1
proposals: 3
experiments: 4
source: legacy_migration
---

## 2026-02-15 â€” 07:02 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention / SSM Readout** (Proposal 009) â€” Priority: **HIGH**
- **Hypothesis**: Applying a head-specific sigmoid gate to the output of linear attention/SSM readout breaks the low-rank bottleneck in output projections, yielding 5â€“15% perplexity improvement with <2% latency overhead.
- **Why it matters**: This is the single most cost-effective proposal in the batch. It's **orthogonal to every other proposal** (it modifies the readout path, not the state transition), meaning it can be stacked with any other architecture change. The technique comes from a NeurIPS 2025 Best Paper applied to softmax attention â€” adapting it to linear attention / SSMs is a clean, low-risk transfer. The authors of the original paper showed it addresses attention-sink artifacts; linear attention suffers from a *more severe* information bottleneck, so the gains should be proportionally larger.
- **Estimated cost**: **<$1** â€” MVE is a tiny cosFormer variant (~80K params) on MQAR, trainable on CPU or a single consumer GPU in minutes.
- **Impact score**: **9/10** â€” Near-zero implementation risk, composable with all other proposals, and cheap to validate. Even a small positive signal justifies always including this gate.

**2. Circulant SSM: Fourier-Domain Parallel Scan** (Proposal 013) â€” Priority: **HIGH**
- **Hypothesis**: Block-circulant state transitions become diagonal in Fourier space via FFT, enabling element-wise parallel scans at O(n log n) per step with full coordinate mixing â€” the efficiency of diagonal SSMs with the expressivity closer to dense transitions.
- **Why it matters**: This is the most elegant resolution of the core SSM tension (diagonal = fast but no mixing, dense = mixing but slow). The FFT diagonalization is mathematically exact, not an approximation. It naturally handles the parallel scan via standard Blelloch in frequency space. The commutativity limitation (circulant algebra is abelian) is a known constraint but doesn't hurt on many practical tasks, and block-circulant variants can break commutativity.
- **Estimated cost**: **<$3** â€” MVE is cyclic group composition on tiny models, all CPU-feasible. Full validation ~24 GPU-hours (~$5â€“8 on spot instances).
- **Impact score**: **8/10** â€” Clean theoretical story, cheap to validate, and if it works, it's a new point on the efficiency-expressivity Pareto frontier. The commutativity limitation is the main risk.


### ðŸ§ª Experiment Updates

- **Experiment 003: OscGate-SSM** (Status: âœ… **COMPLETED â€” PROCEED**)
  - **Proposal**: 007 â€” Oscillatory-Gated Selective SSM
  - **Progress**: Full MVE implemented and validated on selective copying task. Three models tested: OscGate-SSM (input-dependent Ï‰, Î¶), LinOSS (fixed/LTI), DiagonalSSM (standard Mamba-style).
  - **Key findings**: OscGate-SSM achieved **93.0% accuracy** vs. LinOSS at **46.8%** â€” a massive **46.2 pp gap** proving that input-dependent oscillatory parameters enable content-based selectivity while maintaining stability-by-construction (zero NaN/Inf). Speed overhead was only 1.80Ã— vs diagonal SSM (well within the 4Ã— theoretical worst case). Near-parity with unconstrained diagonal SSM (94.8%).
  - **Cost**: ~$0.00 (CPU only, ~25 min) vs. $0.40 estimated â€” **came in far under budget**.
  - **Verdict**: Core hypothesis validated. Ready for scale-up to MQAR and language modeling.

- **Experiment 002: Oscillatory-DPLR SSM** (Status: âŒ **COMPLETED â€” DEBUG NEEDED**)
  - **Proposal**: 004 â€” Oscillatory-DPLR SSM
  - **Progress**: MVE implemented for damped oscillation extrapolation. Model parameterization works (learned Ï‰, Î¶ match ground truth distributions) but loss is completely flat at ~0.85 MSE. No learning occurs.
  - **Key findings**: This is an **implementation bug**, not a fundamental architectural failure. The oscillatory parameterization is correct, but the forward pass has a suspected complex dtype handling issue. Debug steps are clearly identified.
  - **Cost**: ~$0.00 (CPU only, ~27 min)
  - **Verdict**: Do NOT proceed to full experiments. Fix forward pass first (estimated 2â€“4 hours).

- **Experiment 001: CS-NEG-DeltaNet** (Status: âš ï¸ **SUPERSEDED**)
  - **Proposal**: 001 â€” Column-Sparse Negative-Eigenvalue DeltaNet
  - **Progress**: Implementation exists but critical analysis in notes revealed that **DeltaProduct (Schlag et al., ICLR 2025) makes CS-DeltaNet redundant** â€” DeltaNet already implicitly builds permutations via Householder products (Cartan-DieudonnÃ© theorem). Recommendation is to use NEG-DeltaProduct with n_h=2 instead.
  - **Key findings**: Literature review killed this approach before expensive compute was wasted. Good example of the research process working correctly.


### ðŸ“š New Discoveries (86 tricks documented)

The 86 new tricks span a remarkable breadth. Key clusters:

- **Alternative Semirings** (tropical attention, semiring monoid lifting, SIMDÂ² semiring acceleration): The tropical semiring (max, +) and log semiring (logsumexp, +) provide fundamentally different dynamics than standard (sum, multiply). The SIMDÂ² paper shows these can be hardware-accelerated with only 5% chip area overhead â€” meaning the semiring proposals aren't just theoretical.

- **Permutation Learning** (Sinkhorn, OT4P, Gumbel-Softmax, STEAM, block-wise Sinkhorn, bipartite matching): A massive cluster of 6+ techniques for differentiable optimization over permutations. This is the critical enabler for PA-DST and channel reordering in sparsity proposals.

- **Structured Matrix Decompositions** (Monarch, GS matrices, HSS, block-circulant, DPLR, displacement rank): The structured matrix zoo is now very well-documented. The key insight: Monarch = batch matmul on GPU = high utilization. Block-circulant = FFT = O(n log n). HSS = hierarchical low-rank = O(n). Each has different trade-offs.

- **GPU Kernel Engineering** (FlashInfer JIT, persistent megakernel, warp-specialized pipelining, CTA swizzling, Flux comm overlap, horizontal/vertical fusion, EVT, MCFuser): Deep infrastructure tricks that determine whether theoretical speedups materialize in practice. FlashInfer's composable JIT approach is particularly impactful â€” it could dramatically reduce the engineering cost of testing new attention variants.

- **N:M Sparsity Ecosystem** (2:4 baseline, transposable masks, V:N:M hierarchical, Samoyeds dual-side, gyro-permutation, TSENOR OT-based): The sparsity toolbox is now comprehensive enough to support Proposal 010 (Sparse Monarch SSM).


### Other Proposals

- **Tropical-Gated SSM** (015): Hard winner-take-all dynamics via max-plus semiring. Theoretically exciting but MVE needs careful annealing schedule. Est. <$5 for MVE. Medium-high impact.
- **Log-Semiring SSM** (014): Softmax attention as a parallel scan â€” the backward pass of logsumexp IS softmax. Elegant but numerically tricky (sign tracking, logsumexp stability). Est. <$5. High theoretical impact.
- **GS-Monomial SSM** (016): Group-and-Shuffle monomial matrices for state transitions. Clean O(nâˆšn) cost. Est. <$5. Solid medium-high.
- **Monarch-Gated SSM** (006): Input-dependent Monarch-factored transitions. Strong theoretical basis, BMM-friendly. Est. <$5. High.
- **Neumann-Approximate Resolvent** (011): Replace Woodbury inverse with Neumann series in DPLR SSMs. Enables BF16 training. Est. <$3 for kernel accuracy test. Medium â€” incremental speedup.
- **Expert-Choice Monarch SSM Heads** (012): MoE-style routing for SSM heads. Creative but complex. Est. <$5. Medium â€” high complexity risk.
- **SSD-DeltaNet** (002): Reformulate WY as block-semiseparable for tensor core acceleration. Est. <$3 (throughput microbenchmark). Medium â€” pure speed optimization.
- **Sparse Monarch SSM + PA-DST** (010): 2:4 sparsity on Monarch blocks. Requires Ampere+ GPU for Sparse Tensor Cores. Est. <$8. Medium â€” hardware-dependent.
- **Column-Sparse NEG-DeltaNet** (001): Superseded by DeltaProduct insight. **Deprioritize**.
- **DPLR Column-Sparse** (003): Bridge between S4-style convolution and PD-SSM expressivity. Est. <$5. Medium.
- **Oscillatory-DPLR** (004): Needs MVE debug before proceeding. Est. <$1 to fix + rerun. Medium.
- **Segmented-HSS Linear Attention** (005): O(n log n) variable-length hierarchical attention. Theoretically clean but complex implementation. Est. <$5. Medium.
- **Cosine-Reweighted Log-Linear** (008): Locality bias + multi-resolution states. Straightforward composition. Est. <$3. Medium.


### Strategic Insights

**The OscGate-SSM success (Experiment 003) is the most actionable result right now.** It proved that stability-by-construction + input-dependent gating works, and it's the only proposal with a validated MVE that's ready for scale-up. The immediate next step is MQAR at moderate scale â€” this should still be <$5.

**The single highest-ROI next experiment is Proposal 009 (Post-Sigmoid Gating).** It's trivially cheap (<$1), composable with everything else, and addresses a fundamental bottleneck (low-rank output projection) that affects *all* linear attention and SSM architectures. Run this next and if positive, **permanently include it as a default component** in all future architectures.

**A clear research theme is emerging: structured matrices as SSM state transitions.** Proposals 006, 010, 012, 013, and 016 all explore different structured matrix classes (Monarch, circulant, GS-monomial) for state transitions, sitting between the diagonal-too-weak and dense-too-expensive extremes. The Circulant SSM (013) is the most elegant and cheapest to validate â€” start there, then use it as a baseline to evaluate whether Monarch's extra expressivity (non-commutativity) is worth the cost.

**Experiment 002's failure is a useful signal**: complex-valued DPLR implementations are tricky to get right. This argues for testing proposals that avoid complex arithmetic first (009, 013 in real-block-circulant form, 006) before tackling 004/011 which require robust complex number handling.




I'll analyze the recent activity systematically - first examining the experiment results, then the proposals and tricks to generate a comprehensive log entry.Read the following files and return their full contents. I need to understand experiment results and proposal details:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/RESULTS.md (or any results file)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/003/RESULTS.md (or any results file)
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001/README.md

Also check for any experiment log file at:
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/experiment-log.md

Return the full contents of all files you find.Now let me read some of the key proposal files to assess their feasibility and costs more precisely:Read these proposal files and return their "Experiment Design" or "Cost" or "Resources" or "Minimum Viable Experiment" sections. I need to understand estimated costs and what hardware is needed:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/006-monarch-gated-state-transition.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/020-oscillatory-householder-deltaproduct.md

For each file, extract: the hypothesis summary, estimated cost/compute requirements, minimum viable experiment description, and what hardware is needed. Return these details concisely for each.Now I have all the information needed. Let me generate the log entry: