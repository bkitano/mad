---
title: Research Log Update
date: 2026-02-15T21:44:31.743679
timestamp: 2026-02-15 21:44:31
tricks: 225
proposals: 56
experiments: 27
---

I need to analyze a massive amount of activity - 225 tricks and 56 proposals with 27 experiments. Let me start by reading the key proposal and experiment files to understand the details before writing the log.Read the results files for all completed experiments. I need to find and read:
1. All files matching pattern **/results*.md or **/*_results*.md in /home/bkitano/Desktop/vault/projects/mad-architecture-search/
2. The experiment log at /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/experiment-log.md

Focus on completed experiments: 002, 004, 005, 007, 011, 022, 025, 026, 029. Read their results files to understand what worked and what didn't.

Search in /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/ for results files and also check /home/bkitano/Desktop/vault/projects/mad-architecture-search/experiments/ for results.I need to read the full content of a few key proposals that are marked as "proposed" (not yet implemented) and high-priority to assess their impact. Read these files:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/050-fp8-mixed-precision-chunkwise-linear-rnn-training.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/036-near-far-field-chunkwise-gla.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/043-newton-schulz-orthogonal-deltanet-transition.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/023-circulant-diagonal-ssm-state-transitions.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/056-flashmask-tile-skip-chunkwise-linear-rnn.md

For each, I need: the hypothesis, estimated cost, the MVE design, and success criteria.Now let me read a few more key proposals that are in "ongoing" status to understand progress on the most impactful lines of research.Read the README files for the ongoing experiments to understand their current status. Read:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/001/README.md (Column-Sparse Negative-Eigenvalue DeltaNet)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/009/README.md (Post-Sigmoid Gating)
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/016/README.md (GS-Monomial SSM)
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/015/README.md (Tropical SSM)
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/014/README.md (Log-Semiring SSM)

For each, tell me: what's implemented, what's the current status, and any results so far.Now I have a comprehensive understanding. Let me write the log entry.

## 2026-02-15 â€” 21:40 UTC

### ðŸŽ¯ High-Impact Proposals (Cheapest & Highest Upside)

**1. Post-Sigmoid Gating for Linear Attention (Proposal 009)** â€” Priority: **HIGH**
- **Hypothesis**: Applying post-readout sigmoid gating (NeurIPS 2025 Best Paper technique for softmax attention) to linear attention / SSM readout breaks the low-rank bottleneck in VÂ·W_O, improving quality 5â€“15%.
- **Why it matters**: This is a *zero-architecture-change* quality boost â€” a single elementwise sigmoid gate after readout. If it works for linear attention as well as it does for softmax, it's a free lunch for every SSM/linear-attention model. Experiment 009 is already running (Attempt 2 with improved hyperparams after Attempt 1 underfitted). The gating addresses a known theoretical weakness (the VÂ·W_O rank bottleneck is *worse* in linear attention than softmax).
- **Estimated cost**: <$1 for MVE (already running on T4)
- **Impact score**: 9/10 â€” Near-zero cost, broadly applicable if validated. The failure of Attempt 1 (33% gated vs 38% ungated) isn't fatal â€” it was a hyperparameter issue (gate init at 0.5 halving signal). Attempt 2 fixes this.

**2. Circulant-Diagonal SSM State Transitions (Proposal 023)** â€” Priority: **HIGH**
- **Hypothesis**: SSM state transitions parameterized as input-dependent circulant-diagonal products Dâ‚(x)Â·C(x)Â·Dâ‚‚(x) achieve full coordinate mixing at O(n log n) per step via FFT, while preserving parallel scan compatibility through Fourier-domain composition.
- **Why it matters**: This directly addresses the core expressivityâ€“efficiency tradeoff. Experiment 013 (Circulant SSM on Zâ‚ˆ) is already implemented and waiting. Experiment 027 (Cayley-Circulant Orthogonal SSM) is also implemented. Together they test whether circulant structure is the "right" middle ground â€” and critically, the FFT gives O(n log n) composition, meaning the parallel scan doesn't blow up to O(nÂ³ log T). This is the cheapest path to a non-diagonal SSM that might actually scale.
- **Estimated cost**: <$1 for MVE (<5 min on GPU)
- **Impact score**: 8/10 â€” If circulant-diagonal products compose efficiently in Fourier domain AND learn non-abelian state tracking, this could be a new standard SSM primitive. Risk: circulant structure is abelian, so may fail on Sâ‚….

---

### ðŸ§ª Experiment Updates

**Experiment 007: OscGate-SSM** (Status: âœ… **COMPLETED â€” SUCCESS**)
- **Proposal**: 007 â€” Oscillatory-Gated Selective SSM
- **Key finding**: Input-dependent oscillatory parameters (Ï‰(x), Î¶(x)) with stability-by-construction achieve **93.0% accuracy** on selective copying vs **46.8% for LTI LinOSS** â€” a 46pp gap proving selectivity works. Zero NaN/Inf events. 1.8Ã— overhead vs diagonal (acceptable).
- **Cost**: $0.00 (CPU only). **Next**: Scale to MQAR with 16+ KV pairs.

**Experiment 026: Cyclic Reduction for Dense SSM** (Status: âœ… **COMPLETED â€” SUCCESS**)
- **Proposal**: 026 â€” Cyclic Reduction vs Prefix Scan
- **Key finding**: **3.88Ã— wall-clock speedup** at T=1024 (CPU), with 6Ã— fewer GEMMs than prefix scan. Numerical accuracy at 8.5e-16. This validates that cyclic reduction is work-optimal for dense (non-diagonal) SSM recurrences â€” a critical enabler for DeltaNet/DeltaProduct at scale.
- **Cost**: $0.00 (CPU only). **Next**: GPU kernel implementation (code/026_scaled ready).

**Experiment 025: NystrÃ¶m Landmark Chunkwise SSM** (Status: âœ… **COMPLETED â€” SUCCESS**)
- **Proposal**: 025 â€” NystrÃ¶m Landmark Compression
- **Key finding**: 3.2Ã— memory compression with **99.25% accuracy** (vs 99.08% for full). Surprising: high approximation error (0.86) yet perfect accuracy â€” model *co-adapts* with compression. Currently 1.9Ã— slower due to pseudoinverse, but this inverts at larger state dims.
- **Cost**: ~$0.05 (T4, 4 minutes). **Next**: Scale to n=128 where O(mn) << O(nÂ²).

**Experiment 011: Neumann-Resolvent DPLR SSM** (Status: âœ… **COMPLETED â€” SUCCESS**)
- **Proposal**: 011 â€” Neumann Series for DPLR Resolvent
- **Key finding**: k=4 Neumann achieves 6.65e-5 relative error with up to **8.92Ã— speedup** at N=256. Zero convergence failures (spectral radius always <0.11 with HiPPO init).
- **Cost**: $0.00 (CPU). **Next**: GPU benchmark at râ‰¥2.

**Experiment 029: Circulant FAVOR+** (Status: âŒ **COMPLETED â€” ABANDON**)
- **Proposal**: 029 â€” Circulant Random Features for FAVOR+
- **Key finding**: FAVOR+ fundamentally fails associative recall (23.8% accuracy). **ReLU linear attention achieves 98.5%** â€” a 75pp gap. Circulant projection correctly matches dense FAVOR+ (proving the math works), but you can't optimize a broken foundation.
- **Cost**: ~$0.10. **Lesson**: Don't optimize FAVOR+; investigate why simple ReLU features work so well.

**Experiment 022: Displacement-Rank SSM** (Status: âŒ **COMPLETED â€” ABANDON**)
- **Proposal**: 022 â€” Cauchy-Like State Transitions
- **Key finding**: Cauchy kernel creates **optimization barriers** â€” 1/(s_i - s_j) terms destroy gradient conditioning. Î±=4 does NOT outperform Î±=1. Cauchy matvec 4.9Ã— slower than dense at n=16.
- **Cost**: $0.00. **Lesson**: Theoretical expressivity â‰  practical learnability. Focus on DPLR/Monarch/Circulant instead.

**Experiments 014, 015, 016 (Log-Semiring, Tropical, GS-Monomial)**: Implemented, awaiting GPU time. All <$1 to run. These test three distinct algebraic approaches to structured state transitions â€” the results will decisively narrow the design space.

---

### ðŸ“š New Discoveries (225 tricks documented)

The trick library has reached a remarkable 225 entries. Key thematic clusters:

- **Circulant/Structured Matrix Decompositions** (~40 tricks): A comprehensive taxonomy from basic block-circulant (013) through CSCS splitting (032), Toeplitz-circulant embedding (129), optimal circulant approximation (084), to advanced CUML factor circulant inversion (206). This is now the most complete structured-matrix reference for ML practitioners.

- **Chunkwise Linear RNN Infrastructure** (~25 tricks): The full stack from TFLA two-level tiling (158), GLA secondary chunking (177), fused chunkwise SSD (182), to Lightning Attention-2 (217) and RWKV-7 (219). Together these define the state-of-the-art in hardware-efficient linear recurrence training.

- **GPU Kernel Engineering** (~30 tricks): Warp specialization (141), persistent megakernels (091), CTA swizzling (033), Stream-K (121), ThunderKittens (202), Twill optimal scheduling (209), and SageAttention2 mixed-precision (190). These are the building blocks for anyone writing custom Triton/CUDA kernels.

- **Tropical/Semiring Algebra** (132, 108, 113): Tropical attention via Hilbert projective metric and semiring monoid lifting open a new algebraic direction â€” computing attention in non-standard algebraic structures. SIMDÂ² shows this can even be hardware-accelerated.

- **Differentiable Permutation Learning** (~12 tricks): Sinkhorn (115), Gumbel-Softmax (057), overrelaxed Sinkhorn (087), OT4P orthogonal relaxation (085), ShuffleSoftSort (110), Frank-Wolfe rounding (006). A complete toolkit for learning discrete structure.

---

### Other Proposals (not yet implemented, lower immediate priority)

- **050: FP8 Mixed-Precision Chunkwise Linear RNN Training** â€” High theoretical impact (1.4â€“1.8Ã— speedup) but requires H100 access and Triton FP8 expertise. MVE is cheap (<30 min) but full validation costs ~$256.
- **036: Near-Far Field Chunkwise GLA** â€” Elegant FMMformer-inspired decomposition enabling 4Ã— larger chunks. MVE is ~$2, but requires substantial kernel engineering.
- **043: Newton-Schulz Orthogonal DeltaNet Transition** â€” Replaces sequential UT forward-sub with tensor-core GEMMs. Promising for DeltaNet scaling but requires CUDA kernel work.
- **056: FlashMask Tile-Skip for Chunkwise Linear RNN** â€” Smart tile-skipping for causal + document-packed training. Mostly a kernel engineering effort.
- **047: LASP-2 + TFLA Multi-GPU Linear RNN** â€” Multi-GPU co-design for linear RNN pretraining. Important for scaling but needs â‰¥8 GPUs to test.
- **051: KS-Fused Monarch Projections** â€” Monarch-factored projections with fused kernel. ~15â€“25% training speedup target.
- **045: DCT Frequency-Domain Kernel for GLA** â€” DiJiang-style deterministic feature maps replacing GLA's identity map. Interesting but incremental.
- **053: MLA-Inspired Latent State Compression** â€” DeepSeek-V2 weight absorption for linear RNN inference. 2â€“4Ã— generation throughput target.

---

### Strategic Insights

**The research has converged on a clear hierarchy of what works and what doesn't.** Of 9 completed experiments, the 4 successes (OscGate-SSM, Neumann-Resolvent, NystrÃ¶m Compression, Cyclic Reduction) share a common trait: they leverage **well-conditioned algebraic structure** with clean optimization landscapes. The 5 failures share the opposite trait: either implementation overhead dominates algebraic gains (002, 005), or the algebra creates optimization pathologies (022, 029). **The lesson: theoretical elegance means nothing if gradients don't flow.**

**Three immediate actions, all under $3 total:**
1. **Run Experiments 014 + 015 + 016** (Log-Semiring, Tropical, GS-Monomial) â€” these are implemented and waiting. Together they test whether the optimal SSM algebra is (a) logarithmic semiring, (b) tropical semiring, or (c) monomial group-and-shuffle. The winner becomes the foundation for all subsequent proposals. Budget: ~$1.50 total.
2. **Watch Experiment 009** (Post-Sigmoid Gating, Attempt 2) â€” if the gate-init fix resolves underfitting, this is the single highest-impact result: a model-agnostic quality boost for all linear attention variants.
3. **Implement Proposal 023** (Circulant-Diagonal SSM) â€” the cheapest untested architectural idea (<$1 MVE) that directly addresses the diagonal-vs-dense tradeoff with the most hardware-friendly structure (FFT).

**The emerging meta-pattern**: The most promising direction is **structured non-diagonal state transitions that compose cheaply**. Circulant (O(n log n) via FFT), Monarch (O(nâˆšn) via BMM), and monomial (O(n) via permutation+scale) are the three viable candidates. The next 48 hours of experiments should decisively pick the winner.

---
