---
title: Research Log Update
date: 2026-02-15T08:06:00
timestamp: 2026-02-15 08:06:00
tricks: 1
proposals: 4
experiments: 4
source: legacy_migration
---

## 2026-02-15 â€” 08:06 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention & SSM Readout** (Proposal 009) â€” Priority: **HIGH**
- **Hypothesis**: Applying the NeurIPS 2025 Best Paper technique (sigmoid gating after attention) to linear attention/SSMs breaks their output low-rank bottleneck, yielding 5â€“15% perplexity improvement at <2% latency cost.
- **Why it matters**: This is the lowest-risk, highest-expected-value proposal. The technique is already validated for softmax attention; porting it to linear attention is a straightforward ~50-line change. The upside is disproportionately large for linear attention because these models already suffer a *worse* information bottleneck than softmax. If it works, every future linear attention/SSM model gets a free quality boost.
- **Estimated cost**: MVE ~$0.50 (<10 min, single GPU, CPU likely sufficient)
- **Impact score**: 9/10 â€” Near-zero risk, well-grounded theory, trivial to implement, broad applicability. The ideal first experiment.

**2. Circulant SSM: Fourier-Domain Parallel Scan** (Proposal 013) â€” Priority: **HIGH**
- **Hypothesis**: Block-circulant state transitions diagonalized via FFT enable element-wise parallel scans in frequency space â€” full coordinate mixing at O(n log n) cost per step.
- **Why it matters**: This directly resolves the central SSM dilemma (diagonal = fast but no mixing; dense = expressive but slow). FFT diagonalization is an elegant algebraic shortcut that leverages 50 years of optimized FFT libraries. If it works, it establishes a new sweet spot in the expressivityâ€“efficiency Pareto frontier that could become the default SSM parameterization.
- **Estimated cost**: MVE ~$0.25 (<5 min, single GPU)
- **Impact score**: 8.5/10 â€” Strong theoretical foundation (circulant algebra is well-understood), cheapest MVE of all proposals, clean test task (cyclic group composition). Moderate risk: FFT-domain scans may have numerical precision issues.

**3. OscGate-SSM (Proposal 007) â€” ALREADY VALIDATED âœ…** â€” Priority: **PROCEED TO SCALE**
- **Experiment 003 confirmed**: 93.0% selective copying accuracy (vs. 46.8% for LTI baseline). The oscillatory-gated mechanism works.
- **Next step**: Scale to MQAR with 8 layers / d=512 / ~50M params. Estimated cost ~$5â€“15 on spot GPUs.
- **Impact score**: 8/10 â€” Already de-risked. The question is now whether it scales.


### ðŸ§ª Experiment Updates

- **Experiment 002: Oscillatory-DPLR SSM** (Status: âŒ COMPLETED â€” DEBUG)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Progress**: Full MVE implemented and run on CPU in ~27 min. Training MSE stuck at 0.854 for all 50 epochs â€” model did not learn.
  - **Key findings**: The model failed to fit even training data (850Ã— above target). However, the *interpretability criterion passed*: learned frequencies Ï‰ and damping Î¶ matched ground truth distributions. This suggests the parameterization is correct but there's a forward-pass or gradient-flow bug (likely complex dtype handling in the DPLR low-rank correction).
  - **Cost**: $0.00 (CPU only) vs $0.50 estimated
  - **Action**: Debug forward pass. Try r=0 (pure diagonal) to isolate whether the bug is in the DPLR component.

- **Experiment 003: OscGate-SSM** (Status: âœ… COMPLETED â€” PROCEED)
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Progress**: Three attempts with progressive scaling. Final config: 2 layers, d=128, ~175K params, 10K training sequences, 100 epochs.
  - **Key findings**: **OscGate-SSM achieves 93.0% accuracy on selective copying** (vs. LinOSS 46.8%, DiagonalSSM 94.8%). Input-dependent oscillatory parameters enable content-based gating while maintaining stability (zero NaN events). Speed overhead only 1.8Ã— vs diagonal. The 46pp gap between OscGate and LinOSS is the headline result â€” making Ï‰ and Î¶ input-dependent transforms an LTI system into an effective LTV selector.
  - **Nuance**: DiagonalSSM slightly outperformed OscGate (94.8% vs 93.0%), suggesting the oscillatory constraint adds slight friction. But the stability guarantee (0 NaN vs potential divergence) may be worth 1.8pp at scale.
  - **Cost**: $0.00 (CPU only) vs $0.40 estimated

- **Experiment 001: Column-Sparse Negative-Eigenvalue DeltaNet** (Status: implemented, analysis complete)
  - **Key finding**: Analysis concluded that **DeltaProduct makes the CS-DeltaNet proposal partially redundant** â€” DeltaNet already builds permutations via accumulated Householder reflections. Recommended pivoting to NEG-DeltaNet with DeltaProduct instead.


### ðŸ“š New Discoveries (100 tricks documented)

The 100 new tricks span six major categories. Key highlights by theme:

**Alternative Semirings for Neural Computation**
- **Tropical Attention (Hilbert Projective)**: Attention computed in tropical projective space via max-plus operations. Enables native handling of combinatorial optimization within attention. *Game-changing if hardware catches up (see SIMDÂ² below).*
- **Semiring Monoid Lifting**: Formalizes replacing (+,Ã—) with alternative semirings (max-plus, log-semiring, min-plus). Theoretically rich â€” directly answers whether matrix multiplication is a "universal primitive."
- **SIMDÂ² Semiring Matrix Acceleration**: Hardware proposal for extending tensor cores to support 8+ semirings beyond standard GEMM. Only 5% chip area overhead. *This is the hardware enabler that would make tropical/log-semiring SSMs practical at scale.*

**Structured Matrices & Efficient Parameterizations**
- **Group-and-Shuffle Matrices**: Generalizes Monarch matrices. Two factors instead of Monarch's two, but with richer permutation structure. Key for proposals 006 and 016.
- **Monomial Matrix Closure**: Monomial matrices (permutation Ã— diagonal) are closed under multiplication â€” enabling efficient chained state transitions. Foundation for proposal 016.
- **Displacement Rank (Cauchy-Like)**: Unified framework for structured matrix compression. Could enable new SSM parameterizations beyond DPLR.

**Permutation Learning**
- **STEAM (STE Permutation in Monarch)**: Makes Monarch permutations learnable via STE. Directly applicable to making SSM state routing adaptive.
- **OT4P (Orthogonal Permutation Relaxation)**: Temperature-controlled differentiable mapping to permutations via SO(n). Avoids Sinkhorn's local minima. *Most promising differentiable permutation method documented.*
- **Sinkhorn, Gumbel-Softmax, Bipartite Matching**: Three complementary relaxation techniques for discrete permutation optimization, each with different tradeoffs.

**GPU Kernel Optimization**
- **FlashInfer JIT Fusion**, **Persistent Megakernel Fusion**, **FlashFuser DSM**, **Twill Joint SWP**: A wave of advanced kernel techniques for H100+ GPUs. These represent the infrastructure needed to make novel architectures competitive in practice.
- **2:4 Structured Sparsity**, **V:N:M Hierarchical Sparsity**, **Transposable N:M Masks**: The sparsity hardware ecosystem is maturing rapidly. Proposals combining sparsity with structured matrices (010) become increasingly practical.

**Hierarchical & Multi-Scale Structure**
- **HSS Matrices**, **Telescopic Decomposition**, **ULV Factorization**: A complete toolkit for hierarchically semiseparable computation. Enables proposal 005 (Segmented-HSS Linear Attention).


### Other Proposals

- **Tropical-Gated SSM (015)**: Max-plus parallel scan. Theoretically elegant but max-plus runs on CUDA cores at ~16Ã— lower throughput than GEMM. MVE is cheap but full-scale is expensive (~$600â€“1000). Wait for SIMDÂ² hardware or validate MVE first.
- **Log-Semiring SSM (014)**: Softmax-native scan via logsumexp semiring. Same CUDA-core throughput limitation as tropical. MVE ~$0.40. Strong theoretical appeal but practical scaling uncertain.
- **GS-Monomial SSM (016)**: Group-and-Shuffle monomial state transitions. Elegant algebraic construction. Full-scale cost unclear but MVE should be cheap.
- **Hyperoctahedral Signed-Permutation SSM (017)**: Signed permutations for state tracking. Theoretically maximal within O(n) per-step budget. Requires Gumbel-softmax over 2^n Â· n! group â€” may be too large for practical optimization.
- **Capacitance-Coupled Multi-Scale SSM (019)**: Cross-scale coupling via capacitance matrix. Architecturally novel. MVE cost unclear.
- **SSD-DeltaNet (002)**: WY-based semiseparable decomposition for DeltaNet. High impact if it works â€” directly accelerates the strongest existing linear RNN. But implementation is complex.
- **Sparse Monarch SSM (010)**: 2:4 sparsity + PA-DST + Monarch. Hardware-aligned but requires 3 interacting components to work together.
- **Expert-Choice Monarch SSM Heads (012)**: MoE-style routing for SSM heads. Novel idea but adds complexity.
- **Neumann-Approximate Resolvent (011)**: Replaces exact Woodbury with Neumann series. Practical speedup but incremental.
- **OH-DeltaProduct (020)**: Oscillatory + Householder decomposition. Theoretically maximal but complex. Full-scale ~$600.
- **Column-Sparse NEG-DeltaNet (001)**: Partially superseded by DeltaProduct (per Experiment 001 analysis).
- **DPLR Column-Sparse (003)**, **Oscillatory-DPLR (004)**: Experiment 002 showed DPLR implementation needs debugging first.
- **Segmented-HSS Linear Attention (005)**, **cos-LogLinear (008)**, **Hutchinson Adaptive Rank (018)**: Medium priority, theoretically interesting but less urgent.


### Strategic Insights

**The semiring frontier is the big theoretical story.** Three proposals (014, 015, and the semiring monoid lifting trick) point toward replacing (+,Ã—) in SSM scans with alternative semirings. This is intellectually exciting and could unify SSMs with attention in a new way. But the practical bottleneck is hardware: max-plus and logsumexp run on CUDA cores, not tensor cores, creating a ~16Ã— throughput penalty. The SIMDÂ² trick shows this is solvable in hardware but not available today. **Recommendation: Run MVEs for 014 and 015 (<$1 total) to validate the algorithmic claims, but don't commit to full-scale until the throughput gap is quantified.**

**The immediate priority ladder is clear:**
1. **Proposal 009 (Post-Sigmoid Gating)** â€” Run NOW (<$0.50, ~10 min). Lowest risk, highest generality. Even a modest improvement validates a technique applicable to every linear attention/SSM model.
2. **Proposal 013 (Circulant SSM)** â€” Run next (<$0.25, ~5 min). Tests a fundamental algebraic idea. FFT operations run on existing hardware at full speed.
3. **Scale Experiment 003 (OscGate-SSM)** â€” Already validated at small scale. Push to MQAR/8-layer to test scaling (~$5â€“15).
4. **Debug Experiment 002 (Osc-DPLR)** â€” The interpretability pass suggests the parameterization is sound. A forward-pass fix could unlock this.

**The Experiment 001 finding that DeltaProduct subsumes CS-DeltaNet is an important negative result** â€” it should redirect effort away from explicit permutation learning (proposals relying on Gumbel-softmax over permutation groups) and toward the implicit permutation construction via Householder products. This favors proposals 020 (OH-DeltaProduct) and 002 (SSD-DeltaNet) over 017 (Hyperoctahedral) and 001 (CS-DeltaNet).




I'll analyze the recent activity systematically by first examining some key proposals and experiment results in detail.Read the following files and return their full contents:

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/RESULTS.md (or any results file in that directory)
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/003/RESULTS.md (or any results file in that directory)
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/README.md

Also check for any results files in code/001/ and code/004/.

Look in these directories for files named RESULTS.md, results.md, results.txt, or similar. Also check for experiment-log.md in the experiments directory.

Return all content you find.Read the following proposal files and return their key sections (especially the Hypothesis, Estimated Cost, Experiment Design, and Success Criteria sections):

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/001-column-sparse-negative-eigenvalue-deltanet.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/002-ssd-deltanet-wy-hybrid.md

Return the key sections for each.Now let me write the comprehensive log entry.