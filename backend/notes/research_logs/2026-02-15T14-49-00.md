---
title: Research Log Update
date: 2026-02-15T14:49:00
timestamp: 2026-02-15 14:49:00
tricks: 1
proposals: 4
experiments: 5
source: legacy_migration
---

## 2026-02-15 â€” 14:49 UTC

### ðŸŽ¯ High-Impact Proposals

**1. Post-Sigmoid Gating for Linear Attention / SSM Readout** (Proposal 009) â€” Priority: **HIGH**
- **Hypothesis**: Applying the NeurIPS 2025 Best Paper technique (head-specific sigmoid gate after attention readout) to linear attention and SSMs will break the low-rank bottleneck in $W_V W_O$, improving quality 5â€“15% with <2% latency overhead.
- **Why it matters**: This is the *lowest-hanging fruit* in the entire batch. It's a ~5-line code change (add `output *= sigmoid(xW_g)` after readout), requires zero architectural redesign, and targets a known fundamental weakness of linear attention â€” the information bottleneck from compressing history into a fixed $d \times d$ state. The hypothesis is well-grounded: the original paper showed gains on softmax attention, and the bottleneck is *worse* for linear attention, suggesting even larger gains. The MVE cleanly isolates the effect (gated vs. ungated cosFormer on MQAR).
- **Estimated cost**: **<$1** (MVE: ~10 min single GPU)
- **Impact score**: **9/10** â€” Trivial to implement, near-zero risk, directly addresses a known bottleneck. Even if the gain is modest (5% vs. 15%), the cost of finding out is negligible.

**2. Circulant SSM: Fourier-Domain Parallel Scan** (Proposal 013) â€” Priority: **HIGH**
- **Hypothesis**: Circulant state transitions, diagonalized via FFT, enable element-wise parallel scans in frequency space â€” recovering $O(\log T)$ parallel depth of diagonal SSMs with full coordinate mixing at $O(n \log n)$ per step.
- **Why it matters**: This hits the sweet spot of the diagonal vs. dense SSM tradeoff. Unlike Monarch ($O(n\sqrt{n})$) or DeltaProduct ($O(n^2)$), circulant transitions are *exactly* diagonalizable, meaning the parallel scan reduces to the same element-wise form as diagonal SSMs â€” just in the Fourier domain. The MVE already has an implementation (Experiment 013, status: implemented). The critical limitation â€” commutativity means it cannot represent non-abelian groups â€” is acknowledged but acceptable for many practical tasks. At $0.25 MVE cost, this is a cheap test of whether circulant coordinate mixing adds meaningful value over diagonal.
- **Estimated cost**: **<$1** (MVE: ~5 min single GPU)
- **Impact score**: **8/10** â€” Elegant theory, cheap to validate, but commutativity caps its ceiling. If it works on Z_8 but fails on S_5, the *negative result itself* is informative: it proves coordinate mixing alone isn't enough, non-commutativity is the key.


### ðŸ§ª Experiment Updates

- **Experiment 007: OscGate-SSM** (Status: **completed â€” PROCEED**)
  - **Proposal**: 007-oscillatory-gated-selective-ssm
  - **Progress**: Full MVE on selective copying. Three models trained: OscGate-SSM, LinOSS (LTI), DiagonalSSM.
  - **Key findings**: OscGate-SSM achieved **93.0%** accuracy, validating that input-dependent oscillatory parameters enable selectivity. LinOSS (fixed parameters) achieved only **46.8%** â€” a 46pp gap proving selectivity is essential. However, **DiagonalSSM matched at 94.8%**, suggesting the oscillatory inductive bias doesn't provide additional benefit on this task. Zero NaN/Inf events validates stability-by-construction. The oscillatory structure adds 1.8Ã— overhead for no quality gain on selective copying.
  - **Cost**: ~$0.00 (CPU only)
  - **âš ï¸ Implication**: The oscillatory parameterization's value proposition needs a *different* task to justify itself â€” one where long-range periodic memory matters (e.g., periodic signal detection, music modeling). On pure state-tracking tasks, simpler diagonal gating wins.

- **Experiment 022: Displacement-Rank SSM (DR-SSM)** (Status: **completed â€” ABANDON**)
  - **Proposal**: 022-displacement-rank-ssm-state-transitions
  - **Progress**: Tested Î± âˆˆ {0, 1, 2, 4, 16} on S5 permutation composition.
  - **Key findings**: Î±=1 already reached **95.8%** accuracy, invalidating the hypothesis that higher displacement rank systematically improves expressivity. At longer sequences (seq_len=20), *all* Cauchy-structured models catastrophically failed while dense SSM solved it at 97.2%. The $1/(s_i - s_j)$ Cauchy kernel creates **pathological gradient flow**. Cauchy matvec is 4.9Ã— slower than dense at n=16. **Verdict: Displacement rank is theoretically elegant but practically broken.**
  - **Cost**: ~$0.00 (CPU only)

- **Experiment 026: Cyclic Reduction vs Prefix Scan** (Status: **completed â€” PROCEED**)
  - **Proposal**: 026-cyclic-reduction-randmscan-ssm-recurrence
  - **Progress**: CPU benchmark of cyclic reduction vs. prefix scan for dense SSM recurrences.
  - **Key findings**: **3.88Ã— speedup** over prefix scan at T=1024, n=32 after vectorizing back-substitution. GEMM count ratio of 6.01Ã— (near theoretical max of 6.67Ã—). Machine-precision numerical accuracy. All operations are batched GEMMs â€” ideal for tensor cores. A scaled GPU experiment (026_scaled) is implemented and ready for validation.
  - **Cost**: ~$0.00 (CPU only)

- **Experiment 004: Oscillatory-DPLR SSM** (Status: **completed â€” DEBUG**)
  - **Proposal**: 004-oscillatory-dplr-ssm
  - **Key findings**: Model failed to learn (training loss flat at ~0.854 across 50 epochs). Learned parameters are in sensible ranges (Ï‰ âˆˆ [0.012, 0.096], Î¶ âˆˆ [0.26, 0.71]), suggesting a forward computation bug rather than fundamental limitation.

- **Experiments 013, 014, 015, 017, 019, 020, 021, 027, 028, 031** (Status: **implemented**, awaiting runs)
  - A large batch of MVEs are coded and ready. Most promising to run next: **013** (circulant SSM on Z_8), **017** (hyperoctahedral signed-perm SSM on B_3), **028** (Neumann-Cayley orthogonal SSM on S5).


### ðŸ“š New Discoveries

165 tricks were documented â€” a massive cataloging effort. Key thematic clusters:

- **Circulant & structured matrix decompositions** (028, 032, 084, 129, 024, 023, 100, 038, 079, 067): A comprehensive library of circulant, Toeplitz, and block-circulant techniques. The **Toeplitz-to-Circulant Embedding** (129) and **CSCS splitting** (032) are foundational for anyone building circulant SSMs. The **CDFlow** (023) and **CDVFT** (024) tricks show circulant-diagonal products are already being used for PEFT and normalizing flows.

- **HSS/hierarchical matrix algorithms** (001, 008, 052, 054, 059, 063, 088, 097, 098, 122, 123, 127, 131, 138, 146): A deep dive into hierarchically semiseparable matrices. The **black-box randomized HSS compression** (008) and **quasi-optimal greedy HSS approximation** (097) are directly applicable to Proposal 021 (HSS-compressed attention).

- **GPU kernel optimization** (033, 039, 046, 047, 049, 050, 051, 061, 075, 091, 103, 121, 126, 135, 141, 158): The kernel tricks form a toolkit for implementing the algebraic ideas efficiently. **Warp-specialized pipelining** (141), **Stream-K** (121), and **BRGEMM** (batch-reduce GEMM) are critical building blocks for custom SSM kernels. **TFLA** (158) shows two-level tiled chunkwise parallelism already achieves significant speedups for linear RNNs.

- **N:M structured sparsity** (130, 133, 136, 140, and nmSPARSE): The transposable N:M mask techniques (130, 133/TSENOR) are key enablers for Proposals 024, 031, and 035 â€” training sparse SSMs from scratch with hardware acceleration on both forward and backward passes.

- **Semiring generalizations** (108, 113, 132): The **semiring monoid lifting** (108) and **tropical attention** (132) directly motivate Proposals 014 and 015 (log-semiring and tropical SSMs).

- **Permutation learning** (003, 006, 007, 085, 087, 110, 114, 115, 120): A rich set of tools for differentiable permutation optimization â€” **Sinkhorn** (115), **Gumbel-Softmax** (057), **OT4P** (085), **ShuffleSoftSort** (110), **auction algorithm** (003). These are prerequisites for proposals involving learned channel permutations (016, 024, 030).


### Other Proposals

Organized by feasibility tier:

**Sub-$2 MVE (run these next):**
- **Monarch-Gated SSM** (006): Dense transitions via Monarch at $O(n\sqrt{n})$. MVE ~$0.50 on S5 composition. Already a proven architecture pattern.
- **OscGate-SSM** (007): Completed MVE validates selectivity. Need a *periodic* task to test oscillatory advantage.
- **cos-LogLinear Attention** (008): Combine cosFormer locality bias with log-linear multi-resolution states. MVE ~$0.50.
- **Sparse Monarch SSM** (010): 2:4 sparsity on Monarch blocks + PA-DST permutation. MVE ~$0.50.
- **Neumann-Resolvent Chunkwise SSM** (011): Replace exact Woodbury inverse with Neumann series in DPLR training. MVE ~$0.50.
- **Expert-Choice Monarch SSM Heads** (012): MoE-style routing for SSM state heads. MVE ~$0.50.
- **Log-Semiring SSM** (014): Softmax-native parallel scan via logsumexp. MVE implemented, ready to run.
- **Tropical-Gated SSM** (015): Hard max-plus state dynamics. MVE implemented, ready to run.
- **GS-Monomial SSM** (016): Group-and-shuffle monomial state transitions. MVE ~$0.25.
- **Hyperoctahedral SSM** (017): Signed permutations for non-abelian state tracking. MVE implemented.
- **Capacitance-Coupled Multi-Scale SSM** (019): Cross-scale coupling via small capacitance matrix. MVE implemented.
- **OH-DeltaProduct** (020): Oscillatory + Householder decomposition. MVE implemented.
- **HSS-Attention** (021): Adaptive hierarchical attention compression. MVE implemented.
- **Cayley-Circulant Orthogonal SSM** (027): Exact orthogonality via Cayley + circulant. MVE implemented.
- **Neumann-Cayley SSM** (028): Approximate Cayley inverse for input-dependent orthogonality. MVE implemented.

**$2â€“$10 MVE (kernel-level experiments):**
- **Chimera-Fused Chunkwise SSM** (032): GEMM-chain fusion for intra-chunk computation. MVE ~$1.50 (kernel benchmark only).
- **Stream-K BRGEMM Chunkwise** (034): Work-centric decomposition for state accumulation. MVE ~$2.
- **Transposable N:M Sparse GLA** (035): 4:8 sparsity on all GLA projections. MVE ~$2.
- **Near-Far Field GLA** (036): FMM-style chunk decomposition. MVE ~$2.

**Higher cost (deprioritize for now):**
- **EVT-Fused SSM Epilogues** (033): Requires CUTLASS EVT integration, harder to prototype.
- **Circulant FAVOR+** (029): Interesting but incremental over existing random feature methods.
- **Group-Matrix Displacement SSM** (030): Theoretically rich but high complexity.


### Strategic Insights

**1. The expressivity-efficiency frontier is the central battle.** The completed experiments paint a clear picture: diagonal SSMs are too weak (fail on S5 composition), dense SSMs are too expensive, and the Cauchy-structured middle ground (DR-SSM) is broken in practice. The winning zone is **structured non-diagonal transitions** â€” circulant, Monarch, GS-monomial, or Householder products â€” that preserve parallel scan compatibility. Proposals 006, 013, 016, and 027 each attack this from different angles. **Run the implemented MVEs for 013 (circulant/Z_8) and 017 (hyperoctahedral/B_3) immediately â€” they're the cheapest experiments that directly test different points on this frontier.**

**2. Post-readout gating (Proposal 009) is the single highest-ROI experiment.** It costs essentially nothing, requires minimal code, and targets a *proven* technique (NeurIPS 2025 Best Paper) applied to a *known bottleneck* (linear attention's information compression). If it works, it's immediately deployable in any GLA/Mamba variant. **This should be the very first thing to run.**

**3. The experiment pipeline is producing actionable negative results.** The DR-SSM abandonment (Cauchy gradient pathology) and OscGate-SSM's parity with diagonal SSM on selective copying are both valuable. They redirect effort: skip Cauchy-like parameterizations, and test oscillatory models on *periodic* rather than *selective* tasks. The 3.88Ã— cyclic reduction speedup is a strong positive signal â€” the GPU validation (026_scaled) should be run to confirm real-world gains.

**Recommended execution order** (all under $10 total):
1. **009** (post-sigmoid gating) â€” highest ROI, ~$0.50
2. **013** (circulant SSM on Z_8) â€” already implemented, ~$0.25
3. **014** (log-semiring SSM) â€” already implemented, ~$0.25
4. **017** (hyperoctahedral SSM) â€” already implemented, ~$0.25
5. **028** (Neumann-Cayley SSM on S5) â€” already implemented, ~$0.50
6. **027** (Cayley-circulant SSM on delayed copy) â€” already implemented, ~$0.25
7. **026_scaled** (cyclic reduction GPU validation) â€” confirm the 3.88Ã— result on GPU

Total estimated cost: **~$2.25** for 7 experiments covering the full expressivity-efficiency frontier.




I need to analyze a massive amount of research activity. Let me process this systematically by reading the key experiment results and high-impact proposals in detail.Read the following experiment result files and provide me with a concise summary of each (key findings, whether hypothesis was confirmed, actual cost, and any surprises):

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/002/RESULTS.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/022/RESULTS.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/026/RESULTS.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/029/RESULTS.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/004/RESULTS.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/007/RESULTS.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/011/RESULTS.md
8. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/005/RESULTS.md
9. /home/bkitano/Desktop/vault/projects/mad-architecture-search/code/025/RESULTS.md

Just read the files and summarize the key findings for each. Do NOT write any code.Now let me read a few of the most promising-looking proposals in detail to assess their impact potential:Read these proposal files and provide a concise summary of each, focusing on: (1) the core hypothesis, (2) estimated compute cost, (3) what hardware is needed, (4) how novel the combination is, and (5) what the experiment would look like. Do NOT write code, just read and summarize.

1. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/009-post-sigmoid-gating-linear-attention.md
2. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/014-log-semiring-ssm-scan.md
3. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/013-circulant-ssm-fourier-domain-scan.md
4. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/020-oscillatory-householder-deltaproduct.md
5. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/023-circulant-diagonal-ssm-state-transitions.md
6. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/033-evt-fused-ssm-swiglu-epilogues.md
7. /home/bkitano/Desktop/vault/projects/mad-architecture-search/proposals/015-tropical-gated-ssm-scan.mdNow I have a comprehensive picture. Let me generate the log entry.