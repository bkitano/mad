"""
S5 Permutation Group Composition Task

The symmetric group S5 consists of all permutations of {0, 1, 2, 3, 4}.
- |S5| = 5! = 120 elements
- Generated by transpositions (i, j) â€” swaps of two elements

Task: Given a sequence of transpositions [t1, t2, ..., tk], predict the
resulting permutation t1 * t2 * ... * tk.

This is the canonical test for coordinate mixing ability:
- Diagonal SSMs (operating per-coordinate) provably cannot solve this
- Monarch's built-in permutation P_b should enable the coordinate routing needed

Encoding:
- 10 transpositions as generators: (0,1), (0,2), (0,3), (0,4), (1,2), (1,3), (1,4), (2,3), (2,4), (3,4)
- 120 output classes (one per S5 permutation)
- Special tokens: BOS=130, EOS=131, PAD=132
"""

import itertools
import random
from typing import Optional

import torch
from torch.utils.data import Dataset, TensorDataset, IterableDataset


# All 10 transpositions of S5 (generators)
TRANSPOSITIONS = [
    (0, 1), (0, 2), (0, 3), (0, 4),
    (1, 2), (1, 3), (1, 4),
    (2, 3), (2, 4),
    (3, 4),
]

# All 120 permutations of S5
ALL_PERMS = list(itertools.permutations(range(5)))
PERM_TO_IDX = {p: i for i, p in enumerate(ALL_PERMS)}

NUM_GENERATORS = len(TRANSPOSITIONS)     # 10
NUM_ELEMENTS = len(ALL_PERMS)            # 120
IDENTITY = (0, 1, 2, 3, 4)

# Token indices:
# 0-9: transposition generators
# 10-129: group elements (for output, though we use class labels 0-119)
BOS_IDX = 130
EOS_IDX = 131
PAD_IDX = 132
NUM_INPUT_TOKENS = 133  # 0-9 generators + BOS/EOS/PAD


def apply_transposition(perm: tuple, trans: tuple) -> tuple:
    """Apply transposition (i, j) to a permutation.

    Composition: (perm * trans)(x) = perm(trans(x))
    We compute the right-multiplication: perm composed with trans.
    """
    i, j = trans
    perm_list = list(perm)
    perm_list[i], perm_list[j] = perm_list[j], perm_list[i]
    return tuple(perm_list)


def compose_transpositions(transposition_indices: list[int]) -> int:
    """Compose a sequence of transpositions and return the S5 element index.

    Args:
        transposition_indices: List of indices into TRANSPOSITIONS (0-9)

    Returns:
        Index of resulting permutation in ALL_PERMS (0-119)
    """
    perm = IDENTITY
    for t_idx in transposition_indices:
        perm = apply_transposition(perm, TRANSPOSITIONS[t_idx])
    return PERM_TO_IDX[perm]


def generate_s5_batch(
    batch_size: int,
    seq_len: int,
    max_pad_len: int,
) -> tuple[torch.Tensor, torch.Tensor]:
    """Generate a batch of S5 permutation composition samples on-the-fly.

    Each sample:
        input  = [BOS, t1, t2, ..., tk, EOS, PAD, PAD, ...]
        target = index of (t1 * t2 * ... * tk) in ALL_PERMS

    Uses fixed seq_len to avoid distribution shift between train/test.

    Args:
        batch_size: Number of sequences to generate
        seq_len: Exact number of transpositions per sequence
        max_pad_len: Total padded sequence length (includes BOS + seq + EOS)

    Returns:
        inputs: (batch_size, max_pad_len) tensor of token indices
        targets: (batch_size,) tensor of target permutation indices (0-119)
    """
    inputs = torch.full((batch_size, max_pad_len), PAD_IDX, dtype=torch.long)
    targets = torch.zeros(batch_size, dtype=torch.long)

    for i in range(batch_size):
        # Generate random sequence of transpositions
        trans_seq = [random.randint(0, NUM_GENERATORS - 1) for _ in range(seq_len)]

        # Compose to get target permutation
        target_perm_idx = compose_transpositions(trans_seq)

        # Build input: [BOS, t1, t2, ..., tk, EOS, PAD...]
        inputs[i, 0] = BOS_IDX
        for j, t in enumerate(trans_seq):
            inputs[i, j + 1] = t
        inputs[i, seq_len + 1] = EOS_IDX

        targets[i] = target_perm_idx

    return inputs, targets


class S5OnlineDataset(IterableDataset):
    """S5 permutation composition dataset with online data generation.

    Generates fresh random batches on every iteration to prevent memorization.
    This is key for testing true generalization (learning the group operation).
    """

    def __init__(
        self,
        seq_len: int = 20,
        batches_per_epoch: int = 100,
        batch_size: int = 64,
        max_pad_len: int = 24,
    ):
        self.seq_len = seq_len
        self.batches_per_epoch = batches_per_epoch
        self.batch_size = batch_size
        self.max_pad_len = max_pad_len

    def __iter__(self):
        for _ in range(self.batches_per_epoch):
            inputs, targets = generate_s5_batch(
                self.batch_size, self.seq_len, self.max_pad_len
            )
            # Yield individual samples
            for i in range(self.batch_size):
                yield inputs[i], targets[i]


class S5FixedDataset(Dataset):
    """Fixed S5 dataset for evaluation (deterministic)."""

    def __init__(
        self,
        num_samples: int = 2000,
        seq_len: int = 20,
        max_pad_len: int = 24,
        seed: int = 42,
    ):
        random.seed(seed)
        self.inputs, self.targets = generate_s5_batch(
            num_samples, seq_len, max_pad_len
        )
        random.seed()  # Re-randomize

    def __len__(self):
        return len(self.targets)

    def __getitem__(self, idx):
        return self.inputs[idx], self.targets[idx]


# Legacy functions for backward compatibility
def generate_s5_dataset(
    num_samples: int,
    min_seq_len: int,
    max_seq_len: int,
    max_pad_len: int,
    seed: Optional[int] = None,
) -> tuple[torch.Tensor, torch.Tensor]:
    """Generate S5 permutation composition dataset (variable length)."""
    if seed is not None:
        random.seed(seed)

    inputs = torch.full((num_samples, max_pad_len), PAD_IDX, dtype=torch.long)
    targets = torch.zeros(num_samples, dtype=torch.long)

    for i in range(num_samples):
        seq_len = random.randint(min_seq_len, max_seq_len)
        trans_seq = [random.randint(0, NUM_GENERATORS - 1) for _ in range(seq_len)]
        target_perm_idx = compose_transpositions(trans_seq)
        inputs[i, 0] = BOS_IDX
        for j, t in enumerate(trans_seq):
            inputs[i, j + 1] = t
        inputs[i, seq_len + 1] = EOS_IDX
        targets[i] = target_perm_idx

    return inputs, targets


class S5Dataset:
    """S5 dataset with train/test split (fixed, for backward compatibility)."""

    def __init__(self, num_samples=10000, min_seq_len=10, max_seq_len=50,
                 max_pad_len=54, test_fraction=0.2, seed=42):
        inputs, targets = generate_s5_dataset(
            num_samples, min_seq_len, max_seq_len, max_pad_len, seed)
        n_test = int(num_samples * test_fraction)
        perm = torch.randperm(num_samples, generator=torch.Generator().manual_seed(seed))
        inputs, targets = inputs[perm], targets[perm]
        self.train_inputs = inputs[:num_samples - n_test]
        self.train_targets = targets[:num_samples - n_test]
        self.test_inputs = inputs[num_samples - n_test:]
        self.test_targets = targets[num_samples - n_test:]

    def get_train_dataset(self):
        return TensorDataset(self.train_inputs, self.train_targets)

    def get_test_dataset(self):
        return TensorDataset(self.test_inputs, self.test_targets)


def generate_fixed_length_s5(num_samples, seq_len, seed=12345):
    """Generate S5 dataset with exactly seq_len transpositions per sample."""
    return S5FixedDataset(num_samples, seq_len, max_pad_len=seq_len + 4, seed=seed)
