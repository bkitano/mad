# 249: PD-SSM Permutation-Diagonal Transition Matrices

**Category**: decomposition
**Gain type**: expressivity
**Source**: Terzić et al., "Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models" (NeurIPS 2025 Spotlight)
**Paper**: [papers/pd-ssm-structured-sparse-transition.pdf]
**Documented**: 2026-02-16

## Description

PD-SSM parametrizes SSM transition matrices $A(u_t)$ as the product of a binary column one-hot matrix $P(u_t)$ and a complex-valued diagonal matrix $D(u_t)$. This gives SSMs the expressiveness to emulate any $N$-state finite-state automaton (FSA) with a single layer of dimension $N$ and an $N \times N$ readout — provably optimal state size — while keeping the parallel scan cost at $\Theta(LN)$, identical to diagonal SSMs.

The key insight: column one-hot matrices form a **monoid** under matrix multiplication, so their products can be computed via gather-scatter + elementwise multiply in $\Theta(N)$ rather than the usual $\Theta(N^3)$ dense matrix multiply. This makes associative parallel scans over PD matrices as cheap as diagonal scans but far more expressive.

## Mathematical Form

**Core Operation:**

$$
A(u_t) = P(u_t) \, D(u_t)
$$

where $P(u_t) \in \{0, 1\}^{N \times N}$ is a binary column one-hot matrix (each column has exactly one nonzero entry) and $D(u_t) \in \mathbb{C}^{N \times N}$ is a complex diagonal matrix.

**Key Definitions:**

- $P \in \mathbb{H}^{N \times N}$ — the set of column one-hot matrices (a monoid under multiplication)
- $D = \operatorname{diag}(d_1, \ldots, d_N)$ where $|d_i| \in (0,1)$ — ensures BIBO stability
- $x_t = A_t x_{t-1} + b_t$ — the SSM recurrence

**Efficient Matrix Multiplication in $\mathbb{H}^{N \times N}$:**

For $A, B \in \mathbb{H}^{N \times N}$, the product $C = AB$ can be computed in $\Theta(N)$ operations. Each column one-hot matrix is represented as a pair: (index vector, value vector). The product indices are computed via a gather operation, and the values via elementwise multiplication:

$$
C_{ij} = A_{i, \text{idx}_B(j)} \cdot B_{\text{idx}_B(j), j}
$$

**Parallel Scan Element:**

$$
(A_{t+1}, b_{t+1}) \bullet (A_t, b_t) \mapsto (A_{t+1} A_t, \; A_{t+1} b_t + b_{t+1})
$$

Since $A_t \in \mathbb{H}^{N \times N}$, the matrix product $A_{t+1} A_t$ costs $\Theta(N)$ instead of $\Theta(N^3)$.

**Generation Architecture:**

The diagonal magnitudes and phases are generated by MLPs:

$$
|D(u_t)| = \sigma(W_o^M(\sigma_{\text{gelu}}(W_i^M u_t + b_i^M) + b_o^M)) \in (0, 1)^N
$$

$$
\phi(D(u_t)) = 2\pi\sigma(W_o^P(\sigma_{\text{gelu}}(W_i^P u_t + b_i^P) + b_o^P)) \in (0, 2\pi)^N
$$

The permutation matrix $P$ is generated via soft-selection from a learnable dictionary $\{M_k\}_{k \in [K]}$:

$$
s(u_t) = \operatorname{softmax}(S u_t) \in \Delta^{K-1}, \quad M(u_t) = \sum_{k=1}^{K} s_k(u_t) M_k
$$

$$
P_{:,j}(u_t) = \operatorname{hardmax}(M_{:,j}(u_t))
$$

with softmax used as a surrogate gradient in the backward pass.

**BIBO Stability:**

$$
\|x_t\|_2 \leq \sqrt{N} B / \varepsilon \quad \forall t
$$

where $\|A_t\|_\infty \leq 1 - \varepsilon$ and $\|b_t\|_2 \leq B$.

## Complexity

| Operation | Diagonal SSM | Dense SSM | PD-SSM |
|-----------|-------------|-----------|--------|
| Parallel scan (total) | $\Theta(LN)$ | $\Theta(LN^3)$ | $\Theta(LN)$ |
| Recurrent step | $\Theta(N)$ | $\Theta(N^2)$ | $\Theta(N)$ |
| Parallel scan memory | $\Theta(NL)$ | $\Theta(N^2 L)$ | $\Theta(NL)$ |
| FSA expressiveness | Limited | Full | Full |

**Memory:** $O(NL)$ — same as diagonal SSMs, vs $O(N^2 L)$ for dense.

**Runtime:** 71× faster than dense SSM at $D = 5632$; only ~7× slower than pure diagonal SSM (due to MLP overhead for $P$ and $D$ generation).

## Applicability

- **State-space models** needing expressive transition matrices (FSA state tracking, algorithmic tasks)
- **Hybrid Transformer-SSM architectures** where SSM layers need to track complex state
- **Multivariate time-series classification** — achieves SOTA on long-range UEA benchmarks
- Any SSM variant using parallel scans (Mamba, S4, S5, etc.) that wants richer state dynamics without cubic scan cost

## Limitations

- **Gather-scatter operations**: The column one-hot multiplication requires gather/scatter indexing, which breaks coalesced memory access patterns on GPUs. This is a known GPU-unfriendly pattern per the human feedback guidelines.
- **Dictionary overhead**: Requires storing and soft-selecting from $K$ learnable matrices $M_k \in \mathbb{R}^{N \times N}$, adding $O(KN^2)$ parameters.
- **Hardmax non-differentiability**: Relies on surrogate gradients (softmax backward pass for hardmax forward), which can introduce gradient bias.
- **Not matmul-shaped**: The core operation (gather + elementwise multiply) does not map to tensor cores. Practical GPU speedup may be less than theoretical.
- **Primarily validated on small-scale tasks**: FSA emulation benchmarks and time-series classification, not yet proven at 1B+ parameter LLM pretraining scale.

## Implementation Notes

```python
# PD-SSM transition matrix multiplication (core trick)
# Each PD matrix stored as (indices, values) pair
# indices: int tensor [N] — which row is nonzero in each column
# values: complex tensor [N] — the diagonal values

def pd_matmul(idx_A, val_A, idx_B, val_B):
    """Multiply two PD matrices in O(N) time.

    A = P_A @ D_A, B = P_B @ D_B
    C = A @ B = (P_A @ D_A) @ (P_B @ D_B)

    The result indices: gather A's indices using B's indices
    The result values: elementwise multiply gathered A values with B values
    """
    # Gather: new permutation indices
    idx_C = idx_A[idx_B]              # O(N) gather
    # Elementwise: new diagonal values
    val_C = val_A[idx_B] * val_B      # O(N) elementwise
    return idx_C, val_C

# Parallel scan with PD matrices
# Standard Blelloch scan but with O(N) binary operator instead of O(N^3)
def pd_scan(indices, values, inputs):
    """
    indices: [L, N] — permutation indices at each timestep
    values: [L, N] — complex diagonal values at each timestep
    inputs: [L, N] — input vectors b_t
    """
    # Each scan element: (idx_t, val_t, b_t)
    # Binary op: (A2, b2) • (A1, b1) = (A2@A1, A2@b1 + b2)
    # A2@A1 uses pd_matmul: O(N)
    # A2@b1 is just: val_A2 * b1[idx_A2] which is also O(N)
    pass  # Use standard parallel scan with custom binary op
```

## References

- Terzić, A., Menet, N., Hersche, M., Hofmann, T., & Rahimi, A. (2025). Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models. NeurIPS 2025 (Spotlight). arXiv:2509.22284.
- GitHub: https://github.com/IBM/expressive-sparse-state-space-model
- Blelloch, G. E. (1990). Prefix sums and their applications. Tech Report CMU-CS-90-190.
- Gu, A. & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces.
